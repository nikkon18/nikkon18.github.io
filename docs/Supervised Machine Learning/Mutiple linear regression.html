<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Nikki&#39;s Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Nikki&#39;s Blog Atom Feed"><title data-react-helmet="true">4 - Mutiple linear regression | Nikki&#x27;s Blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://nikkon18.github.io//docs/Supervised Machine Learning/Mutiple linear regression"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="4 - Mutiple linear regression | Nikki&#x27;s Blog"><meta data-react-helmet="true" name="description" content="Linear Regression with Multiple Features"><meta data-react-helmet="true" property="og:description" content="Linear Regression with Multiple Features"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Mutiple linear regression"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Mutiple linear regression" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Mutiple linear regression" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9c11c536.css">
<link rel="preload" href="/assets/js/runtime~main.852293d6.js" as="script">
<link rel="preload" href="/assets/js/main.026a2937.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Nikki&#x27;s Blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/Supervised Machine Learning/Overview of Machine learning">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">üåú</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">üåû</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/Supervised Machine Learning/Overview of Machine learning">Supervised Machine Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Overview of Machine learning">1 - Overview of Machine learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Regression Model">2 - Regression Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Train the model with gradient descent">3 - Train the model with gradient descent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Supervised Machine Learning/Mutiple linear regression">4 - Mutiple linear regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Gradient descent in practice">5 - Gradient descent in practice</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/classification with logistic regression">6 - classification with logistic regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/The problem of overditting">7 - The problem of overditting</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/Advanced Learning Algorithms/Neural networks intuition">Advanced Learning Algorithms</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>4 - Mutiple linear regression</h1><h2 class="anchor anchorWithStickyNavbar_mojV" id="linear-regression-with-multiple-features">Linear Regression with Multiple Features<a class="hash-link" href="#linear-regression-with-multiple-features" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">‚Äã</a></h3><ul><li>Linear regression can be extended to include multiple features for prediction.</li><li>By considering additional features, we can gather more information to improve predictions.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="notation">Notation<a class="hash-link" href="#notation" title="Direct link to heading">‚Äã</a></h3><ul><li>Variables:<ul><li>X‚ÇÅ, X‚ÇÇ, X‚ÇÉ, X‚ÇÑ: Denote the four features.</li><li>X·µ¢‚±º: Refers to the jth feature of the ith training example.</li><li>X·µ¢: Represents the list of features for the ith training example.</li><li>n: Total number of features (n = 4 in this example).</li></ul></li><li>Example:<ul><li>X¬≤: Vector of features for the second training example: <!-- -->[1416, 3, 2, 40]<!-- -->.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="model-with-multiple-features">Model with Multiple Features<a class="hash-link" href="#model-with-multiple-features" title="Direct link to heading">‚Äã</a></h3><ul><li>In the original linear regression, the model was defined as f‚Çí·µ§‚Çú(x) = wx + b.</li><li>With multiple features, the model is defined as follows:<ul><li>f‚Çí·µ§‚Çú(x) = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + w‚ÇÉx‚ÇÉ + w‚ÇÑx‚ÇÑ + b.</li></ul></li><li>Example for housing price prediction:<ul><li>Model: f(x) = 0.1x‚ÇÅ + 4x‚ÇÇ + 10x‚ÇÉ - 2x‚ÇÑ + 80.</li></ul></li><li>Parameter interpretation:<ul><li>b (80): Base price of a house with no features.</li><li>0.1x‚ÇÅ: Price increase per square foot.</li><li>4x‚ÇÇ: Price increase per bedroom.</li><li>10x‚ÇÉ: Price increase per floor.</li><li>-2x‚ÇÑ: Price decrease per year of house age.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="general-model-with-n-features">General Model with n Features<a class="hash-link" href="#general-model-with-n-features" title="Direct link to heading">‚Äã</a></h3><ul><li>Model definition with n features: f‚Çí·µ§‚Çú(x) = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô + b.</li><li>Introduction of vector notation:<ul><li>W: List of parameters <!-- -->[w‚ÇÅ, w‚ÇÇ, ..., w‚Çô]<!-- -->.</li><li>X: List of features <!-- -->[x‚ÇÅ, x‚ÇÇ, ..., x‚Çô]<!-- -->.</li></ul></li><li>Model can be rewritten as: f‚Çí·µ§‚Çú(x) = W‚ãÖX + b.</li><li>Dot product of vectors:<ul><li>Dot product of W and X: W‚ãÖX = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çôx‚Çô.</li></ul></li><li>Compact form of the model: f‚Çí·µ§‚Çú(x) = W‚ãÖX + b.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="multiple-linear-regression">Multiple Linear Regression<a class="hash-link" href="#multiple-linear-regression" title="Direct link to heading">‚Äã</a></h3><ul><li>Linear regression with multiple input features is called multiple linear regression.</li><li>It extends the concept of univariate regression, which has only one feature.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">‚Äã</a></h3><ul><li>Linear regression can incorporate multiple features for better predictions.</li><li>Multiple linear regression models are defined using vector notation.</li><li>The dot product of vectors simplifies the model representation.</li><li>Multiple linear regression extends univariate regression.</li><li>Vectorization is a useful technique for implementing learning algorithms.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="vectorization-in-machine-learning-part1">Vectorization in Machine Learning part1<a class="hash-link" href="#vectorization-in-machine-learning-part1" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-1">Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">‚Äã</a></h3><ul><li>Vectorization is a powerful concept in implementing learning algorithms.</li><li>It makes code shorter and more efficient.</li><li>Taking advantage of numerical linear algebra libraries and GPU hardware further enhances performance.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="understanding-vectorization">Understanding Vectorization<a class="hash-link" href="#understanding-vectorization" title="Direct link to heading">‚Äã</a></h3><ul><li>Vectorization allows concise and efficient code implementation.</li><li>Parameters (w) and features (x) are represented as vectors.</li><li>Indexing in Python starts from 0, unlike linear algebra where it starts from 1.</li><li>NumPy library is widely used for numerical linear algebra operations in Python and machine learning.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="non-vectorized-implementation">Non-Vectorized Implementation<a class="hash-link" href="#non-vectorized-implementation" title="Direct link to heading">‚Äã</a></h3><ul><li>Non-vectorized code involves multiplying each parameter with its associated feature.</li><li>It becomes inefficient when the number of features (n) is large.</li><li>Using a for loop and summation can improve efficiency but still lacks vectorization.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="vectorized-implementation">Vectorized Implementation<a class="hash-link" href="#vectorized-implementation" title="Direct link to heading">‚Äã</a></h3><ul><li>Vectorized code leverages the dot product operation.</li><li>The dot product is computed using the NumPy dot function.</li><li>The dot function is a vectorized implementation of the dot product.</li><li>It significantly improves efficiency, especially with large values of n.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="benefits-of-vectorization">Benefits of Vectorization<a class="hash-link" href="#benefits-of-vectorization" title="Direct link to heading">‚Äã</a></h3><ul><li>Vectorization has two main benefits:<ul><li>Shorter code: It reduces the code to a single line.</li><li>Faster execution: It takes advantage of parallel hardware, such as GPUs.</li></ul></li><li>The NumPy dot function utilizes parallel hardware, making it more computationally efficient.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-1">Conclusion<a class="hash-link" href="#conclusion-1" title="Direct link to heading">‚Äã</a></h3><ul><li>Vectorization is a powerful technique in machine learning.</li><li>It simplifies code and improves its performance.</li><li>By leveraging numerical linear algebra libraries and parallel hardware, vectorized code achieves faster execution.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="vectorization-in-machine-learning-part2">Vectorization in Machine Learning part2<a class="hash-link" href="#vectorization-in-machine-learning-part2" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-2">Introduction<a class="hash-link" href="#introduction-2" title="Direct link to heading">‚Äã</a></h3><ul><li>Vectorization is a technique that significantly improves the efficiency of machine learning algorithms.</li><li>It involves performing operations on vectors rather than using explicit for loops.</li><li>Vectorized code is concise, easier to understand, and utilizes parallel hardware for faster execution.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="understanding-vectorization-1">Understanding Vectorization<a class="hash-link" href="#understanding-vectorization-1" title="Direct link to heading">‚Äã</a></h3><ul><li>Vectorization allows for efficient implementation of algorithms by operating on entire vectors simultaneously.</li><li>Non-vectorized code performs operations one step at a time, resulting in slower execution.</li><li>Vectorized code takes advantage of parallel processing hardware to perform calculations in a single step.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="benefits-of-vectorization-1">Benefits of Vectorization<a class="hash-link" href="#benefits-of-vectorization-1" title="Direct link to heading">‚Äã</a></h3><ul><li>Code Efficiency: Vectorized code is shorter and more concise, reducing the number of lines required.</li><li>Faster Execution: By leveraging parallel hardware, vectorized code performs calculations much faster than non-vectorized code.</li><li>Scalability: Vectorization is crucial for handling large datasets and training complex models.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="example-multiple-linear-regression">Example: Multiple Linear Regression<a class="hash-link" href="#example-multiple-linear-regression" title="Direct link to heading">‚Äã</a></h3><ul><li>In multiple linear regression, vectorization plays a key role in updating the parameters efficiently.</li><li>Without vectorization, each parameter is updated individually using a for loop.</li><li>With vectorization, the computer performs parallel computations to update all parameters simultaneously.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="implementation-with-numpy">Implementation with NumPy<a class="hash-link" href="#implementation-with-numpy" title="Direct link to heading">‚Äã</a></h3><ul><li>NumPy is a popular Python library for numerical computations and vectorization.</li><li>It provides functions like dot product (dot) that operate on vectors efficiently.</li><li>NumPy arrays, or vectors, are used to store and manipulate data in a vectorized manner.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="benefits-of-vectorization-in-machine-learning">Benefits of Vectorization in Machine Learning<a class="hash-link" href="#benefits-of-vectorization-in-machine-learning" title="Direct link to heading">‚Äã</a></h3><ul><li>Improved Efficiency: Vectorization significantly speeds up the execution of learning algorithms.</li><li>Handling Large Datasets: Vectorized code is essential for processing large amounts of data effectively.</li><li>Scaling Machine Learning Algorithms: Vectorization enables algorithms to scale well to handle complex models and datasets.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-2">Conclusion<a class="hash-link" href="#conclusion-2" title="Direct link to heading">‚Äã</a></h3><ul><li>Vectorization is a powerful technique in machine learning for efficient algorithm implementation.</li><li>It leverages parallel processing hardware and libraries like NumPy to perform calculations on vectors.</li><li>Vectorized code is concise, faster, and crucial for handling large datasets and complex models.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-for-multiple-linear-regression-with-vectorization">Gradient Descent for Multiple Linear Regression with Vectorization<a class="hash-link" href="#gradient-descent-for-multiple-linear-regression-with-vectorization" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-3">Introduction<a class="hash-link" href="#introduction-3" title="Direct link to heading">‚Äã</a></h3><ul><li>Review of multiple linear regression and vector notation.</li><li>Applying gradient descent to multiple linear regression with vectorization.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="multiple-linear-regression-in-vector-notation">Multiple Linear Regression in Vector Notation<a class="hash-link" href="#multiple-linear-regression-in-vector-notation" title="Direct link to heading">‚Äã</a></h3><ul><li>Parameters w_1 to w_n and b are combined into a parameter vector w.</li><li>Model: f_w, b(x) = w‚ãÖx + b (dot product notation).</li><li>Cost function: J(w, b).</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-for-multiple-linear-regression">Gradient Descent for Multiple Linear Regression<a class="hash-link" href="#gradient-descent-for-multiple-linear-regression" title="Direct link to heading">‚Äã</a></h3><ul><li>Update rule for each parameter w_j: w_j = w_j - Œ± <!-- -->*<!-- --> ‚àÇJ/‚àÇw_j.</li><li>Update rule for b: b = b - Œ± <!-- -->*<!-- --> ‚àÇJ/‚àÇb.</li><li>Derivative of J with respect to w<em>1: ‚àÇJ/‚àÇw_1 = (1/m) </em> ‚àë(f<em>w,b(x) - y) </em> x_1.</li><li>For multiple features (n &gt; 1): ‚àÇJ/‚àÇw<em>j = (1/m) </em> ‚àë(f<em>w,b(x) - y) </em> x_j.</li><li>Update all parameters w_1 to w_n and b iteratively.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="normal-equation-method-side-note">Normal Equation Method (Side Note)<a class="hash-link" href="#normal-equation-method-side-note" title="Direct link to heading">‚Äã</a></h3><ul><li>Alternative method to find w and b for linear regression.</li><li>Solves for w and b directly without iterations.</li><li>Only applicable to linear regression.</li><li>Not generalized to other learning algorithms.</li><li>Slow for large number of features.</li><li>Often used in mature machine learning libraries.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-3">Conclusion<a class="hash-link" href="#conclusion-3" title="Direct link to heading">‚Äã</a></h3><ul><li>Gradient descent with vectorization is a powerful approach for multiple linear regression.</li><li>Parameters are updated iteratively using derivative calculations.</li><li>Normal equation method provides an alternative, but less flexible, approach.</li><li>Gradient descent is widely used and offers better scalability and generality.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/4-Mutiple linear regression.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/Train the model with gradient descent"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">3 - Train the model with gradient descent</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/Gradient descent in practice"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">5 - Gradient descent in practice</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#linear-regression-with-multiple-features" class="table-of-contents__link toc-highlight">Linear Regression with Multiple Features</a><ul><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#notation" class="table-of-contents__link toc-highlight">Notation</a></li><li><a href="#model-with-multiple-features" class="table-of-contents__link toc-highlight">Model with Multiple Features</a></li><li><a href="#general-model-with-n-features" class="table-of-contents__link toc-highlight">General Model with n Features</a></li><li><a href="#multiple-linear-regression" class="table-of-contents__link toc-highlight">Multiple Linear Regression</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#vectorization-in-machine-learning-part1" class="table-of-contents__link toc-highlight">Vectorization in Machine Learning part1</a><ul><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#understanding-vectorization" class="table-of-contents__link toc-highlight">Understanding Vectorization</a></li><li><a href="#non-vectorized-implementation" class="table-of-contents__link toc-highlight">Non-Vectorized Implementation</a></li><li><a href="#vectorized-implementation" class="table-of-contents__link toc-highlight">Vectorized Implementation</a></li><li><a href="#benefits-of-vectorization" class="table-of-contents__link toc-highlight">Benefits of Vectorization</a></li><li><a href="#conclusion-1" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#vectorization-in-machine-learning-part2" class="table-of-contents__link toc-highlight">Vectorization in Machine Learning part2</a><ul><li><a href="#introduction-2" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#understanding-vectorization-1" class="table-of-contents__link toc-highlight">Understanding Vectorization</a></li><li><a href="#benefits-of-vectorization-1" class="table-of-contents__link toc-highlight">Benefits of Vectorization</a></li><li><a href="#example-multiple-linear-regression" class="table-of-contents__link toc-highlight">Example: Multiple Linear Regression</a></li><li><a href="#implementation-with-numpy" class="table-of-contents__link toc-highlight">Implementation with NumPy</a></li><li><a href="#benefits-of-vectorization-in-machine-learning" class="table-of-contents__link toc-highlight">Benefits of Vectorization in Machine Learning</a></li><li><a href="#conclusion-2" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#gradient-descent-for-multiple-linear-regression-with-vectorization" class="table-of-contents__link toc-highlight">Gradient Descent for Multiple Linear Regression with Vectorization</a><ul><li><a href="#introduction-3" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#multiple-linear-regression-in-vector-notation" class="table-of-contents__link toc-highlight">Multiple Linear Regression in Vector Notation</a></li><li><a href="#gradient-descent-for-multiple-linear-regression" class="table-of-contents__link toc-highlight">Gradient Descent for Multiple Linear Regression</a></li><li><a href="#normal-equation-method-side-note" class="table-of-contents__link toc-highlight">Normal Equation Method (Side Note)</a></li><li><a href="#conclusion-3" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs">Notes</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright ¬© 2023</div></div></div></footer></div>
<script src="/assets/js/runtime~main.852293d6.js"></script>
<script src="/assets/js/main.026a2937.js"></script>
</body>
</html>