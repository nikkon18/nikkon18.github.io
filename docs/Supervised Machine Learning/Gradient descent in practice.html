<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Nikki&#39;s Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Nikki&#39;s Blog Atom Feed"><title data-react-helmet="true">5 - Gradient descent in practice | Nikki&#x27;s Blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://nikkon18.github.io//docs/Supervised Machine Learning/Gradient descent in practice"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="5 - Gradient descent in practice | Nikki&#x27;s Blog"><meta data-react-helmet="true" name="description" content="Feature Scaling for Improved Gradient Descent"><meta data-react-helmet="true" property="og:description" content="Feature Scaling for Improved Gradient Descent"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Gradient descent in practice"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Gradient descent in practice" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Gradient descent in practice" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9c11c536.css">
<link rel="preload" href="/assets/js/runtime~main.852293d6.js" as="script">
<link rel="preload" href="/assets/js/main.026a2937.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Nikki&#x27;s Blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/Supervised Machine Learning/Overview of Machine learning">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/Supervised Machine Learning/Overview of Machine learning">Supervised Machine Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Overview of Machine learning">1 - Overview of Machine learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Regression Model">2 - Regression Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Train the model with gradient descent">3 - Train the model with gradient descent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Mutiple linear regression">4 - Mutiple linear regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Supervised Machine Learning/Gradient descent in practice">5 - Gradient descent in practice</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/classification with logistic regression">6 - classification with logistic regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/The problem of overditting">7 - The problem of overditting</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/Advanced Learning Algorithms/Neural networks intuition">Advanced Learning Algorithms</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>5 - Gradient descent in practice</h1><h2 class="anchor anchorWithStickyNavbar_mojV" id="feature-scaling-for-improved-gradient-descent">Feature Scaling for Improved Gradient Descent<a class="hash-link" href="#feature-scaling-for-improved-gradient-descent" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h3><ul><li>Feature scaling can enhance the performance of gradient descent.</li><li>Scaling ensures that features with different value ranges do not hinder convergence.</li><li>Rescaling features to comparable ranges enables faster gradient descent.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="understanding-the-impact-of-feature-size">Understanding the Impact of Feature Size<a class="hash-link" href="#understanding-the-impact-of-feature-size" title="Direct link to heading">â€‹</a></h3><ul><li>Example: Predicting house prices using size (x1) and number of bedrooms (x2).</li><li>x1 ranges from 300 to 2000 square feet, while x2 ranges from 0 to 5 bedrooms.</li><li>Parameters: w1, w2, and b.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="effect-of-parameter-choices">Effect of Parameter Choices<a class="hash-link" href="#effect-of-parameter-choices" title="Direct link to heading">â€‹</a></h3><ul><li>Incorrect parameter choices can lead to inaccurate predictions.</li><li>Example: For a house with 2000 sq. ft., 5 bedrooms, and $500,000 price:<ul><li>Incorrect parameters: w1 = 50, w2 = 0.1, b = 50.<ul><li>Predicted price: $100,150,050 (far from actual price).</li></ul></li><li>Correct parameters: w1 = 0.1, w2 = 50, b = 50.<ul><li>Predicted price: $500,000 (reasonable estimate).</li></ul></li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="relationship-to-gradient-descent">Relationship to Gradient Descent<a class="hash-link" href="#relationship-to-gradient-descent" title="Direct link to heading">â€‹</a></h3><ul><li>Feature size affects parameter choices and convergence.</li><li>Scatter plot: x1 (size) vs. x2 (bedrooms).</li><li>Contour plot of cost function: x1 (0-1) vs. x2 (10-100).</li><li>Tall and skinny contours for large x1 range, small x2 range.</li><li>Small changes in w1 have a significant impact on predictions and cost (J).</li><li>Gradient descent may oscillate before reaching the global minimum.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="importance-of-feature-scaling">Importance of Feature Scaling<a class="hash-link" href="#importance-of-feature-scaling" title="Direct link to heading">â€‹</a></h3><ul><li>Scaling features mitigates issues with varying value ranges.</li><li>Rescale x1 and x2 to comparable ranges (e.g., 0-1).</li><li>Transformed data leads to circular contours in the cost function.</li><li>Gradient descent follows a more direct path to the global minimum.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">â€‹</a></h3><ul><li>Feature scaling is crucial for efficient gradient descent.</li><li>Features with disparate value ranges can impede convergence.</li><li>Rescaling features to comparable ranges enhances the performance of gradient descent.</li><li>Scaling ensures that all features contribute equally to the learning process.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="feature-scaling-for-improved-gradient-descent-1">Feature Scaling for Improved Gradient Descent<a class="hash-link" href="#feature-scaling-for-improved-gradient-descent-1" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-1">Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">â€‹</a></h3><ul><li>Feature scaling improves gradient descent performance.</li><li>Scaling makes features with different value ranges comparable.</li><li>Scaling methods: Maximum scaling, mean normalization, Z-score normalization.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="maximum-scaling">Maximum Scaling<a class="hash-link" href="#maximum-scaling" title="Direct link to heading">â€‹</a></h3><ul><li>Divide each feature value by its maximum to obtain a scaled version.</li><li>Example: x1 ranges from 3-2000, x2 ranges from 0-5.</li><li>Scale x1: Divide each value by 2000.</li><li>Scale x2: Divide each value by 5.</li><li>Scaled features range from 0-1.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="mean-normalization">Mean Normalization<a class="hash-link" href="#mean-normalization" title="Direct link to heading">â€‹</a></h3><ul><li>Center features around zero by subtracting the mean.</li><li>Divide by the range of values to normalize.</li><li>Example: x1 mean = 600 sq. ft., x2 mean = 2.3 bedrooms.</li><li>Normalize x1: Subtract mean and divide by (2000-300).</li><li>Normalize x2: Subtract mean and divide by (5-0).</li><li>Normalized features range from -1 to 1.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="z-score-normalization">Z-score Normalization<a class="hash-link" href="#z-score-normalization" title="Direct link to heading">â€‹</a></h3><ul><li>Use the mean and standard deviation to normalize features.</li><li>Calculate mean (Mu) and standard deviation (Sigma) for each feature.</li><li>Example: x1 Mu = 600, Sigma = 450; x2 Mu = 2.3, Sigma = 1.4.</li><li>Normalize x1: Subtract Mu and divide by Sigma.</li><li>Normalize x2: Subtract Mu and divide by Sigma.</li><li>Normalized features have a mean of 0 and standard deviation of 1.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="importance-of-feature-scaling-1">Importance of Feature Scaling<a class="hash-link" href="#importance-of-feature-scaling-1" title="Direct link to heading">â€‹</a></h3><ul><li>Scaling facilitates faster convergence of gradient descent.</li><li>Features with similar ranges prevent oscillation and improve performance.</li><li>Aim for features to range between -1 and 1 (flexible guideline).</li><li>Scaling handles disparate ranges and enhances algorithm efficiency.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-1">Conclusion<a class="hash-link" href="#conclusion-1" title="Direct link to heading">â€‹</a></h3><ul><li>Feature scaling is essential for gradient descent effectiveness.</li><li>Scaling methods include maximum scaling, mean normalization, and Z-score normalization.</li><li>Scaling ensures features contribute equally and aids convergence.</li><li>Implementing feature scaling improves the performance of machine learning models.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="understanding-convergence-of-gradient-descent">Understanding Convergence of Gradient Descent<a class="hash-link" href="#understanding-convergence-of-gradient-descent" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-2">Introduction<a class="hash-link" href="#introduction-2" title="Direct link to heading">â€‹</a></h3><ul><li>Convergence refers to the process of finding parameters close to the global minimum of the cost function.</li><li>Recognizing convergence helps in selecting an appropriate learning rate (Alpha).</li><li>Plotting the cost function J at each iteration aids in assessing convergence.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="learning-curve">Learning Curve<a class="hash-link" href="#learning-curve" title="Direct link to heading">â€‹</a></h3><ul><li>Plot J against the number of iterations of gradient descent.</li><li>Horizontal axis: Number of iterations.</li><li>Vertical axis: Cost J.</li><li>Learning curve shows the change in J after each iteration.</li><li>Ideally, J should decrease with each iteration.</li><li>If J increases after an iteration, Alpha may be too large or there may be a bug in the code.</li><li>Convergence is indicated when the curve levels off and J no longer decreases significantly.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="determining-convergence">Determining Convergence<a class="hash-link" href="#determining-convergence" title="Direct link to heading">â€‹</a></h3><ul><li>Examining the learning curve helps identify if gradient descent is converging.</li><li>Convergence iterations vary across applications.</li><li>Creating a learning curve graph assists in determining when training can be stopped.</li><li>Automatic convergence tests can also be employed.</li><li>Use a small number epsilon as a threshold for J decrease in one iteration.</li><li>If J decreases by less than epsilon, convergence is declared.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="choosing-the-right-threshold">Choosing the Right Threshold<a class="hash-link" href="#choosing-the-right-threshold" title="Direct link to heading">â€‹</a></h3><ul><li>Selecting the appropriate epsilon threshold can be challenging.</li><li>Graphical analysis of the learning curve provides insights into convergence.</li><li>Observing the learning curve graph warns of potential issues with gradient descent.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-2">Conclusion<a class="hash-link" href="#conclusion-2" title="Direct link to heading">â€‹</a></h3><ul><li>Convergence is crucial in finding optimal parameter values.</li><li>Learning curves display the change in cost J over iterations.</li><li>Proper convergence is characterized by a decreasing J.</li><li>Choosing the right learning rate and threshold epsilon is essential for successful convergence.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="choosing-the-learning-rate-in-gradient-descent">Choosing the Learning Rate in Gradient Descent<a class="hash-link" href="#choosing-the-learning-rate-in-gradient-descent" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-3">Introduction<a class="hash-link" href="#introduction-3" title="Direct link to heading">â€‹</a></h3><ul><li>The learning rate (Alpha) selection is crucial for effective gradient descent.</li><li>A small Alpha leads to slow convergence, while a large Alpha may prevent convergence.</li><li>Plotting the cost function over iterations helps in identifying issues with the learning rate.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="effects-of-learning-rate">Effects of Learning Rate<a class="hash-link" href="#effects-of-learning-rate" title="Direct link to heading">â€‹</a></h3><ul><li>If the learning rate is too large, the cost may increase instead of decreasing.</li><li>Overshooting the minimum due to large updates causes this behavior.</li><li>A smaller learning rate helps reduce overshooting and ensures gradual descent towards the minimum.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="debugging-gradient-descent">Debugging Gradient Descent<a class="hash-link" href="#debugging-gradient-descent" title="Direct link to heading">â€‹</a></h3><ul><li>Consistently increasing costs indicate a possible issue with the learning rate.</li><li>Incorrectly updating parameters using the derivative term can lead to this problem.</li><li>Ensure the code updates parameters as w_1 updated by w_1 - Alpha <!-- -->*<!-- --> derivative term.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="debugging-tip">Debugging Tip<a class="hash-link" href="#debugging-tip" title="Direct link to heading">â€‹</a></h3><ul><li>Set Alpha to a very small value to debug gradient descent.</li><li>If the cost doesn&#x27;t decrease on every iteration, there may be a bug in the code.</li><li>Note that using a very small Alpha is not efficient for actual training.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="choosing-the-learning-rate">Choosing the Learning Rate<a class="hash-link" href="#choosing-the-learning-rate" title="Direct link to heading">â€‹</a></h3><ul><li>A trade-off exists between convergence speed and the learning rate&#x27;s size.</li><li>Try a range of values for Alpha and observe the cost function&#x27;s behavior.</li><li>Plot the cost function J against the number of iterations for each Alpha.</li><li>Select a learning rate that consistently and rapidly decreases the cost.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="technique-for-choosing-alpha">Technique for Choosing Alpha<a class="hash-link" href="#technique-for-choosing-alpha" title="Direct link to heading">â€‹</a></h3><ul><li>Start with a small learning rate, e.g., 0.001, and run gradient descent for a few iterations.</li><li>Increase Alpha gradually, e.g., try 0.003 (three times larger than the previous Alpha).</li><li>Continue this process, trying larger values like 0.01 (roughly three times larger than the previous Alpha).</li><li>Find the smallest Alpha that causes the cost to increase and the largest reasonable Alpha.</li><li>Choose a learning rate slightly smaller than the largest reasonable value found.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="importance-of-experimentation">Importance of Experimentation<a class="hash-link" href="#importance-of-experimentation" title="Direct link to heading">â€‹</a></h3><ul><li>Trying different learning rates helps in finding the optimal choice.</li><li>It is important to experiment and gain intuition about different Alpha values.</li><li>Feature scaling and the learning rate&#x27;s impact can be explored in the optional lab.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-3">Conclusion<a class="hash-link" href="#conclusion-3" title="Direct link to heading">â€‹</a></h3><ul><li>Choosing the right learning rate is crucial for effective gradient descent.</li><li>A large learning rate can cause overshooting and prevent convergence.</li><li>Debugging and experimentation help in finding an appropriate learning rate.</li><li>Balancing convergence speed and the learning rate&#x27;s size is essential for successful training.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="feature-engineering-for-improved-learning-algorithm-performance">Feature Engineering for Improved Learning Algorithm Performance<a class="hash-link" href="#feature-engineering-for-improved-learning-algorithm-performance" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-4">Introduction<a class="hash-link" href="#introduction-4" title="Direct link to heading">â€‹</a></h3><ul><li>Choosing or engineering the right features is crucial for a learning algorithm&#x27;s success.</li><li>Feature engineering involves transforming or combining existing features to improve predictions.</li><li>We&#x27;ll explore feature engineering using the example of predicting house prices.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="initial-features">Initial Features<a class="hash-link" href="#initial-features" title="Direct link to heading">â€‹</a></h3><ul><li>Two features for each house: x_1 (frontage/width) and x_2 (depth).</li><li>Model: f(x) = w_1x_1 + w_2x_2 + b, where x_1 is frontage and x_2 is depth.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="introducing-a-new-feature">Introducing a New Feature<a class="hash-link" href="#introducing-a-new-feature" title="Direct link to heading">â€‹</a></h3><ul><li>Area of the land can be a more predictive feature than frontage and depth individually.</li><li>Define a new feature, x_3, as x_1 <!-- -->*<!-- --> x_2 (area of the plot of land).</li><li>Updated model: f_w,b(x) = w_1x_1 + w_2x_2 + w_3x_3 + b.</li><li>The model can now choose parameters w_1, w_2, and w_3 based on feature importance.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="feature-engineering">Feature Engineering<a class="hash-link" href="#feature-engineering" title="Direct link to heading">â€‹</a></h3><ul><li>Feature engineering involves transforming or combining features to aid the learning algorithm.</li><li>Use domain knowledge or intuition to design new features for better predictions.</li><li>Creating new features can significantly improve the model&#x27;s performance.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="benefits-of-feature-engineering">Benefits of Feature Engineering<a class="hash-link" href="#benefits-of-feature-engineering" title="Direct link to heading">â€‹</a></h3><ul><li>Enables fitting curves and non-linear functions, not just straight lines.</li><li>Helps capture complex relationships and patterns in the data.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-4">Conclusion<a class="hash-link" href="#conclusion-4" title="Direct link to heading">â€‹</a></h3><ul><li>Feature engineering plays a vital role in improving learning algorithm performance.</li><li>By creating new features based on domain knowledge or intuition, we can enhance predictions.</li><li>Feature engineering allows fitting curves and non-linear functions to capture complex patterns.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="polynomial-regression-and-feature-engineering">Polynomial Regression and Feature Engineering<a class="hash-link" href="#polynomial-regression-and-feature-engineering" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-5">Introduction<a class="hash-link" href="#introduction-5" title="Direct link to heading">â€‹</a></h3><ul><li>Fitting straight lines may not be sufficient for certain datasets.</li><li>Polynomial regression combines feature engineering and multiple linear regression to fit curves.</li><li>Polynomial regression allows for non-linear functions to be fitted to the data.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="example-housing-dataset">Example: Housing Dataset<a class="hash-link" href="#example-housing-dataset" title="Direct link to heading">â€‹</a></h3><ul><li>Dataset with feature x as the size of houses in square feet.</li><li>Straight lines do not fit the data well.</li><li>The need for curves motivates quadratic and cubic functions.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="polynomial-regression">Polynomial Regression<a class="hash-link" href="#polynomial-regression" title="Direct link to heading">â€‹</a></h3><ul><li>Quadratic function example: f(x) = w<em>1 </em> x + w<em>2 </em> x^2 + b.</li><li>Cubic function example: f(x) = w<em>1 </em> x + w<em>2 </em> x^2 + w_3 <!-- -->*<!-- --> x^3 + b.</li><li>Additional features are created by raising the original feature to a power.</li><li>Feature scaling becomes important when using higher power features.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="choosing-features">Choosing Features<a class="hash-link" href="#choosing-features" title="Direct link to heading">â€‹</a></h3><ul><li>The choice of features depends on the problem and desired model.</li><li>Different models with different features can be compared for performance.</li><li>Feature engineering allows for flexibility in selecting features.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="alternative-features">Alternative Features<a class="hash-link" href="#alternative-features" title="Direct link to heading">â€‹</a></h3><ul><li>Other choices for features include square root functions.</li><li>Example: f(x) = w<em>1 </em> x + w<em>2 </em> sqrt(x) + b.</li><li>Feature engineering provides a wide range of feature options.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="importance-of-feature-engineering">Importance of Feature Engineering<a class="hash-link" href="#importance-of-feature-engineering" title="Direct link to heading">â€‹</a></h3><ul><li>Feature engineering improves model performance and accuracy.</li><li>Polynomial regression enables better modeling of complex relationships.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="practical-implementation">Practical Implementation<a class="hash-link" href="#practical-implementation" title="Direct link to heading">â€‹</a></h3><ul><li>Scikit-learn is a popular machine learning library for implementing regression models.</li><li>Familiarity with implementing algorithms is important alongside using libraries.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-5">Conclusion<a class="hash-link" href="#conclusion-5" title="Direct link to heading">â€‹</a></h3><ul><li>Polynomial regression combines feature engineering and multiple linear regression.</li><li>Feature engineering allows fitting curves and non-linear functions to data.</li><li>Scikit-learn is a valuable tool for practical implementation.</li><li>Understanding algorithm implementation is crucial for model understanding and effective usage.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/5-Gradient descent in practice.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/Mutiple linear regression"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">4 - Mutiple linear regression</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/classification with logistic regression"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">6 - classification with logistic regression</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#feature-scaling-for-improved-gradient-descent" class="table-of-contents__link toc-highlight">Feature Scaling for Improved Gradient Descent</a><ul><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#understanding-the-impact-of-feature-size" class="table-of-contents__link toc-highlight">Understanding the Impact of Feature Size</a></li><li><a href="#effect-of-parameter-choices" class="table-of-contents__link toc-highlight">Effect of Parameter Choices</a></li><li><a href="#relationship-to-gradient-descent" class="table-of-contents__link toc-highlight">Relationship to Gradient Descent</a></li><li><a href="#importance-of-feature-scaling" class="table-of-contents__link toc-highlight">Importance of Feature Scaling</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#feature-scaling-for-improved-gradient-descent-1" class="table-of-contents__link toc-highlight">Feature Scaling for Improved Gradient Descent</a><ul><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#maximum-scaling" class="table-of-contents__link toc-highlight">Maximum Scaling</a></li><li><a href="#mean-normalization" class="table-of-contents__link toc-highlight">Mean Normalization</a></li><li><a href="#z-score-normalization" class="table-of-contents__link toc-highlight">Z-score Normalization</a></li><li><a href="#importance-of-feature-scaling-1" class="table-of-contents__link toc-highlight">Importance of Feature Scaling</a></li><li><a href="#conclusion-1" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#understanding-convergence-of-gradient-descent" class="table-of-contents__link toc-highlight">Understanding Convergence of Gradient Descent</a><ul><li><a href="#introduction-2" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#learning-curve" class="table-of-contents__link toc-highlight">Learning Curve</a></li><li><a href="#determining-convergence" class="table-of-contents__link toc-highlight">Determining Convergence</a></li><li><a href="#choosing-the-right-threshold" class="table-of-contents__link toc-highlight">Choosing the Right Threshold</a></li><li><a href="#conclusion-2" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#choosing-the-learning-rate-in-gradient-descent" class="table-of-contents__link toc-highlight">Choosing the Learning Rate in Gradient Descent</a><ul><li><a href="#introduction-3" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#effects-of-learning-rate" class="table-of-contents__link toc-highlight">Effects of Learning Rate</a></li><li><a href="#debugging-gradient-descent" class="table-of-contents__link toc-highlight">Debugging Gradient Descent</a></li><li><a href="#debugging-tip" class="table-of-contents__link toc-highlight">Debugging Tip</a></li><li><a href="#choosing-the-learning-rate" class="table-of-contents__link toc-highlight">Choosing the Learning Rate</a></li><li><a href="#technique-for-choosing-alpha" class="table-of-contents__link toc-highlight">Technique for Choosing Alpha</a></li><li><a href="#importance-of-experimentation" class="table-of-contents__link toc-highlight">Importance of Experimentation</a></li><li><a href="#conclusion-3" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#feature-engineering-for-improved-learning-algorithm-performance" class="table-of-contents__link toc-highlight">Feature Engineering for Improved Learning Algorithm Performance</a><ul><li><a href="#introduction-4" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#initial-features" class="table-of-contents__link toc-highlight">Initial Features</a></li><li><a href="#introducing-a-new-feature" class="table-of-contents__link toc-highlight">Introducing a New Feature</a></li><li><a href="#feature-engineering" class="table-of-contents__link toc-highlight">Feature Engineering</a></li><li><a href="#benefits-of-feature-engineering" class="table-of-contents__link toc-highlight">Benefits of Feature Engineering</a></li><li><a href="#conclusion-4" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#polynomial-regression-and-feature-engineering" class="table-of-contents__link toc-highlight">Polynomial Regression and Feature Engineering</a><ul><li><a href="#introduction-5" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#example-housing-dataset" class="table-of-contents__link toc-highlight">Example: Housing Dataset</a></li><li><a href="#polynomial-regression" class="table-of-contents__link toc-highlight">Polynomial Regression</a></li><li><a href="#choosing-features" class="table-of-contents__link toc-highlight">Choosing Features</a></li><li><a href="#alternative-features" class="table-of-contents__link toc-highlight">Alternative Features</a></li><li><a href="#importance-of-feature-engineering" class="table-of-contents__link toc-highlight">Importance of Feature Engineering</a></li><li><a href="#practical-implementation" class="table-of-contents__link toc-highlight">Practical Implementation</a></li><li><a href="#conclusion-5" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs">Notes</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023</div></div></div></footer></div>
<script src="/assets/js/runtime~main.852293d6.js"></script>
<script src="/assets/js/main.026a2937.js"></script>
</body>
</html>