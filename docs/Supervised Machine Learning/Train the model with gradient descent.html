<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Nikki&#39;s Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Nikki&#39;s Blog Atom Feed"><title data-react-helmet="true">3 - Train the model with gradient descent | Nikki&#x27;s Blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://nikkon18.github.io//docs/Supervised Machine Learning/Train the model with gradient descent"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="3 - Train the model with gradient descent | Nikki&#x27;s Blog"><meta data-react-helmet="true" name="description" content="Gradient Descent for Minimizing Cost Function"><meta data-react-helmet="true" property="og:description" content="Gradient Descent for Minimizing Cost Function"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Train the model with gradient descent"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Train the model with gradient descent" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/Train the model with gradient descent" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9c11c536.css">
<link rel="preload" href="/assets/js/runtime~main.852293d6.js" as="script">
<link rel="preload" href="/assets/js/main.026a2937.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Nikki&#x27;s Blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/Supervised Machine Learning/Overview of Machine learning">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/Supervised Machine Learning/Overview of Machine learning">Supervised Machine Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Overview of Machine learning">1 - Overview of Machine learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Regression Model">2 - Regression Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Supervised Machine Learning/Train the model with gradient descent">3 - Train the model with gradient descent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Mutiple linear regression">4 - Mutiple linear regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Gradient descent in practice">5 - Gradient descent in practice</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/classification with logistic regression">6 - classification with logistic regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/The problem of overditting">7 - The problem of overditting</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/Advanced Learning Algorithms/Neural networks intuition">Advanced Learning Algorithms</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>3 - Train the model with gradient descent</h1><h2 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-for-minimizing-cost-function">Gradient Descent for Minimizing Cost Function<a class="hash-link" href="#gradient-descent-for-minimizing-cost-function" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent is an algorithm used to minimize any function, not just a cost function for linear regression.</li><li>It can be used to minimize a cost function, J(w,b), which is a measure of how well the line f(x) fits the data points.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="objective">Objective<a class="hash-link" href="#objective" title="Direct link to heading">â€‹</a></h3><ul><li>The objective is to minimize J over the parameters w and b.</li><li>You want to pick values for w and b that give you the smallest possible value of J.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="algorithm">Algorithm<a class="hash-link" href="#algorithm" title="Direct link to heading">â€‹</a></h3><ul><li>Start with some initial guesses for w and b. A common choice is to set them both to 0.</li><li>Keep changing the parameters w and b a bit every time to try to reduce the cost J of w, b until hopefully J settles at or near a minimum.</li><li>Gradient descent applies to more general functions, including other cost functions that work with models that have more than two parameters.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="visualizing-gradient-descent">Visualizing Gradient Descent<a class="hash-link" href="#visualizing-gradient-descent" title="Direct link to heading">â€‹</a></h3><ul><li>Imagine that the cost function J is a view of a slightly hilly outdoor park or a golf course where the high points are hills and the low points are valleys.</li><li>The goal is to start at the top of a hill and get to the bottom of one of these valleys as efficiently as possible.</li><li>The algorithm spins around 360 degrees and looks around to find the direction of steepest descent.</li><li>The direction of steepest descent is the best direction to take your next step downhill.</li><li>After taking this step, repeat the process and keep going until you find yourself at the bottom of the valley, at a local minimum.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="properties-of-gradient-descent">Properties of Gradient Descent<a class="hash-link" href="#properties-of-gradient-descent" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent has an interesting property where the choice of initial parameters affects the final result.</li><li>If you start the algorithm with different initial parameters, you may end up at a different local minimum.</li><li>The bottoms of both the first and the second valleys are called local minima.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent is an algorithm used to minimize a cost function J over the parameters w and b.</li><li>The algorithm starts with some initial guesses for w and b and keeps changing the parameters to reduce the cost J until it settles at or near a minimum.</li><li>The algorithm finds the direction of steepest descent and takes steps downhill until it reaches a local minimum.</li><li>The choice of initial parameters affects the final result, and different initial parameters may lead to different local minima.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="implementing-gradient-descent-algorithm">Implementing Gradient Descent Algorithm<a class="hash-link" href="#implementing-gradient-descent-algorithm" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-1">Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">â€‹</a></h3><ul><li>Implementing the gradient descent algorithm</li><li>Updating the parameter <code>w</code> on each step</li><li>The update equation: <code>w = w - Î± * (dJ(wb)/dw)</code></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="understanding-the-equation">Understanding the Equation<a class="hash-link" href="#understanding-the-equation" title="Direct link to heading">â€‹</a></h3><ul><li><code>=</code> is the assignment operator in programming contexts</li><li><code>Î±</code> is the learning rate, a small positive number between 0 and 1</li><li>Learning rate controls the step size downhill</li><li><code>(dJ(wb)/dw)</code> is the derivative term of the cost function <code>J</code></li><li>Derivative term indicates the direction to take a &quot;baby step&quot;</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="updating-w-and-b">Updating <code>w</code> and <code>b</code><a class="hash-link" href="#updating-w-and-b" title="Direct link to heading">â€‹</a></h3><ul><li>Model has two parameters, <code>w</code> and <code>b</code></li><li>To update <code>b</code>, use similar equation with <code>dJ(wb)/db</code></li><li>Repeat the update steps until the algorithm converges</li><li>Convergence is when <code>w</code> and <code>b</code> no longer change much with each step</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="simultaneous-update">Simultaneous Update<a class="hash-link" href="#simultaneous-update" title="Direct link to heading">â€‹</a></h3><ul><li>Update both <code>w</code> and <code>b</code> simultaneously</li><li>Compute both update terms and store in temporary variables</li><li>Copy the values of temporary variables to <code>w</code> and <code>b</code></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="incorrect-implementation">Incorrect Implementation<a class="hash-link" href="#incorrect-implementation" title="Direct link to heading">â€‹</a></h3><ul><li>Non-simultaneous update will probably work but not correct</li><li>The updated <code>w</code> and <code>b</code> values differ from the correct simultaneous update</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-1">Conclusion<a class="hash-link" href="#conclusion-1" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent is a powerful algorithm for optimizing models</li><li>Derivatives come from calculus, but not necessary to know calculus to use gradient descent</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-intuition-and-learning-rate">Gradient Descent Intuition and Learning Rate<a class="hash-link" href="#gradient-descent-intuition-and-learning-rate" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-2">Introduction<a class="hash-link" href="#introduction-2" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent algorithm updates model parameters w and b</li><li>Learning rate alpha controls the step size in updating w and b</li><li>Derivative term dJ/dw describes the slope of the cost function J at a given point</li><li>Positive derivative means move to the left on the graph and decrease w</li><li>Negative derivative means move to the right on the graph and increase w</li><li>The goal is to reach the minimum of the cost function J</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-on-one-parameter">Gradient Descent on One Parameter<a class="hash-link" href="#gradient-descent-on-one-parameter" title="Direct link to heading">â€‹</a></h3><ul><li>Consider a cost function J of just one parameter w</li><li>Plot the cost as a function of w</li><li>Gradient descent updates w to w - alpha <!-- -->*<!-- --> dJ/dw</li><li>Positive derivative means decrease w, move to the left on the graph</li><li>Negative derivative means increase w, move to the right on the graph</li><li>Goal is to decrease the cost J and reach the minimum</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="choosing-the-learning-rate-alpha">Choosing the Learning Rate Alpha<a class="hash-link" href="#choosing-the-learning-rate-alpha" title="Direct link to heading">â€‹</a></h3><ul><li>Alpha controls the step size in updating the parameter w</li><li>Alpha too small means slow convergence to the minimum</li><li>Alpha too largecan lead to overshooting the minimum and never converging</li><li>It&#x27;s important to choose an appropriate learning rate for effective gradient descent</li><li>A good starting point for alpha is 0.01 and can be adjusted based on the specific problem</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="visualizing-learning-rate">Visualizing Learning Rate<a class="hash-link" href="#visualizing-learning-rate" title="Direct link to heading">â€‹</a></h3><ul><li>If the learning rate is too small, the algorithm takes many small steps to reach the minimum.</li><li>If the learning rate is too large, the algorithm takes large steps and may overshoot the minimum or oscillate around it.</li><li>It&#x27;s ideal to have a learning rate that allows the algorithm to converge efficiently without overshooting.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="learning-rate-tuning">Learning Rate Tuning<a class="hash-link" href="#learning-rate-tuning" title="Direct link to heading">â€‹</a></h3><ul><li>If the learning rate is too small, the algorithm may take a long time to converge.</li><li>If the learning rate is too large, the algorithm may fail to converge or oscillate around the minimum.</li><li>You can try different learning rates and observe the behavior of the algorithm.</li><li>If the algorithm is not converging, try reducing the learning rate.</li><li>If the algorithm is converging slowly, try increasing the learning rate.</li><li>The learning rate is a hyperparameter that needs to be tuned for each specific problem.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-2">Conclusion<a class="hash-link" href="#conclusion-2" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent updates the model parameters based on the learning rate and the derivative of the cost function.</li><li>The learning rate controls the step size in updating the parameters.</li><li>Choosing an appropriate learning rate is crucial for efficient convergence of the algorithm.</li><li>A learning rate that is too small can result in slow convergence, while a learning rate that is too large can lead to overshooting the minimum or oscillations.</li><li>Tuning the learning rate is important to find the right balance for effective gradient descent.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/3-Train the model with gradient descent.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/Regression Model"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">2 - Regression Model</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/Mutiple linear regression"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">4 - Mutiple linear regression</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#gradient-descent-for-minimizing-cost-function" class="table-of-contents__link toc-highlight">Gradient Descent for Minimizing Cost Function</a><ul><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#objective" class="table-of-contents__link toc-highlight">Objective</a></li><li><a href="#algorithm" class="table-of-contents__link toc-highlight">Algorithm</a></li><li><a href="#visualizing-gradient-descent" class="table-of-contents__link toc-highlight">Visualizing Gradient Descent</a></li><li><a href="#properties-of-gradient-descent" class="table-of-contents__link toc-highlight">Properties of Gradient Descent</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#implementing-gradient-descent-algorithm" class="table-of-contents__link toc-highlight">Implementing Gradient Descent Algorithm</a><ul><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#understanding-the-equation" class="table-of-contents__link toc-highlight">Understanding the Equation</a></li><li><a href="#updating-w-and-b" class="table-of-contents__link toc-highlight">Updating <code>w</code> and <code>b</code></a></li><li><a href="#simultaneous-update" class="table-of-contents__link toc-highlight">Simultaneous Update</a></li><li><a href="#incorrect-implementation" class="table-of-contents__link toc-highlight">Incorrect Implementation</a></li><li><a href="#conclusion-1" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#gradient-descent-intuition-and-learning-rate" class="table-of-contents__link toc-highlight">Gradient Descent Intuition and Learning Rate</a><ul><li><a href="#introduction-2" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#gradient-descent-on-one-parameter" class="table-of-contents__link toc-highlight">Gradient Descent on One Parameter</a></li><li><a href="#choosing-the-learning-rate-alpha" class="table-of-contents__link toc-highlight">Choosing the Learning Rate Alpha</a></li><li><a href="#visualizing-learning-rate" class="table-of-contents__link toc-highlight">Visualizing Learning Rate</a></li><li><a href="#learning-rate-tuning" class="table-of-contents__link toc-highlight">Learning Rate Tuning</a></li><li><a href="#conclusion-2" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs">Notes</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023</div></div></div></footer></div>
<script src="/assets/js/runtime~main.852293d6.js"></script>
<script src="/assets/js/main.026a2937.js"></script>
</body>
</html>