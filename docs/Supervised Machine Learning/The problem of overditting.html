<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Nikki&#39;s Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Nikki&#39;s Blog Atom Feed"><title data-react-helmet="true">7 - The problem of overditting | Nikki&#x27;s Blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://nikkon18.github.io//docs/Supervised Machine Learning/The problem of overditting"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="7 - The problem of overditting | Nikki&#x27;s Blog"><meta data-react-helmet="true" name="description" content="Understanding Overfitting and Underfitting"><meta data-react-helmet="true" property="og:description" content="Understanding Overfitting and Underfitting"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://nikkon18.github.io//docs/Supervised Machine Learning/The problem of overditting"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/The problem of overditting" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/The problem of overditting" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9c11c536.css">
<link rel="preload" href="/assets/js/runtime~main.852293d6.js" as="script">
<link rel="preload" href="/assets/js/main.026a2937.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Nikki&#x27;s Blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/Supervised Machine Learning/Overview of Machine learning">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/Supervised Machine Learning/Overview of Machine learning">Supervised Machine Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Overview of Machine learning">1 - Overview of Machine learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Regression Model">2 - Regression Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Train the model with gradient descent">3 - Train the model with gradient descent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Mutiple linear regression">4 - Mutiple linear regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Gradient descent in practice">5 - Gradient descent in practice</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/classification with logistic regression">6 - classification with logistic regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Supervised Machine Learning/The problem of overditting">7 - The problem of overditting</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/Advanced Learning Algorithms/Neural networks intuition">Advanced Learning Algorithms</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>7 - The problem of overditting</h1><h2 class="anchor anchorWithStickyNavbar_mojV" id="understanding-overfitting-and-underfitting">Understanding Overfitting and Underfitting<a class="hash-link" href="#understanding-overfitting-and-underfitting" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h3><ul><li>Overfitting and underfitting are common problems in machine learning algorithms.</li><li>Linear regression and logistic regression are effective algorithms but can encounter these issues.</li><li>Overfitting results in poor performance, while underfitting indicates high bias.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="overfitting">Overfitting<a class="hash-link" href="#overfitting" title="Direct link to heading">â€‹</a></h3><ul><li>Overfitting occurs when an algorithm excessively fits the training data.</li><li>It leads to poor generalization to new, unseen examples.</li><li>Overfitting is characterized by high variance in the model.</li><li>The algorithm tries too hard to fit every training example, resulting in a complex and wiggly curve.</li><li>This behavior is undesirable and hampers prediction accuracy.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="underfitting">Underfitting<a class="hash-link" href="#underfitting" title="Direct link to heading">â€‹</a></h3><ul><li>Underfitting happens when an algorithm fails to capture the underlying patterns in the training data.</li><li>The model is too simplistic to fit the training set well.</li><li>Underfitting is associated with high bias.</li><li>The algorithm has a strong preconception or bias that doesn&#x27;t align with the data.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="linear-regression-example">Linear Regression Example<a class="hash-link" href="#linear-regression-example" title="Direct link to heading">â€‹</a></h3><ul><li>Consider the task of predicting housing prices based on the size of the house.</li><li>A linear regression model can be fitted to the data.</li><li>If the linear model is too simple, it may underfit the data, resulting in poor predictions.</li><li>This model has low flexibility and doesn&#x27;t capture the non-linear relationship between size and price.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="quadratic-regression-example">Quadratic Regression Example<a class="hash-link" href="#quadratic-regression-example" title="Direct link to heading">â€‹</a></h3><ul><li>A quadratic regression model with two features (size and size squared) can fit the data better.</li><li>This model captures the non-linear relationship and improves predictions.</li><li>It strikes a balance between fitting the training set and generalizing to new examples.</li><li>Generalization refers to the ability to make accurate predictions on unseen data.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="overfitting-with-a-fourth-order-polynomial">Overfitting with a Fourth-Order Polynomial<a class="hash-link" href="#overfitting-with-a-fourth-order-polynomial" title="Direct link to heading">â€‹</a></h3><ul><li>Fitting a fourth-order polynomial to the data allows the model to pass through all training examples exactly.</li><li>Although it fits the training set perfectly, it results in a wiggly curve.</li><li>This model overfits the data and has high variance.</li><li>It is unlikely to generalize well to new examples and may produce unreliable predictions.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="goldilocks-principle">Goldilocks Principle<a class="hash-link" href="#goldilocks-principle" title="Direct link to heading">â€‹</a></h3><ul><li>The goal in machine learning is to find a model that neither underfits nor overfits the data.</li><li>Too few features lead to underfitting, while too many features cause overfitting.</li><li>Striking the right balance is crucial for optimal performance.</li><li>This principle is similar to Goldilocks and the Three Bears, where the ideal temperature is &quot;just right.&quot;</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="overfitting-in-classification">Overfitting in Classification<a class="hash-link" href="#overfitting-in-classification" title="Direct link to heading">â€‹</a></h3><ul><li>Overfitting is applicable to classification tasks as well.</li><li>A logistic regression model can be used to classify tumors as malignant or benign.</li><li>A simple linear decision boundary may underfit the data and have high bias.</li><li>Adding quadratic features allows for a more flexible decision boundary that fits the data better.</li><li>This balanced model achieves reasonable predictions and generalization.</li><li>Fitting a high-order polynomial may result in an overfitting problem with high variance.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">â€‹</a></h3><ul><li>Overfitting and underfitting are common challenges in machine learning.</li><li>Overfitting occurs when the model fits the training data too well, leading to poor generalization.</li><li>Underfitting happens when the model fails to capture the underlying patterns in the data.</li><li>Striking the right balance between the number of features and the complexity of the model is crucial.</li><li>The goal is to find a model that neither underfits nor overfits, achieving optimal performance and generalization.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="addressing-overfitting-in-machine-learning">Addressing Overfitting in Machine Learning<a class="hash-link" href="#addressing-overfitting-in-machine-learning" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-1">Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">â€‹</a></h3><ul><li>Overfitting occurs when a model fits the training data too closely, resulting in poor generalization to new examples.</li><li>There are several approaches to address overfitting in machine learning algorithms.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="option-1-collect-more-training-data">Option 1: Collect More Training Data<a class="hash-link" href="#option-1-collect-more-training-data" title="Direct link to heading">â€‹</a></h3><ul><li>Increasing the size of the training set can help reduce overfitting.</li><li>With more training examples, the algorithm can learn to fit a smoother function.</li><li>The learning algorithm will have a better chance of generalizing well to unseen data.</li><li>Collecting more data is not always feasible but can be highly effective when available.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="option-2-use-fewer-features">Option 2: Use Fewer Features<a class="hash-link" href="#option-2-use-fewer-features" title="Direct link to heading">â€‹</a></h3><ul><li>Overfitting can also occur when there are too many features relative to the amount of training data.</li><li>By reducing the number of features, the model becomes less complex and prone to overfitting.</li><li>Feature selection involves choosing the most relevant features for the prediction task.</li><li>Selecting a smaller subset of features can help mitigate overfitting.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="option-3-regularization">Option 3: Regularization<a class="hash-link" href="#option-3-regularization" title="Direct link to heading">â€‹</a></h3><ul><li>Regularization is a technique to reduce overfitting by preventing the model parameters from becoming too large.</li><li>It encourages the learning algorithm to shrink the parameter values without eliminating features entirely.</li><li>By reducing the impact of some features, regularization helps prevent overfitting while retaining useful information.</li><li>Regularization is particularly useful in models with high-dimensional feature spaces.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-1">Conclusion<a class="hash-link" href="#conclusion-1" title="Direct link to heading">â€‹</a></h3><ul><li>Overfitting is a common challenge in machine learning, but there are effective strategies to address it.</li><li>Collecting more training data, using fewer features, and applying regularization are key approaches.</li><li>Each approach has its advantages and considerations, and the choice depends on the specific problem at hand.</li><li>Understanding and mitigating overfitting is crucial for building models that generalize well and make accurate predictions.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="regularization-in-machine-learning">Regularization in Machine Learning<a class="hash-link" href="#regularization-in-machine-learning" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-2">Introduction<a class="hash-link" href="#introduction-2" title="Direct link to heading">â€‹</a></h3><ul><li>Regularization helps reduce overfitting in machine learning models.</li><li>It involves penalizing large parameter values to promote simpler models.</li><li>Regularization is implemented by modifying the cost function of the learning algorithm.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="example-motivation">Example Motivation<a class="hash-link" href="#example-motivation" title="Direct link to heading">â€‹</a></h3><ul><li>A quadratic function provides a good fit to the data, while a high-order polynomial overfits.</li><li>Introducing a modified cost function with additional terms penalizes large parameter values.</li><li>By minimizing this function, the model is encouraged to have smaller parameter values.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="regularization-principle">Regularization Principle<a class="hash-link" href="#regularization-principle" title="Direct link to heading">â€‹</a></h3><ul><li>Smaller parameter values lead to simpler models and reduce overfitting.</li><li>Regularization penalizes all parameter values to achieve a smoother, less complex function.</li><li>The regularization term is added to the original cost function, striking a balance between fitting the training data and keeping parameter values small.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="regularization-implementation">Regularization Implementation<a class="hash-link" href="#regularization-implementation" title="Direct link to heading">â€‹</a></h3><ul><li>Regularization is typically applied to all parameters, as it&#x27;s challenging to determine the most important ones.</li><li>The regularization parameter, lambda, controls the strength of regularization.</li><li>The regularization term is calculated as lambda times the sum of squared parameter values.</li><li>To ease parameter selection, lambda is divided by 2m, scaling both terms equally.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="effect-of-lambda">Effect of Lambda<a class="hash-link" href="#effect-of-lambda" title="Direct link to heading">â€‹</a></h3><ul><li>Choosing lambda = 0 leads to overfitting, with a complex model that fits the training data too closely.</li><li>Setting lambda to a very large value (e.g., 10^10) heavily weights the regularization term.</li><li>Large lambda values result in underfitting, where the model becomes too simple.</li><li>The appropriate lambda value balances between fitting the data and keeping parameter values small.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="summary-of-regularization">Summary of Regularization<a class="hash-link" href="#summary-of-regularization" title="Direct link to heading">â€‹</a></h3><ul><li>Regularization adds a regularization term to the cost function to reduce overfitting.</li><li>It promotes simpler models by penalizing large parameter values.</li><li>The regularization parameter, lambda, controls the trade-off between fitting the data and regularization.</li><li>Proper selection of lambda leads to a well-balanced model that avoids overfitting.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-2">Conclusion<a class="hash-link" href="#conclusion-2" title="Direct link to heading">â€‹</a></h3><ul><li>Regularization is a powerful technique to address overfitting in machine learning.</li><li>It helps create simpler models by penalizing large parameter values.</li><li>By finding the right balance between fitting the data and regularization, models can generalize better.</li><li>Regularization can be applied to various machine learning algorithms like linear regression and logistic regression.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="notes-on-regularized-linear-regression-with-gradient-descent">Notes on Regularized Linear Regression with Gradient Descent<a class="hash-link" href="#notes-on-regularized-linear-regression-with-gradient-descent" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-3">Introduction<a class="hash-link" href="#introduction-3" title="Direct link to heading">â€‹</a></h3><ul><li>Regularized linear regression helps reduce overfitting in the model.</li><li>The cost function for regularized linear regression consists of the squared error cost function and an additional regularization term.</li><li>The goal is to find parameters w and b that minimize the regularized cost function.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-for-regularized-linear-regression">Gradient Descent for Regularized Linear Regression<a class="hash-link" href="#gradient-descent-for-regularized-linear-regression" title="Direct link to heading">â€‹</a></h3><ul><li>The gradient descent algorithm is used to update the parameters w and b iteratively.</li><li>The update rule for w_j, where j is from 1 to n, is given by:<ul><li>w_j := w_j(1 - alpha(lambda/m)) - (alpha/m)(sum((h_theta(x_i) - y_i)x_ij) + lambda<!-- -->*<!-- -->w_j)</li></ul></li><li>The update rule for b remains the same as before.</li><li>Alpha is the learning rate, lambda is the regularization parameter, and m is the training set size.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="intuition-behind-the-update-rule">Intuition Behind the Update Rule<a class="hash-link" href="#intuition-behind-the-update-rule" title="Direct link to heading">â€‹</a></h3><ul><li>The update rule for w_j can be rewritten as:<ul><li>w_j := w_j(1 - alpha(lambda/m)) - (alpha/m)(sum((h_theta(x_i) - y_i)x_ij)</li></ul></li><li>The first term in parentheses, 1 - alpha(lambda/m), is slightly less than 1 and causes the value of w_j to shrink on each iteration.</li><li>The second term is the usual update for unregularized linear regression.</li><li>Regularization shrinks the value of w_j, promoting a simpler model.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="derivatives-for-regularized-linear-regression">Derivatives for Regularized Linear Regression<a class="hash-link" href="#derivatives-for-regularized-linear-regression" title="Direct link to heading">â€‹</a></h3><ul><li>The derivative of the cost function with respect to w_j is given by:<ul><li>1/m <!-- -->*<!-- --> sum((h_theta(x_i) - y_i)x_ij) + (lambda/m)<!-- -->*<!-- -->w_j</li></ul></li><li>The derivative of the cost function with respect to b remains the same as before.</li><li>The regularization term affects the derivative of w_j, adding an additional term.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-3">Conclusion<a class="hash-link" href="#conclusion-3" title="Direct link to heading">â€‹</a></h3><ul><li>Regularized linear regression uses gradient descent to update the parameters.</li><li>The update rule includes a regularization term that shrinks the parameter values on each iteration.</li><li>Derivatives of the cost function are modified to include the regularization term.</li><li>Regularization helps reduce overfitting and improves model performance, especially with many features and a small training set.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="regularized-logistic-regression">Regularized Logistic Regression<a class="hash-link" href="#regularized-logistic-regression" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-4">Introduction<a class="hash-link" href="#introduction-4" title="Direct link to heading">â€‹</a></h3><ul><li>Regularized logistic regression helps prevent overfitting in the model.</li><li>When training logistic regression with many features, there is a higher risk of overfitting.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="the-cost-function-for-regularized-logistic-regression">The Cost Function for Regularized Logistic Regression<a class="hash-link" href="#the-cost-function-for-regularized-logistic-regression" title="Direct link to heading">â€‹</a></h3><ul><li>The cost function for logistic regression includes a regularization term.</li><li>The regularization term penalizes the parameters w_1, w_2, ..., w_n to prevent them from being too large.</li><li>The cost function for regularized logistic regression is given by:<ul><li>J(w, b) = (1/m) <em> sum((-y_i </em> log(f(x<em>i))) - ((1 - y_i) </em> log(1 - f(x<em>i)))) + (lambda/(2m)) </em> sum(w_j^2)</li><li>f(x_i) is the logistic function applied to z.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-for-regularized-logistic-regression">Gradient Descent for Regularized Logistic Regression<a class="hash-link" href="#gradient-descent-for-regularized-logistic-regression" title="Direct link to heading">â€‹</a></h3><ul><li>The update rules for gradient descent in regularized logistic regression are similar to regularized linear regression.</li><li>The update rule for w_j is given by:<ul><li>w_j := w_j(1 - alpha(lambda/m)) - (alpha/m)(sum((f(x_i) - y_i) <!-- -->*<!-- --> x_ij)) + (lambda/m)<!-- -->*<!-- -->w_j</li></ul></li><li>The update rule for b remains the same as before.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="implementation-of-regularized-logistic-regression">Implementation of Regularized Logistic Regression<a class="hash-link" href="#implementation-of-regularized-logistic-regression" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent is used to minimize the cost function.</li><li>The update rules for w_j and b are applied iteratively.</li><li>The regularization term is added to the derivative of the cost function with respect to w_j.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-4">Conclusion<a class="hash-link" href="#conclusion-4" title="Direct link to heading">â€‹</a></h3><ul><li>Regularized logistic regression helps prevent overfitting in models with many features.</li><li>The cost function includes a regularization term to penalize large parameter values.</li><li>Gradient descent is used to minimize the cost function and update the parameters.</li><li>Regularization is an important technique in machine learning to improve model performance and prevent overfitting.</li><li>Understanding linear regression, logistic regression, and regularization are valuable skills in the real-world applications of machine learning.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/7-The problem of overditting.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/classification with logistic regression"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">6 - classification with logistic regression</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/Advanced Learning Algorithms/Neural networks intuition"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">1 - Neural networks intuition</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#understanding-overfitting-and-underfitting" class="table-of-contents__link toc-highlight">Understanding Overfitting and Underfitting</a><ul><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#overfitting" class="table-of-contents__link toc-highlight">Overfitting</a></li><li><a href="#underfitting" class="table-of-contents__link toc-highlight">Underfitting</a></li><li><a href="#linear-regression-example" class="table-of-contents__link toc-highlight">Linear Regression Example</a></li><li><a href="#quadratic-regression-example" class="table-of-contents__link toc-highlight">Quadratic Regression Example</a></li><li><a href="#overfitting-with-a-fourth-order-polynomial" class="table-of-contents__link toc-highlight">Overfitting with a Fourth-Order Polynomial</a></li><li><a href="#goldilocks-principle" class="table-of-contents__link toc-highlight">Goldilocks Principle</a></li><li><a href="#overfitting-in-classification" class="table-of-contents__link toc-highlight">Overfitting in Classification</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#addressing-overfitting-in-machine-learning" class="table-of-contents__link toc-highlight">Addressing Overfitting in Machine Learning</a><ul><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#option-1-collect-more-training-data" class="table-of-contents__link toc-highlight">Option 1: Collect More Training Data</a></li><li><a href="#option-2-use-fewer-features" class="table-of-contents__link toc-highlight">Option 2: Use Fewer Features</a></li><li><a href="#option-3-regularization" class="table-of-contents__link toc-highlight">Option 3: Regularization</a></li><li><a href="#conclusion-1" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#regularization-in-machine-learning" class="table-of-contents__link toc-highlight">Regularization in Machine Learning</a><ul><li><a href="#introduction-2" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#example-motivation" class="table-of-contents__link toc-highlight">Example Motivation</a></li><li><a href="#regularization-principle" class="table-of-contents__link toc-highlight">Regularization Principle</a></li><li><a href="#regularization-implementation" class="table-of-contents__link toc-highlight">Regularization Implementation</a></li><li><a href="#effect-of-lambda" class="table-of-contents__link toc-highlight">Effect of Lambda</a></li><li><a href="#summary-of-regularization" class="table-of-contents__link toc-highlight">Summary of Regularization</a></li><li><a href="#conclusion-2" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#notes-on-regularized-linear-regression-with-gradient-descent" class="table-of-contents__link toc-highlight">Notes on Regularized Linear Regression with Gradient Descent</a><ul><li><a href="#introduction-3" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#gradient-descent-for-regularized-linear-regression" class="table-of-contents__link toc-highlight">Gradient Descent for Regularized Linear Regression</a></li><li><a href="#intuition-behind-the-update-rule" class="table-of-contents__link toc-highlight">Intuition Behind the Update Rule</a></li><li><a href="#derivatives-for-regularized-linear-regression" class="table-of-contents__link toc-highlight">Derivatives for Regularized Linear Regression</a></li><li><a href="#conclusion-3" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#regularized-logistic-regression" class="table-of-contents__link toc-highlight">Regularized Logistic Regression</a><ul><li><a href="#introduction-4" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#the-cost-function-for-regularized-logistic-regression" class="table-of-contents__link toc-highlight">The Cost Function for Regularized Logistic Regression</a></li><li><a href="#gradient-descent-for-regularized-logistic-regression" class="table-of-contents__link toc-highlight">Gradient Descent for Regularized Logistic Regression</a></li><li><a href="#implementation-of-regularized-logistic-regression" class="table-of-contents__link toc-highlight">Implementation of Regularized Logistic Regression</a></li><li><a href="#conclusion-4" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs">Notes</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023</div></div></div></footer></div>
<script src="/assets/js/runtime~main.852293d6.js"></script>
<script src="/assets/js/main.026a2937.js"></script>
</body>
</html>