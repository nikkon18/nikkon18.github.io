<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Nikki&#39;s Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Nikki&#39;s Blog Atom Feed"><title data-react-helmet="true">6 - classification with logistic regression | Nikki&#x27;s Blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://nikkon18.github.io//docs/Supervised Machine Learning/classification with logistic regression"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="6 - classification with logistic regression | Nikki&#x27;s Blog"><meta data-react-helmet="true" name="description" content="Logistic Regression"><meta data-react-helmet="true" property="og:description" content="Logistic Regression"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://nikkon18.github.io//docs/Supervised Machine Learning/classification with logistic regression"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/classification with logistic regression" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Supervised Machine Learning/classification with logistic regression" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9c11c536.css">
<link rel="preload" href="/assets/js/runtime~main.852293d6.js" as="script">
<link rel="preload" href="/assets/js/main.026a2937.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Nikki&#x27;s Blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/Supervised Machine Learning/Overview of Machine learning">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/Supervised Machine Learning/Overview of Machine learning">Supervised Machine Learning</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Overview of Machine learning">1 - Overview of Machine learning</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Regression Model">2 - Regression Model</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Train the model with gradient descent">3 - Train the model with gradient descent</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Mutiple linear regression">4 - Mutiple linear regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/Gradient descent in practice">5 - Gradient descent in practice</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Supervised Machine Learning/classification with logistic regression">6 - classification with logistic regression</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Supervised Machine Learning/The problem of overditting">7 - The problem of overditting</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/Advanced Learning Algorithms/Neural networks intuition">Advanced Learning Algorithms</a></div></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>6 - classification with logistic regression</h1><h2 class="anchor anchorWithStickyNavbar_mojV" id="logistic-regression">Logistic Regression<a class="hash-link" href="#logistic-regression" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="classification-problems">Classification Problems<a class="hash-link" href="#classification-problems" title="Direct link to heading">â€‹</a></h3><ul><li>Linear regression is not suitable for classification problems</li><li>Classification predicts discrete values, not continuous</li><li>Examples: spam email detection, fraudulent transaction detection, tumor classification</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="binary-classification">Binary Classification<a class="hash-link" href="#binary-classification" title="Direct link to heading">â€‹</a></h3><ul><li>Binary classification: output variable y has two possible values (0 or 1)</li><li>Classifying between two possible classes or categories</li><li>Common designations: no/yes, false/true, 0/1</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="positive-and-negative-classes">Positive and Negative Classes<a class="hash-link" href="#positive-and-negative-classes" title="Direct link to heading">â€‹</a></h3><ul><li>False/zero class: negative class</li><li>True/one class: positive class</li><li>Example: spam classification<ul><li>Non-spam email: negative example (y = 0)</li><li>Spam email: positive training example (y = 1)</li></ul></li><li>Negative and positive do not imply good versus bad, but the absence or presence of something</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="building-a-classification-algorithm">Building a Classification Algorithm<a class="hash-link" href="#building-a-classification-algorithm" title="Direct link to heading">â€‹</a></h3><ul><li>Example: classifying tumor malignancy</li><li>Training set: tumor size (x) and corresponding label (y)</li><li>Linear regression can be attempted, but it predicts continuous values</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="threshold-and-decision-boundary">Threshold and Decision Boundary<a class="hash-link" href="#threshold-and-decision-boundary" title="Direct link to heading">â€‹</a></h3><ul><li>Applying a threshold (e.g., 0.5) to distinguish categories</li><li>Threshold intersects the best-fit line, creating a decision boundary</li><li>Values below the threshold: predicted as 0</li><li>Values equal to or above the threshold: predicted as 1</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="limitations-of-linear-regression-for-classification">Limitations of Linear Regression for Classification<a class="hash-link" href="#limitations-of-linear-regression-for-classification" title="Direct link to heading">â€‹</a></h3><ul><li>Addition of a new training example can significantly shift the decision boundary</li><li>Linear regression fails to maintain consistent classification with new data</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="logistic-regression-1">Logistic Regression<a class="hash-link" href="#logistic-regression-1" title="Direct link to heading">â€‹</a></h3><ul><li>Logistic regression: algorithm for binary classification</li><li>Output value between 0 and 1</li><li>Solves binary classification problems effectively</li><li>Name can be confusing as it contains &quot;regression&quot; while used for classification</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">â€‹</a></h3><ul><li>Logistic regression is the preferred algorithm for binary classification</li><li>Linear regression is not suitable for classification due to its limitations</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="logistic-regression-2">Logistic Regression<a class="hash-link" href="#logistic-regression-2" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h3><ul><li>Logistic regression is a widely used classification algorithm</li><li>Used for classifying whether a tumor is malignant or benign</li><li>Positive class (malignant): labeled as 1, represented by &quot;yes&quot;</li><li>Negative class (benign): labeled as 0, represented by &quot;no&quot;</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="sigmoid-function">Sigmoid Function<a class="hash-link" href="#sigmoid-function" title="Direct link to heading">â€‹</a></h3><ul><li>Sigmoid function (logistic function) is important in logistic regression</li><li>Outputs values between 0 and 1</li><li>Denoted as g(z), where z is a linear function of features (wx + b)</li><li>Formula: g(z) = 1 / (1 + e^(-z))</li><li>e is a mathematical constant (~2.7)</li><li>Sigmoid function has an S-shaped curve</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="logistic-regression-model">Logistic Regression Model<a class="hash-link" href="#logistic-regression-model" title="Direct link to heading">â€‹</a></h3><ul><li>Combining the linear function (wx + b) and sigmoid function (g(z))</li><li>Logistic regression model: f(x) = g(wx + b)</li><li>Outputs a value between 0 and 1</li><li>Interpretation: probability of y = 1 given input x</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="interpreting-logistic-regression-output">Interpreting Logistic Regression Output<a class="hash-link" href="#interpreting-logistic-regression-output" title="Direct link to heading">â€‹</a></h3><ul><li>Output probability represents the chance of y = 1</li><li>Example: if model outputs 0.7 for a tumor size x</li><li>Interpretation: 70% chance that the tumor is malignant (y = 1)</li><li>Complementary probability: 30% chance that the tumor is benign (y = 0)</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-1">Conclusion<a class="hash-link" href="#conclusion-1" title="Direct link to heading">â€‹</a></h3><ul><li>Logistic regression is a widely used classification algorithm</li><li>Applies the sigmoid function to a linear function of features</li><li>Output represents the probability of the positive class (y = 1)</li><li>Complementary probability represents the probability of the negative class (y = 0)</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="logistic-regression-understanding-decision-boundaries">Logistic Regression: Understanding Decision Boundaries<a class="hash-link" href="#logistic-regression-understanding-decision-boundaries" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-1">Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">â€‹</a></h3><ul><li>Recap of logistic regression model</li><li>Two steps: compute z = wx + b, apply sigmoid function g(z)</li><li>Formula: f(x) = g(wx + b)</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="decision-boundary">Decision Boundary<a class="hash-link" href="#decision-boundary" title="Direct link to heading">â€‹</a></h3><ul><li>Defining when to predict y = 1 or y = 0</li><li>Threshold approach: set threshold (e.g., 0.5)</li><li>If f(x) &gt;= 0.5, predict y = 1</li><li>If f(x) &lt; 0.5, predict y = 0</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="deriving-decision-boundary">Deriving Decision Boundary<a class="hash-link" href="#deriving-decision-boundary" title="Direct link to heading">â€‹</a></h3><ul><li>f(x) = g(z), so f(x) &gt;= 0.5 if g(z) &gt;= 0.5</li><li>g(z) &gt;= 0.5 when z &gt;= 0</li><li>z = wx + b, so z &gt;= 0 when wx + b &gt;= 0</li><li>Decision boundary: wx + b = 0</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="visualizing-decision-boundary">Visualizing Decision Boundary<a class="hash-link" href="#visualizing-decision-boundary" title="Direct link to heading">â€‹</a></h3><ul><li>Example: two features (x1, x2)</li><li>Training set with positive (y = 1) and negative (y = 0) examples</li><li>Decision boundary equation: wx1 + wx2 + b = 0</li><li>Example parameters: w1 = 1, w2 = 1, b = -3</li><li>Decision boundary: x1 + x2 - 3 = 0</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="non-linear-decision-boundaries">Non-Linear Decision Boundaries<a class="hash-link" href="#non-linear-decision-boundaries" title="Direct link to heading">â€‹</a></h3><ul><li>Polynomial features in logistic regression</li><li>Example with z = w1x1^2 + w2x2^2 + b</li><li>Decision boundary: x1^2 + x2^2 = 1 (circle)</li><li>Complex decision boundaries possible with higher-order polynomials</li><li>Logistic regression can fit complex data</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="linear-decision-boundaries">Linear Decision Boundaries<a class="hash-link" href="#linear-decision-boundaries" title="Direct link to heading">â€‹</a></h3><ul><li>Without higher-order polynomials, decision boundary is linear</li><li>Decision boundary will always be a straight line</li><li>Features x1, x2, x3, etc. result in linear decision boundary</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-2">Conclusion<a class="hash-link" href="#conclusion-2" title="Direct link to heading">â€‹</a></h3><ul><li>Logistic regression uses decision boundary to make predictions</li><li>Decision boundary determined by parameters (w) and bias (b)</li><li>Can be linear or non-linear depending on features and polynomials</li></ul><p>In conclusion, logistic regression models use decision boundaries to predict whether an instance belongs to a certain class. The decision boundary is determined by the parameters (w) and bias (b) of the logistic regression model. It can be linear or non-linear, depending on the features and the use of polynomial terms. Logistic regression is capable of fitting complex data by incorporating higher-order polynomials. However, if only linear features are used, the decision boundary will always be a straight line. Understanding decision boundaries helps in visualizing how logistic regression makes predictions and how different models can be obtained by varying the parameters and features.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="cost-function-for-logistic-regression">Cost Function for Logistic Regression<a class="hash-link" href="#cost-function-for-logistic-regression" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-2">Introduction<a class="hash-link" href="#introduction-2" title="Direct link to heading">â€‹</a></h3><ul><li>The cost function measures how well a set of parameters fits the training data.</li><li>The squared error cost function is not ideal for logistic regression.</li><li>We need a different cost function that can help us choose better parameters.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="logistic-regression-model-1">Logistic Regression Model<a class="hash-link" href="#logistic-regression-model-1" title="Direct link to heading">â€‹</a></h3><ul><li>Training set consists of rows representing patient visits and diagnoses.</li><li>Each training example has n features (e.g., tumor size, age).</li><li>Target label y takes values 0 or 1.</li><li>Logistic regression model: f(x) = 1 / (1 + e^(-wx + b))</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="squared-error-cost-function-for-linear-regression">Squared Error Cost Function for Linear Regression<a class="hash-link" href="#squared-error-cost-function-for-linear-regression" title="Direct link to heading">â€‹</a></h3><ul><li>Cost function for linear regression: J(w, b) = (1/2m) <!-- -->*<!-- --> âˆ‘(f(x) - y)^2</li><li>This cost function is convex and suitable for linear regression.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="non-convexity-of-squared-error-cost-function-for-logistic-regression">Non-Convexity of Squared Error Cost Function for Logistic Regression<a class="hash-link" href="#non-convexity-of-squared-error-cost-function-for-logistic-regression" title="Direct link to heading">â€‹</a></h3><ul><li>Using squared error cost function with logistic regression leads to a non-convex cost function.</li><li>Gradient descent may get stuck in local minima.</li><li>We need a convex cost function to guarantee convergence to the global minimum.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="redefining-the-cost-function-for-logistic-regression">Redefining the Cost Function for Logistic Regression<a class="hash-link" href="#redefining-the-cost-function-for-logistic-regression" title="Direct link to heading">â€‹</a></h3><ul><li>Modify the cost function to ensure convexity.</li><li>Introduce a loss function L(f(x), y) for a single training example.</li><li>Define the loss function for logistic regression:<ul><li>If y = 1, loss = -log(f(x))</li><li>If y = 0, loss = -log(1 - f(x))</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="analysis-of-loss-function-for-y--1">Analysis of Loss Function for y = 1<a class="hash-link" href="#analysis-of-loss-function-for-y--1" title="Direct link to heading">â€‹</a></h3><ul><li>Plotting the loss function for y = 1:<ul><li>Logarithm of f(x) is a convex curve intersecting the horizontal axis at f = 1.</li><li>Negative log of f(x) is a flipped curve that incentivizes accurate predictions.</li><li>Higher loss for lower predicted values when true label is 1.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="analysis-of-loss-function-for-y--0">Analysis of Loss Function for y = 0<a class="hash-link" href="#analysis-of-loss-function-for-y--0" title="Direct link to heading">â€‹</a></h3><ul><li>Plotting the loss function for y = 0:<ul><li>Loss is negative log of 1 - f(x).</li><li>Higher loss as predicted value moves away from true label 0.</li><li>Loss approaches infinity as prediction approaches 1.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-3">Conclusion<a class="hash-link" href="#conclusion-3" title="Direct link to heading">â€‹</a></h3><ul><li>Squared error cost function is unsuitable for logistic regression.</li><li>New loss function ensures convexity of the cost function.</li><li>Gradient descent can reliably find the global minimum.</li><li>The cost function is an average of the loss function on all training examples.</li><li>In the next video, we&#x27;ll define the overall cost function for the entire training set and simplify its notation.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="lecture-notes-logistic-regression">Lecture Notes: Logistic Regression<a class="hash-link" href="#lecture-notes-logistic-regression" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-3">Introduction<a class="hash-link" href="#introduction-3" title="Direct link to heading">â€‹</a></h3><ul><li>Logistic regression is a model used for classification tasks.</li><li>Parameters w and b need to be determined to fit the model.</li><li>The goal is to minimize the cost function J(w, b) by using gradient descent.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="finding-the-parameters">Finding the Parameters<a class="hash-link" href="#finding-the-parameters" title="Direct link to heading">â€‹</a></h3><ul><li>The parameters w and b are chosen to make predictions or estimate the probability of a label being one.</li><li>The cost function J(w, b) is minimized using gradient descent.</li><li>Gradient descent involves updating parameters iteratively: w = w - Î± <em> âˆ‚J/âˆ‚w and b = b - Î± </em> âˆ‚J/âˆ‚b.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="derivatives-of-the-cost-function">Derivatives of the Cost Function<a class="hash-link" href="#derivatives-of-the-cost-function" title="Direct link to heading">â€‹</a></h3><ul><li>The derivative of J with respect to w<em>j is given by 1/m </em> âˆ‘(f - y) _ x_j.</li><li>f is the predicted value, y is the label, x_j is the j-th feature, and m is the number of training examples.</li><li>The derivative of J with respect to b is given by 1/m <!-- -->*<!-- --> âˆ‘(f - y).</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-for-logistic-regression">Gradient Descent for Logistic Regression<a class="hash-link" href="#gradient-descent-for-logistic-regression" title="Direct link to heading">â€‹</a></h3><ul><li>The derivative expressions are plugged into the gradient descent update equations.</li><li>Logistic regression and linear regression share similar update equations, but the definition of f(x) differs.</li><li>In logistic regression, f(x) is the sigmoid function applied to wx + b.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="convergence-and-vectorization">Convergence and Vectorization<a class="hash-link" href="#convergence-and-vectorization" title="Direct link to heading">â€‹</a></h3><ul><li>Similar to linear regression, monitoring convergence is essential for gradient descent in logistic regression.</li><li>Vectorization can be used to speed up the computation of gradient descent.</li><li>Feature scaling, which scales features to similar ranges, can aid convergence in logistic regression.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/6-classification with logistic regression.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/Gradient descent in practice"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">5 - Gradient descent in practice</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/Supervised Machine Learning/The problem of overditting"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">7 - The problem of overditting</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#logistic-regression" class="table-of-contents__link toc-highlight">Logistic Regression</a><ul><li><a href="#classification-problems" class="table-of-contents__link toc-highlight">Classification Problems</a></li><li><a href="#binary-classification" class="table-of-contents__link toc-highlight">Binary Classification</a></li><li><a href="#positive-and-negative-classes" class="table-of-contents__link toc-highlight">Positive and Negative Classes</a></li><li><a href="#building-a-classification-algorithm" class="table-of-contents__link toc-highlight">Building a Classification Algorithm</a></li><li><a href="#threshold-and-decision-boundary" class="table-of-contents__link toc-highlight">Threshold and Decision Boundary</a></li><li><a href="#limitations-of-linear-regression-for-classification" class="table-of-contents__link toc-highlight">Limitations of Linear Regression for Classification</a></li><li><a href="#logistic-regression-1" class="table-of-contents__link toc-highlight">Logistic Regression</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#logistic-regression-2" class="table-of-contents__link toc-highlight">Logistic Regression</a><ul><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#sigmoid-function" class="table-of-contents__link toc-highlight">Sigmoid Function</a></li><li><a href="#logistic-regression-model" class="table-of-contents__link toc-highlight">Logistic Regression Model</a></li><li><a href="#interpreting-logistic-regression-output" class="table-of-contents__link toc-highlight">Interpreting Logistic Regression Output</a></li><li><a href="#conclusion-1" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#logistic-regression-understanding-decision-boundaries" class="table-of-contents__link toc-highlight">Logistic Regression: Understanding Decision Boundaries</a><ul><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#decision-boundary" class="table-of-contents__link toc-highlight">Decision Boundary</a></li><li><a href="#deriving-decision-boundary" class="table-of-contents__link toc-highlight">Deriving Decision Boundary</a></li><li><a href="#visualizing-decision-boundary" class="table-of-contents__link toc-highlight">Visualizing Decision Boundary</a></li><li><a href="#non-linear-decision-boundaries" class="table-of-contents__link toc-highlight">Non-Linear Decision Boundaries</a></li><li><a href="#linear-decision-boundaries" class="table-of-contents__link toc-highlight">Linear Decision Boundaries</a></li><li><a href="#conclusion-2" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#cost-function-for-logistic-regression" class="table-of-contents__link toc-highlight">Cost Function for Logistic Regression</a><ul><li><a href="#introduction-2" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#logistic-regression-model-1" class="table-of-contents__link toc-highlight">Logistic Regression Model</a></li><li><a href="#squared-error-cost-function-for-linear-regression" class="table-of-contents__link toc-highlight">Squared Error Cost Function for Linear Regression</a></li><li><a href="#non-convexity-of-squared-error-cost-function-for-logistic-regression" class="table-of-contents__link toc-highlight">Non-Convexity of Squared Error Cost Function for Logistic Regression</a></li><li><a href="#redefining-the-cost-function-for-logistic-regression" class="table-of-contents__link toc-highlight">Redefining the Cost Function for Logistic Regression</a></li><li><a href="#analysis-of-loss-function-for-y--1" class="table-of-contents__link toc-highlight">Analysis of Loss Function for y = 1</a></li><li><a href="#analysis-of-loss-function-for-y--0" class="table-of-contents__link toc-highlight">Analysis of Loss Function for y = 0</a></li><li><a href="#conclusion-3" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#lecture-notes-logistic-regression" class="table-of-contents__link toc-highlight">Lecture Notes: Logistic Regression</a><ul><li><a href="#introduction-3" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#finding-the-parameters" class="table-of-contents__link toc-highlight">Finding the Parameters</a></li><li><a href="#derivatives-of-the-cost-function" class="table-of-contents__link toc-highlight">Derivatives of the Cost Function</a></li><li><a href="#gradient-descent-for-logistic-regression" class="table-of-contents__link toc-highlight">Gradient Descent for Logistic Regression</a></li><li><a href="#convergence-and-vectorization" class="table-of-contents__link toc-highlight">Convergence and Vectorization</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs">Notes</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023</div></div></div></footer></div>
<script src="/assets/js/runtime~main.852293d6.js"></script>
<script src="/assets/js/main.026a2937.js"></script>
</body>
</html>