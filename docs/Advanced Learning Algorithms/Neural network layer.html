<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Nikki&#39;s Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Nikki&#39;s Blog Atom Feed"><title data-react-helmet="true">2 - Neural network layer | Nikki&#x27;s Blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://nikkon18.github.io//docs/Advanced Learning Algorithms/Neural network layer"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="2 - Neural network layer | Nikki&#x27;s Blog"><meta data-react-helmet="true" name="description" content="Neural Networks layer"><meta data-react-helmet="true" property="og:description" content="Neural Networks layer"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://nikkon18.github.io//docs/Advanced Learning Algorithms/Neural network layer"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Advanced Learning Algorithms/Neural network layer" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Advanced Learning Algorithms/Neural network layer" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9c11c536.css">
<link rel="preload" href="/assets/js/runtime~main.852293d6.js" as="script">
<link rel="preload" href="/assets/js/main.026a2937.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Nikki&#x27;s Blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/Supervised Machine Learning/Overview of Machine learning">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/Supervised Machine Learning/Overview of Machine learning">Supervised Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/Advanced Learning Algorithms/Neural networks intuition">Advanced Learning Algorithms</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Neural networks intuition">1 - Neural networks intuition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Advanced Learning Algorithms/Neural network layer">2 - Neural network layer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/TensorFlow implementation">3 - TensorFlow implementation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Neural network implementation in Python">4-Neural network implementation in Python</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Speculations on AGI">5-Paths to Artificial General Intelligence (AGI)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Training a Neural Network in TensorFlow">6-Neural Network Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Activation Functions">7-Activation Functions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Multiclass Classification">8-Multiclass Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Advice for applying machine learning">9-Advice for applying machine learning</a></li></ul></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>2 - Neural network layer</h1><h2 class="anchor anchorWithStickyNavbar_mojV" id="neural-networks-layer">Neural Networks layer<a class="hash-link" href="#neural-networks-layer" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h3><ul><li>Neural networks consist of layers of neurons</li><li>Layers can be combined to form larger neural networks</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="layer-of-neurons">Layer of Neurons<a class="hash-link" href="#layer-of-neurons" title="Direct link to heading">â€‹</a></h3><ul><li>Example: Demand prediction with input features and output neuron</li><li>Hidden layer contains three neurons</li></ul><h4 class="anchor anchorWithStickyNavbar_mojV" id="neuron-computation">Neuron Computation<a class="hash-link" href="#neuron-computation" title="Direct link to heading">â€‹</a></h4><ul><li>Neurons in hidden layer perform logistic regression</li><li>Each neuron has parameters w and b</li><li>Activation value a is computed using logistic function g</li></ul><h4 class="anchor anchorWithStickyNavbar_mojV" id="neuron-outputs">Neuron Outputs<a class="hash-link" href="#neuron-outputs" title="Direct link to heading">â€‹</a></h4><ul><li>Each neuron in hidden layer produces an activation value</li><li>Activation values form a vector passed to the output layer</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="layer-indexing">Layer Indexing<a class="hash-link" href="#layer-indexing" title="Direct link to heading">â€‹</a></h3><ul><li>Different layers in neural networks are numbered</li><li>Hidden layer is layer 1, output layer is layer 2</li><li>Input layer can be called layer 0</li><li>Superscripts in square brackets denote layer indexing</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="layer-1-computation">Layer 1 Computation<a class="hash-link" href="#layer-1-computation" title="Direct link to heading">â€‹</a></h3><ul><li>Layer 1 takes input from the input layer</li><li>Neurons in layer 1 compute activation values using parameters w and b</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="layer-2-computation">Layer 2 Computation<a class="hash-link" href="#layer-2-computation" title="Direct link to heading">â€‹</a></h3><ul><li>Layer 2 takes input from layer 1</li><li>Output layer contains a single neuron</li><li>Neuron computes activation value using sigmoid function g</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="binary-prediction">Binary Prediction<a class="hash-link" href="#binary-prediction" title="Direct link to heading">â€‹</a></h3><ul><li>Optional step: Thresholding for binary prediction</li><li>Output value can be thresholded at 0.5 for binary classification</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">â€‹</a></h3><ul><li>Neural networks consist of layers of neurons</li><li>Each layer performs computations using parameters and activation functions</li><li>Layers are combined to build complex neural network models</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="complex-neural-networks">Complex neural Networks<a class="hash-link" href="#complex-neural-networks" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-1">Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">â€‹</a></h3><ul><li>Neural networks consist of layers of neurons.</li><li>Each layer performs computations using parameters and activation functions.</li><li>Layers are combined to build complex neural network models.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="neural-network-layers">Neural Network Layers<a class="hash-link" href="#neural-network-layers" title="Direct link to heading">â€‹</a></h3><ul><li>Neural network layers take input vectors and produce output vectors.</li><li>Layers are numbered, with the input layer as Layer 0.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="layer-computation">Layer Computation<a class="hash-link" href="#layer-computation" title="Direct link to heading">â€‹</a></h3><ul><li>Neurons in a layer compute activation values using parameters and activation functions.</li><li>Activation values are computed for each neuron in the layer.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="layer-indexing-1">Layer Indexing<a class="hash-link" href="#layer-indexing-1" title="Direct link to heading">â€‹</a></h3><ul><li>Layers are indexed using superscripts in square brackets.</li><li>Hidden layers are numbered starting from 1.</li><li>Output layer is the final layer of the network.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="layer-3-computation">Layer 3 Computation<a class="hash-link" href="#layer-3-computation" title="Direct link to heading">â€‹</a></h3><ul><li>Layer 3 takes input from the previous layer (Layer 2).</li><li>Neurons in Layer 3 compute activation values using parameters and activation functions.</li><li>Activation values form a vector, denoted as a_3.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="general-equation-for-layer-activation">General Equation for Layer Activation<a class="hash-link" href="#general-equation-for-layer-activation" title="Direct link to heading">â€‹</a></h3><ul><li>The activation of a unit j in layer l is computed using the sigmoid function.</li><li>The activation is calculated as the sigmoid of the dot product of weights and the activation values of the previous layer, plus the bias term.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="activation-function">Activation Function<a class="hash-link" href="#activation-function" title="Direct link to heading">â€‹</a></h3><ul><li>The sigmoid function is commonly used as the activation function in neural networks.</li><li>The activation function outputs the activation values for each unit in a layer.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="notation-and-input-vector">Notation and Input Vector<a class="hash-link" href="#notation-and-input-vector" title="Direct link to heading">â€‹</a></h3><ul><li>Input vector X can be denoted as a_0 for consistency.</li><li>The activation values of any layer can be computed using parameters and the activations of the previous layer.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="inference-algorithm">Inference Algorithm<a class="hash-link" href="#inference-algorithm" title="Direct link to heading">â€‹</a></h3><ul><li>The computed activation values can be used for making predictions.</li><li>The inference algorithm uses the computed activations to make predictions.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-1">Conclusion<a class="hash-link" href="#conclusion-1" title="Direct link to heading">â€‹</a></h3><ul><li>Neural networks are constructed using layers of neurons.</li><li>Each layer performs computations using parameters and activation functions.</li><li>Activation values are computed for each layer based on the previous layer&#x27;s activations.</li><li>The inference algorithm utilizes the computed activations to make predictions.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="building-neural-networks">Building Neural Networks<a class="hash-link" href="#building-neural-networks" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-to-forward-propagation">Introduction to Forward Propagation<a class="hash-link" href="#introduction-to-forward-propagation" title="Direct link to heading">â€‹</a></h3><ul><li>Forward propagation is an algorithm used for making predictions in neural networks.</li><li>It involves computing the activations of neurons in a neural network to go from input to output.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="motivating-example-handwritten-digit-recognition">Motivating Example: Handwritten Digit Recognition<a class="hash-link" href="#motivating-example-handwritten-digit-recognition" title="Direct link to heading">â€‹</a></h3><ul><li>The example used is binary classification of handwritten digits zero and one.</li><li>An 8x8 image is represented as a matrix of 64 pixel intensity values.</li><li>Neural network architecture:<ul><li>Input layer: 64 units (corresponding to the 64 input features)</li><li>First hidden layer: 25 units</li><li>Second hidden layer: 15 units</li><li>Output layer: 1 unit (predicting the chance of being digit one)</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="sequence-of-computations">Sequence of Computations<a class="hash-link" href="#sequence-of-computations" title="Direct link to heading">â€‹</a></h3><ol><li><p>Compute a1: Activation values of the first hidden layer.</p><ul><li>a1 = sigmoid(W1 <!-- -->*<!-- --> a0 + b1), where a0 is the input feature vector.</li></ul></li><li><p>Compute a2: Activation values of the second hidden layer.</p><ul><li>a2 = sigmoid(W2 <!-- -->*<!-- --> a1 + b2)</li></ul></li><li><p>Compute a3: Activation value of the output layer.</p><ul><li>a3 = sigmoid(W3 <!-- -->*<!-- --> a2 + b3)</li></ul></li><li><p>Optional: Threshold a3 at 0.5 to obtain a binary classification label.</p></li></ol><h3 class="anchor anchorWithStickyNavbar_mojV" id="forward-propagation-and-activation-functions">Forward Propagation and Activation Functions<a class="hash-link" href="#forward-propagation-and-activation-functions" title="Direct link to heading">â€‹</a></h3><ul><li>Forward propagation is also known as propagating activations from left to right.</li><li>Activation values are computed using the sigmoid activation function.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-2">Conclusion<a class="hash-link" href="#conclusion-2" title="Direct link to heading">â€‹</a></h3><ul><li>Forward propagation is an algorithm used for making predictions in neural networks.</li><li>It involves computing activation values of neurons from input to output.</li><li>Activation values are computed using the sigmoid activation function.</li><li>TensorFlow can be used for implementing neural networks.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Advanced Learning Algorithms/9-Neural network layer.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/Advanced Learning Algorithms/Neural networks intuition"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">1 - Neural networks intuition</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/Advanced Learning Algorithms/TensorFlow implementation"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">3 - TensorFlow implementation</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#neural-networks-layer" class="table-of-contents__link toc-highlight">Neural Networks layer</a><ul><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#layer-of-neurons" class="table-of-contents__link toc-highlight">Layer of Neurons</a></li><li><a href="#layer-indexing" class="table-of-contents__link toc-highlight">Layer Indexing</a></li><li><a href="#layer-1-computation" class="table-of-contents__link toc-highlight">Layer 1 Computation</a></li><li><a href="#layer-2-computation" class="table-of-contents__link toc-highlight">Layer 2 Computation</a></li><li><a href="#binary-prediction" class="table-of-contents__link toc-highlight">Binary Prediction</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#complex-neural-networks" class="table-of-contents__link toc-highlight">Complex neural Networks</a><ul><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#neural-network-layers" class="table-of-contents__link toc-highlight">Neural Network Layers</a></li><li><a href="#layer-computation" class="table-of-contents__link toc-highlight">Layer Computation</a></li><li><a href="#layer-indexing-1" class="table-of-contents__link toc-highlight">Layer Indexing</a></li><li><a href="#layer-3-computation" class="table-of-contents__link toc-highlight">Layer 3 Computation</a></li><li><a href="#general-equation-for-layer-activation" class="table-of-contents__link toc-highlight">General Equation for Layer Activation</a></li><li><a href="#activation-function" class="table-of-contents__link toc-highlight">Activation Function</a></li><li><a href="#notation-and-input-vector" class="table-of-contents__link toc-highlight">Notation and Input Vector</a></li><li><a href="#inference-algorithm" class="table-of-contents__link toc-highlight">Inference Algorithm</a></li><li><a href="#conclusion-1" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#building-neural-networks" class="table-of-contents__link toc-highlight">Building Neural Networks</a><ul><li><a href="#introduction-to-forward-propagation" class="table-of-contents__link toc-highlight">Introduction to Forward Propagation</a></li><li><a href="#motivating-example-handwritten-digit-recognition" class="table-of-contents__link toc-highlight">Motivating Example: Handwritten Digit Recognition</a></li><li><a href="#sequence-of-computations" class="table-of-contents__link toc-highlight">Sequence of Computations</a></li><li><a href="#forward-propagation-and-activation-functions" class="table-of-contents__link toc-highlight">Forward Propagation and Activation Functions</a></li><li><a href="#conclusion-2" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs">Notes</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023</div></div></div></footer></div>
<script src="/assets/js/runtime~main.852293d6.js"></script>
<script src="/assets/js/main.026a2937.js"></script>
</body>
</html>