<!doctype html>
<html class="docs-version-current" lang="en" dir="ltr">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-beta.15">
<link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="Nikki&#39;s Blog RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="Nikki&#39;s Blog Atom Feed"><title data-react-helmet="true">8-Multiclass Classification | Nikki&#x27;s Blog</title><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:url" content="https://nikkon18.github.io//docs/Advanced Learning Algorithms/Multiclass Classification"><meta data-react-helmet="true" name="docusaurus_locale" content="en"><meta data-react-helmet="true" name="docusaurus_version" content="current"><meta data-react-helmet="true" name="docusaurus_tag" content="docs-default-current"><meta data-react-helmet="true" property="og:title" content="8-Multiclass Classification | Nikki&#x27;s Blog"><meta data-react-helmet="true" name="description" content="Muticalss"><meta data-react-helmet="true" property="og:description" content="Muticalss"><link data-react-helmet="true" rel="icon" href="/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://nikkon18.github.io//docs/Advanced Learning Algorithms/Multiclass Classification"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Advanced Learning Algorithms/Multiclass Classification" hreflang="en"><link data-react-helmet="true" rel="alternate" href="https://nikkon18.github.io//docs/Advanced Learning Algorithms/Multiclass Classification" hreflang="x-default"><link rel="stylesheet" href="/assets/css/styles.9c11c536.css">
<link rel="preload" href="/assets/js/runtime~main.852293d6.js" as="script">
<link rel="preload" href="/assets/js/main.026a2937.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<div role="region"><a href="#" class="skipToContent_ZgBM">Skip to main content</a></div><nav class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Navigation bar toggle" class="navbar__toggle clean-btn" type="button" tabindex="0"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--light_TfLj"><img src="/img/logo.svg" alt="My Site Logo" class="themedImage_W2Cr themedImage--dark_oUvU"></div><b class="navbar__title">Nikki&#x27;s Blog</b></a><a class="navbar__item navbar__link navbar__link--active" href="/docs/Supervised Machine Learning/Overview of Machine learning">Notes</a><a class="navbar__item navbar__link" href="/blog">Blog</a></div><div class="navbar__items navbar__items--right"><div class="toggle_Pssr toggle_TdHA toggleDisabled_jDku"><div class="toggleTrack_SSoT" role="button" tabindex="-1"><div class="toggleTrackCheck_XobZ"><span class="toggleIcon_eZtF">ðŸŒœ</span></div><div class="toggleTrackX_YkSC"><span class="toggleIcon_eZtF">ðŸŒž</span></div><div class="toggleTrackThumb_uRm4"></div></div><input type="checkbox" class="toggleScreenReader_JnkT" aria-label="Switch between dark and light mode"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div class="main-wrapper docs-wrapper docs-doc-page"><div class="docPage_P2Lg"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_RiI4" type="button"></button><aside class="theme-doc-sidebar-container docSidebarContainer_rKC_"><div class="sidebar_CW9Y"><nav class="menu thin-scrollbar menu_SkdO"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist hasHref_VCh3" href="/docs/Supervised Machine Learning/Overview of Machine learning">Supervised Machine Learning</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--active hasHref_VCh3" aria-current="page" href="/docs/Advanced Learning Algorithms/Neural networks intuition">Advanced Learning Algorithms</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Neural networks intuition">1 - Neural networks intuition</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Neural network layer">2 - Neural network layer</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/TensorFlow implementation">3 - TensorFlow implementation</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Neural network implementation in Python">4-Neural network implementation in Python</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Speculations on AGI">5-Paths to Artificial General Intelligence (AGI)</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Training a Neural Network in TensorFlow">6-Neural Network Training</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Activation Functions">7-Activation Functions</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Advanced Learning Algorithms/Multiclass Classification">8-Multiclass Classification</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Advanced Learning Algorithms/Advice for applying machine learning">9-Advice for applying machine learning</a></li></ul></li></ul></nav></div></aside><main class="docMainContainer_TCnq"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_DM6M"><div class="docItemContainer_vinB"><article><div class="tocCollapsible_jdIR theme-doc-toc-mobile tocMobile_TmEX"><button type="button" class="clean-btn tocCollapsibleButton_Fzxq">On this page</button></div><div class="theme-doc-markdown markdown"><h1>8-Multiclass Classification</h1><h2 class="anchor anchorWithStickyNavbar_mojV" id="muticalss">Muticalss<a class="hash-link" href="#muticalss" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction">Introduction<a class="hash-link" href="#introduction" title="Direct link to heading">â€‹</a></h3><p>Multiclass classification refers to classification problems where there are more than two possible output labels. Unlike binary classification, which distinguishes between two classes (e.g., 0 and 1), multiclass classification deals with multiple discrete categories.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="examples">Examples<a class="hash-link" href="#examples" title="Direct link to heading">â€‹</a></h3><ol><li>Handwritten Digit Classification: In this problem, we aim to recognize handwritten digits, not just 0 and 1, but all 10 possible digits.</li><li>Disease Classification: Classifying patients into multiple disease categories, such as three or five different possible diseases.</li><li>Visual Defect Inspection: Inspecting manufactured parts for defects like scratches, discoloration, or chips, where there are multiple classes of defects.</li></ol><h3 class="anchor anchorWithStickyNavbar_mojV" id="multiclass-classification-vs-binary-classification">Multiclass Classification vs. Binary Classification<a class="hash-link" href="#multiclass-classification-vs-binary-classification" title="Direct link to heading">â€‹</a></h3><p>In binary classification, we have a dataset with features x1 and x2, and logistic regression estimates the probability of y being 1 given the features x. However, in multiclass classification, the dataset consists of multiple classes. For example, we may have four classes represented by O, X, â–², and â– . Instead of estimating the chance of y being equal to 1, we want to estimate the chances of y being equal to 1, 2, 3, or 4. The algorithm used for multiclass classification can learn decision boundaries that divide the space into multiple categories.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="softmax-regression-algorithm">Softmax Regression Algorithm<a class="hash-link" href="#softmax-regression-algorithm" title="Direct link to heading">â€‹</a></h3><p>The softmax regression algorithm is a generalization of logistic regression specifically designed for multiclass classification problems. It extends logistic regression to estimate the probabilities of each class label. By learning a decision boundary, the algorithm can classify examples into multiple categories.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="multiclass-classification-with-neural-networks">Multiclass Classification with Neural Networks<a class="hash-link" href="#multiclass-classification-with-neural-networks" title="Direct link to heading">â€‹</a></h3><p>After understanding softmax regression, we can integrate it into a neural network architecture. This allows us to train a neural network to solve multiclass classification problems effectively.</p><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion">Conclusion<a class="hash-link" href="#conclusion" title="Direct link to heading">â€‹</a></h3><p>Multiclass classification expands the scope of classification problems beyond binary classification. By employing algorithms like softmax regression and neural networks, we can estimate the probabilities of multiple classes and classify examples into multiple categories.</p><h2 class="anchor anchorWithStickyNavbar_mojV" id="softmax-regression">Softmax Regression<a class="hash-link" href="#softmax-regression" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-1">Introduction<a class="hash-link" href="#introduction-1" title="Direct link to heading">â€‹</a></h3><ul><li>Softmax regression is a generalization of logistic regression for multiclass classification.</li><li>Logistic regression applies to binary classification, where the output can be either 0 or 1.</li><li>Softmax regression computes probabilities for multiple output classes.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="logistic-regression-recap">Logistic regression recap<a class="hash-link" href="#logistic-regression-recap" title="Direct link to heading">â€‹</a></h3><ul><li>Logistic regression computes the probability of the output being 1 given the input features.</li><li>It uses the sigmoid function to estimate the probability.</li><li>The probabilities of 0 and 1 must add up to 1.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="generalizing-to-softmax-regression">Generalizing to softmax regression<a class="hash-link" href="#generalizing-to-softmax-regression" title="Direct link to heading">â€‹</a></h3><ul><li>Softmax regression extends logistic regression to multiple output classes.</li><li>Let&#x27;s consider an example where the output can be 1, 2, 3, or 4.</li><li>Softmax regression computes values z for each output class.</li><li>It uses parameters w and b for each class.</li><li>The softmax regression formula:<ul><li>Compute a for each class using the softmax function.</li><li>Each a value represents the estimated probability of the corresponding class.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="softmax-regression-formula">Softmax regression formula<a class="hash-link" href="#softmax-regression-formula" title="Direct link to heading">â€‹</a></h3><ul><li>Softmax regression computes a for each output class.</li><li>The formula for a class j:<ul><li>Compute zj using parameters wj and bj.</li><li>Compute aj using the softmax function.</li></ul></li><li>The softmax function:<ul><li>Calculate the exponential of each z value.</li><li>Divide the exponential of the current z by the sum of exponentials of all z values.</li><li>The resulting aj represents the estimated probability of class j.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="softmax-regression-for-n-possible-output-classes">Softmax regression for n possible output classes<a class="hash-link" href="#softmax-regression-for-n-possible-output-classes" title="Direct link to heading">â€‹</a></h3><ul><li>Softmax regression can handle n possible output classes.</li><li>The formula for softmax regression with n classes:<ul><li>Compute zj using parameters wj and bj.</li><li>Compute aj using the softmax function, summing over all classes.</li></ul></li><li>The sum of all a values always adds up to 1.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="softmax-regression-vs-logistic-regression">Softmax regression vs. logistic regression<a class="hash-link" href="#softmax-regression-vs-logistic-regression" title="Direct link to heading">â€‹</a></h3><ul><li>Softmax regression reduces to logistic regression when there are only two output classes.</li><li>The parameters and formulas differ slightly, but the concepts are similar.</li><li>Softmax regression is a generalization of logistic regression.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="cost-function-for-softmax-regression">Cost function for softmax regression<a class="hash-link" href="#cost-function-for-softmax-regression" title="Direct link to heading">â€‹</a></h3><ul><li>The cost function measures the error of the model.</li><li>For logistic regression, the loss function is based on probabilities.</li><li>The loss function for softmax regression:<ul><li>The loss is the negative logarithm of the estimated probability of the correct class.</li><li>The loss is computed for the actual class value.</li></ul></li><li>The goal is to minimize the loss function over the entire training set.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-1">Conclusion<a class="hash-link" href="#conclusion-1" title="Direct link to heading">â€‹</a></h3><ul><li>Softmax regression is a generalization of logistic regression for multiclass classification.</li><li>It computes probabilities for each output class using the softmax function.</li><li>The cost function measures the error of the model.</li><li>Softmax regression can be used to build multiclass classification algorithms.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="neural-networks-with-softmax-output">Neural Networks with Softmax output<a class="hash-link" href="#neural-networks-with-softmax-output" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-2">Introduction<a class="hash-link" href="#introduction-2" title="Direct link to heading">â€‹</a></h3><ul><li>Softmax regression is used for multi-class classification in neural networks.</li><li>It can be applied to the output layer of a neural network to estimate class probabilities.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="softmax-output-layer">Softmax Output Layer<a class="hash-link" href="#softmax-output-layer" title="Direct link to heading">â€‹</a></h3><ul><li>Modify the neural network architecture for multi-class classification.</li><li>Use a Softmax output layer with as many units as there are classes.</li><li>Each unit represents the estimated probability of a specific class.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="forward-propagation">Forward Propagation<a class="hash-link" href="#forward-propagation" title="Direct link to heading">â€‹</a></h3><ul><li>Compute activations for each layer in the neural network.</li><li>For the Softmax output layer:<ul><li>Compute z values (Z1 to Z10) using parameters and previous layer activations.</li><li>Apply the Softmax function to calculate estimated probabilities (a1 to a10).</li><li>Each a value represents the chance of the corresponding class.</li></ul></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="softmax-function">Softmax Function<a class="hash-link" href="#softmax-function" title="Direct link to heading">â€‹</a></h3><ul><li>The Softmax function calculates the estimated probability of each class.</li><li>For each class j:<ul><li>Compute zj using parameters and previous layer activations.</li><li>Compute aj using the Softmax function.</li></ul></li><li>The denominator in the Softmax function is the sum of exponentials of all z values.</li><li>The resulting aj represents the estimated probability of class j.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="softmax-activation-function">Softmax Activation Function<a class="hash-link" href="#softmax-activation-function" title="Direct link to heading">â€‹</a></h3><ul><li>The Softmax layer is sometimes called the Softmax activation function.</li><li>Unlike other activation functions, each activation value depends on all z values.</li><li>To compute a1 to a10, all z1 to z10 values are used simultaneously.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="implementing-softmax-regression-in-tensorflow">Implementing Softmax Regression in TensorFlow<a class="hash-link" href="#implementing-softmax-regression-in-tensorflow" title="Direct link to heading">â€‹</a></h3><ul><li>In TensorFlow, use the Sequential model to build the neural network.</li><li>Specify the layers sequentially: input layer, hidden layers, and Softmax output layer.</li><li>Use the ReLU activation function for hidden layers.</li><li>Use the Softmax activation function for the output layer.</li><li>The cost function for Softmax regression is called SparseCategoricalCrossentropy.</li><li>It handles multi-class classification with sparse target labels.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="training-the-model">Training the Model<a class="hash-link" href="#training-the-model" title="Direct link to heading">â€‹</a></h3><ul><li>Train the model using TensorFlow&#x27;s training code.</li><li>The process is similar to logistic regression.</li><li>Use the recommended version of the code for better accuracy.</li><li>Avoid using the code shown in the lecture, as a better version will be introduced later.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-2">Conclusion<a class="hash-link" href="#conclusion-2" title="Direct link to heading">â€‹</a></h3><ul><li>Softmax regression is applied to the output layer of a neural network for multi-class classification.</li><li>It estimates the probabilities of each class using the Softmax function.</li><li>The implementation in TensorFlow involves specifying the layers and training the model.</li><li>Use the recommended code for improved accuracy in computing class probabilities.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="improved-implementation-of-softmax">Improved implementation of softmax<a class="hash-link" href="#improved-implementation-of-softmax" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-3">Introduction<a class="hash-link" href="#introduction-3" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent is a widely-used optimization algorithm in machine learning.</li><li>Foundation for linear regression, logistic regression, and early neural network implementations.</li><li>Explore an algorithm that speeds up neural network training compared to gradient descent.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="gradient-descent-overview">Gradient Descent Overview<a class="hash-link" href="#gradient-descent-overview" title="Direct link to heading">â€‹</a></h3><ul><li>Gradient descent update: <code>w_j = w_j - Î± * âˆ‚J/âˆ‚w_j</code></li><li>Î± (learning rate) controls step size.</li><li>Challenge: Choosing the right Î±.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="the-need-for-adaptivity">The Need for Adaptivity<a class="hash-link" href="#the-need-for-adaptivity" title="Direct link to heading">â€‹</a></h3><ul><li>Visualize cost function J as ellipses.</li><li>Gradient descent may take small steps in the same direction.</li><li>Solution: Increase Î± for faster convergence.</li></ul><h4 class="anchor anchorWithStickyNavbar_mojV" id="adaptive-moment-estimation-adam">Adaptive Moment Estimation (Adam)<a class="hash-link" href="#adaptive-moment-estimation-adam" title="Direct link to heading">â€‹</a></h4><ul><li>Adam stands for Adaptive Moment Estimation.</li><li>It adjusts learning rates for each parameter independently.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="adam-algorithm">Adam Algorithm<a class="hash-link" href="#adam-algorithm" title="Direct link to heading">â€‹</a></h3><ul><li><p>Increase Î± if a parameter keeps moving consistently.</p></li><li><p>Reduce Î± if a parameter oscillates.</p></li><li><p>Algorithm details are complex (not covered here).</p></li><li><p>Implementation in code:</p><div class="codeBlockContainer_I0IT language-python theme-code-block"><div class="codeBlockContent_wNvx python"><pre tabindex="0" class="prism-code language-python codeBlock_jd64 thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_mRuA"><span class="token-line" style="color:#393A34"><span class="token plain">optimizer </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> tf</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">keras</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">optimizers</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">Adam</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">learning_rate</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">1e-3</span><span class="token punctuation" style="color:#393A34">)</span><br></span></code></pre><button type="button" aria-label="Copy code to clipboard" class="copyButton_wuS7 clean-btn">Copy</button></div></div></li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="choosing-the-learning-rate">Choosing the Learning Rate<a class="hash-link" href="#choosing-the-learning-rate" title="Direct link to heading">â€‹</a></h3><ul><li>Experiment with different initial learning rates.</li><li>Adam is more robust to learning rate selection than traditional gradient descent.</li><li>Fine-tune Î± for faster learning.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-3">Conclusion<a class="hash-link" href="#conclusion-3" title="Direct link to heading">â€‹</a></h3><ul><li>Adam optimization algorithm works faster than gradient descent.</li><li>De facto standard in neural network training.</li><li>Safe choice for practitioners.</li></ul><h2 class="anchor anchorWithStickyNavbar_mojV" id="additional-layer-types">Additional Layer Types<a class="hash-link" href="#additional-layer-types" title="Direct link to heading">â€‹</a></h2><h3 class="anchor anchorWithStickyNavbar_mojV" id="introduction-4">Introduction<a class="hash-link" href="#introduction-4" title="Direct link to heading">â€‹</a></h3><ul><li>Dense layers have been our focus so far, where every neuron connects to all previous layer activations.</li><li>There are other layer types in neural networks, offering unique properties.</li><li>In this blog, we&#x27;ll explore a different layer type known as the convolutional layer and its advantages.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="dense-layers-recap">Dense Layers Recap<a class="hash-link" href="#dense-layers-recap" title="Direct link to heading">â€‹</a></h3><ul><li>Activation in a dense layer depends on all previous layer activations.</li><li>Powerful for many applications.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="introducing-convolutional-layers">Introducing Convolutional Layers<a class="hash-link" href="#introducing-convolutional-layers" title="Direct link to heading">â€‹</a></h3><ul><li>Convolutional layers have neurons that only focus on specific regions of the input.</li><li>Neurons in a convolutional layer process local input regions.</li><li>Example: Image with handwritten digit &quot;nine.&quot;</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="benefits-of-convolutional-layers">Benefits of Convolutional Layers<a class="hash-link" href="#benefits-of-convolutional-layers" title="Direct link to heading">â€‹</a></h3><ol><li><strong>Speed</strong>: Faster computation due to localized processing.</li><li><strong>Data Efficiency</strong>: Requires less training data.</li><li><strong>Overfitting</strong>: Less prone to overfitting.</li></ol><h3 class="anchor anchorWithStickyNavbar_mojV" id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)<a class="hash-link" href="#convolutional-neural-networks-cnns" title="Direct link to heading">â€‹</a></h3><ul><li>CNNs consist of multiple convolutional layers.</li><li>Each layer processes different input regions.</li><li>Can be more effective than dense layers for certain tasks.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="detailed-example-ekg-signal-classification">Detailed Example: EKG Signal Classification<a class="hash-link" href="#detailed-example-ekg-signal-classification" title="Direct link to heading">â€‹</a></h3><ul><li>Consider a one-dimensional input, such as an electrocardiogram (EKG) signal.</li><li>Aim: Classify the presence of heart disease.</li><li>First hidden layer: Neurons focus on small windows of the input signal.</li><li>Second hidden layer: Neurons process different window regions.</li><li>Output layer: Makes a binary classification based on the activations from the previous layer.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="architectural-choices-in-convolutional-layers">Architectural Choices in Convolutional Layers<a class="hash-link" href="#architectural-choices-in-convolutional-layers" title="Direct link to heading">â€‹</a></h3><ul><li>Parameters like window size and the number of neurons influence layer effectiveness.</li><li>Careful architectural choices can lead to more powerful neural networks.</li></ul><h3 class="anchor anchorWithStickyNavbar_mojV" id="conclusion-4">Conclusion<a class="hash-link" href="#conclusion-4" title="Direct link to heading">â€‹</a></h3><ul><li>Convolutional layers offer a unique approach in neural networks.</li><li>They process localized input regions, providing advantages in speed, data efficiency, and overfitting.</li><li>Understanding different layer types helps grasp the diversity of neural network architectures.</li><li>The field of neural networks continues to evolve, with researchers inventing new layer types to build more powerful networks.</li></ul></div><footer class="theme-doc-footer docusaurus-mt-lg"><div class="theme-doc-footer-edit-meta-row row"><div class="col"><a href="https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Advanced Learning Algorithms/15-Multiclass Classification.md" target="_blank" rel="noreferrer noopener" class="theme-edit-this-page"><svg fill="currentColor" height="20" width="20" viewBox="0 0 40 40" class="iconEdit_dcUD" aria-hidden="true"><g><path d="m34.5 11.7l-3 3.1-6.3-6.3 3.1-3q0.5-0.5 1.2-0.5t1.1 0.5l3.9 3.9q0.5 0.4 0.5 1.1t-0.5 1.2z m-29.5 17.1l18.4-18.5 6.3 6.3-18.4 18.4h-6.3v-6.2z"></path></g></svg>Edit this page</a></div><div class="col lastUpdated_foO9"></div></div></footer></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/docs/Advanced Learning Algorithms/Activation Functions"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">7-Activation Functions</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/docs/Advanced Learning Algorithms/Advice for applying machine learning"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">9-Advice for applying machine learning</div></a></div></nav></div></div><div class="col col--3"><div class="tableOfContents_cNA8 thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#muticalss" class="table-of-contents__link toc-highlight">Muticalss</a><ul><li><a href="#introduction" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#examples" class="table-of-contents__link toc-highlight">Examples</a></li><li><a href="#multiclass-classification-vs-binary-classification" class="table-of-contents__link toc-highlight">Multiclass Classification vs. Binary Classification</a></li><li><a href="#softmax-regression-algorithm" class="table-of-contents__link toc-highlight">Softmax Regression Algorithm</a></li><li><a href="#multiclass-classification-with-neural-networks" class="table-of-contents__link toc-highlight">Multiclass Classification with Neural Networks</a></li><li><a href="#conclusion" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#softmax-regression" class="table-of-contents__link toc-highlight">Softmax Regression</a><ul><li><a href="#introduction-1" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#logistic-regression-recap" class="table-of-contents__link toc-highlight">Logistic regression recap</a></li><li><a href="#generalizing-to-softmax-regression" class="table-of-contents__link toc-highlight">Generalizing to softmax regression</a></li><li><a href="#softmax-regression-formula" class="table-of-contents__link toc-highlight">Softmax regression formula</a></li><li><a href="#softmax-regression-for-n-possible-output-classes" class="table-of-contents__link toc-highlight">Softmax regression for n possible output classes</a></li><li><a href="#softmax-regression-vs-logistic-regression" class="table-of-contents__link toc-highlight">Softmax regression vs. logistic regression</a></li><li><a href="#cost-function-for-softmax-regression" class="table-of-contents__link toc-highlight">Cost function for softmax regression</a></li><li><a href="#conclusion-1" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#neural-networks-with-softmax-output" class="table-of-contents__link toc-highlight">Neural Networks with Softmax output</a><ul><li><a href="#introduction-2" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#softmax-output-layer" class="table-of-contents__link toc-highlight">Softmax Output Layer</a></li><li><a href="#forward-propagation" class="table-of-contents__link toc-highlight">Forward Propagation</a></li><li><a href="#softmax-function" class="table-of-contents__link toc-highlight">Softmax Function</a></li><li><a href="#softmax-activation-function" class="table-of-contents__link toc-highlight">Softmax Activation Function</a></li><li><a href="#implementing-softmax-regression-in-tensorflow" class="table-of-contents__link toc-highlight">Implementing Softmax Regression in TensorFlow</a></li><li><a href="#training-the-model" class="table-of-contents__link toc-highlight">Training the Model</a></li><li><a href="#conclusion-2" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#improved-implementation-of-softmax" class="table-of-contents__link toc-highlight">Improved implementation of softmax</a><ul><li><a href="#introduction-3" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#gradient-descent-overview" class="table-of-contents__link toc-highlight">Gradient Descent Overview</a></li><li><a href="#the-need-for-adaptivity" class="table-of-contents__link toc-highlight">The Need for Adaptivity</a></li><li><a href="#adam-algorithm" class="table-of-contents__link toc-highlight">Adam Algorithm</a></li><li><a href="#choosing-the-learning-rate" class="table-of-contents__link toc-highlight">Choosing the Learning Rate</a></li><li><a href="#conclusion-3" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li><li><a href="#additional-layer-types" class="table-of-contents__link toc-highlight">Additional Layer Types</a><ul><li><a href="#introduction-4" class="table-of-contents__link toc-highlight">Introduction</a></li><li><a href="#dense-layers-recap" class="table-of-contents__link toc-highlight">Dense Layers Recap</a></li><li><a href="#introducing-convolutional-layers" class="table-of-contents__link toc-highlight">Introducing Convolutional Layers</a></li><li><a href="#benefits-of-convolutional-layers" class="table-of-contents__link toc-highlight">Benefits of Convolutional Layers</a></li><li><a href="#convolutional-neural-networks-cnns" class="table-of-contents__link toc-highlight">Convolutional Neural Networks (CNNs)</a></li><li><a href="#detailed-example-ekg-signal-classification" class="table-of-contents__link toc-highlight">Detailed Example: EKG Signal Classification</a></li><li><a href="#architectural-choices-in-convolutional-layers" class="table-of-contents__link toc-highlight">Architectural Choices in Convolutional Layers</a></li><li><a href="#conclusion-4" class="table-of-contents__link toc-highlight">Conclusion</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container container-fluid"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">More</div><ul class="footer__items"><li class="footer__item"><a class="footer__link-item" href="/docs">Notes</a></li></ul></div></div><div class="footer__bottom text--center"><div class="footer__copyright">Copyright Â© 2023</div></div></div></footer></div>
<script src="/assets/js/runtime~main.852293d6.js"></script>
<script src="/assets/js/main.026a2937.js"></script>
</body>
</html>