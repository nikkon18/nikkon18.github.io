"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[6825],{3905:(e,t,n)=>{n.d(t,{Zo:()=>s,kt:()=>m});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var u=a.createContext({}),c=function(e){var t=a.useContext(u),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},s=function(e){var t=c(e.components);return a.createElement(u.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},d=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,u=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),d=c(n),m=i,k=d["".concat(u,".").concat(m)]||d[m]||p[m]||r;return n?a.createElement(k,o(o({ref:t},s),{},{components:n})):a.createElement(k,o({ref:t},s))}));function m(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=d;var l={};for(var u in t)hasOwnProperty.call(t,u)&&(l[u]=t[u]);l.originalType=e,l.mdxType="string"==typeof e?e:i,o[1]=l;for(var c=2;c<r;c++)o[c]=n[c];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}d.displayName="MDXCreateElement"},9176:(e,t,n)=>{n.r(t),n.d(t,{frontMatter:()=>r,contentTitle:()=>o,metadata:()=>l,toc:()=>u,default:()=>s});var a=n(7462),i=(n(7294),n(3905));const r={},o="2 - Neural network layer",l={unversionedId:"Advanced Learning Algorithms/Neural network layer",id:"Advanced Learning Algorithms/Neural network layer",title:"2 - Neural network layer",description:"Neural Networks layer",source:"@site/docs/Advanced Learning Algorithms/9-Neural network layer.md",sourceDirName:"Advanced Learning Algorithms",slug:"/Advanced Learning Algorithms/Neural network layer",permalink:"/docs/Advanced Learning Algorithms/Neural network layer",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Advanced Learning Algorithms/9-Neural network layer.md",tags:[],version:"current",sidebarPosition:9,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"1 - Neural networks intuition",permalink:"/docs/Advanced Learning Algorithms/Neural networks intuition"},next:{title:"3 - TensorFlow implementation",permalink:"/docs/Advanced Learning Algorithms/TensorFlow implementation"}},u=[{value:"Neural Networks layer",id:"neural-networks-layer",children:[{value:"Introduction",id:"introduction",children:[],level:3},{value:"Layer of Neurons",id:"layer-of-neurons",children:[{value:"Neuron Computation",id:"neuron-computation",children:[],level:4},{value:"Neuron Outputs",id:"neuron-outputs",children:[],level:4}],level:3},{value:"Layer Indexing",id:"layer-indexing",children:[],level:3},{value:"Layer 1 Computation",id:"layer-1-computation",children:[],level:3},{value:"Layer 2 Computation",id:"layer-2-computation",children:[],level:3},{value:"Binary Prediction",id:"binary-prediction",children:[],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2},{value:"Complex neural Networks",id:"complex-neural-networks",children:[{value:"Introduction",id:"introduction-1",children:[],level:3},{value:"Neural Network Layers",id:"neural-network-layers",children:[],level:3},{value:"Layer Computation",id:"layer-computation",children:[],level:3},{value:"Layer Indexing",id:"layer-indexing-1",children:[],level:3},{value:"Layer 3 Computation",id:"layer-3-computation",children:[],level:3},{value:"General Equation for Layer Activation",id:"general-equation-for-layer-activation",children:[],level:3},{value:"Activation Function",id:"activation-function",children:[],level:3},{value:"Notation and Input Vector",id:"notation-and-input-vector",children:[],level:3},{value:"Inference Algorithm",id:"inference-algorithm",children:[],level:3},{value:"Conclusion",id:"conclusion-1",children:[],level:3}],level:2},{value:"Building Neural Networks",id:"building-neural-networks",children:[{value:"Introduction to Forward Propagation",id:"introduction-to-forward-propagation",children:[],level:3},{value:"Motivating Example: Handwritten Digit Recognition",id:"motivating-example-handwritten-digit-recognition",children:[],level:3},{value:"Sequence of Computations",id:"sequence-of-computations",children:[],level:3},{value:"Forward Propagation and Activation Functions",id:"forward-propagation-and-activation-functions",children:[],level:3},{value:"Conclusion",id:"conclusion-2",children:[],level:3}],level:2}],c={toc:u};function s(e){let{components:t,...n}=e;return(0,i.kt)("wrapper",(0,a.Z)({},c,n,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"2---neural-network-layer"},"2 - Neural network layer"),(0,i.kt)("h2",{id:"neural-networks-layer"},"Neural Networks layer"),(0,i.kt)("h3",{id:"introduction"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Neural networks consist of layers of neurons"),(0,i.kt)("li",{parentName:"ul"},"Layers can be combined to form larger neural networks")),(0,i.kt)("h3",{id:"layer-of-neurons"},"Layer of Neurons"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Example: Demand prediction with input features and output neuron"),(0,i.kt)("li",{parentName:"ul"},"Hidden layer contains three neurons")),(0,i.kt)("h4",{id:"neuron-computation"},"Neuron Computation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Neurons in hidden layer perform logistic regression"),(0,i.kt)("li",{parentName:"ul"},"Each neuron has parameters w and b"),(0,i.kt)("li",{parentName:"ul"},"Activation value a is computed using logistic function g")),(0,i.kt)("h4",{id:"neuron-outputs"},"Neuron Outputs"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Each neuron in hidden layer produces an activation value"),(0,i.kt)("li",{parentName:"ul"},"Activation values form a vector passed to the output layer")),(0,i.kt)("h3",{id:"layer-indexing"},"Layer Indexing"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Different layers in neural networks are numbered"),(0,i.kt)("li",{parentName:"ul"},"Hidden layer is layer 1, output layer is layer 2"),(0,i.kt)("li",{parentName:"ul"},"Input layer can be called layer 0"),(0,i.kt)("li",{parentName:"ul"},"Superscripts in square brackets denote layer indexing")),(0,i.kt)("h3",{id:"layer-1-computation"},"Layer 1 Computation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Layer 1 takes input from the input layer"),(0,i.kt)("li",{parentName:"ul"},"Neurons in layer 1 compute activation values using parameters w and b")),(0,i.kt)("h3",{id:"layer-2-computation"},"Layer 2 Computation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Layer 2 takes input from layer 1"),(0,i.kt)("li",{parentName:"ul"},"Output layer contains a single neuron"),(0,i.kt)("li",{parentName:"ul"},"Neuron computes activation value using sigmoid function g")),(0,i.kt)("h3",{id:"binary-prediction"},"Binary Prediction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Optional step: Thresholding for binary prediction"),(0,i.kt)("li",{parentName:"ul"},"Output value can be thresholded at 0.5 for binary classification")),(0,i.kt)("h3",{id:"conclusion"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Neural networks consist of layers of neurons"),(0,i.kt)("li",{parentName:"ul"},"Each layer performs computations using parameters and activation functions"),(0,i.kt)("li",{parentName:"ul"},"Layers are combined to build complex neural network models")),(0,i.kt)("h2",{id:"complex-neural-networks"},"Complex neural Networks"),(0,i.kt)("h3",{id:"introduction-1"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Neural networks consist of layers of neurons."),(0,i.kt)("li",{parentName:"ul"},"Each layer performs computations using parameters and activation functions."),(0,i.kt)("li",{parentName:"ul"},"Layers are combined to build complex neural network models.")),(0,i.kt)("h3",{id:"neural-network-layers"},"Neural Network Layers"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Neural network layers take input vectors and produce output vectors."),(0,i.kt)("li",{parentName:"ul"},"Layers are numbered, with the input layer as Layer 0.")),(0,i.kt)("h3",{id:"layer-computation"},"Layer Computation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Neurons in a layer compute activation values using parameters and activation functions."),(0,i.kt)("li",{parentName:"ul"},"Activation values are computed for each neuron in the layer.")),(0,i.kt)("h3",{id:"layer-indexing-1"},"Layer Indexing"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Layers are indexed using superscripts in square brackets."),(0,i.kt)("li",{parentName:"ul"},"Hidden layers are numbered starting from 1."),(0,i.kt)("li",{parentName:"ul"},"Output layer is the final layer of the network.")),(0,i.kt)("h3",{id:"layer-3-computation"},"Layer 3 Computation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Layer 3 takes input from the previous layer (Layer 2)."),(0,i.kt)("li",{parentName:"ul"},"Neurons in Layer 3 compute activation values using parameters and activation functions."),(0,i.kt)("li",{parentName:"ul"},"Activation values form a vector, denoted as a_3.")),(0,i.kt)("h3",{id:"general-equation-for-layer-activation"},"General Equation for Layer Activation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The activation of a unit j in layer l is computed using the sigmoid function."),(0,i.kt)("li",{parentName:"ul"},"The activation is calculated as the sigmoid of the dot product of weights and the activation values of the previous layer, plus the bias term.")),(0,i.kt)("h3",{id:"activation-function"},"Activation Function"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The sigmoid function is commonly used as the activation function in neural networks."),(0,i.kt)("li",{parentName:"ul"},"The activation function outputs the activation values for each unit in a layer.")),(0,i.kt)("h3",{id:"notation-and-input-vector"},"Notation and Input Vector"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Input vector X can be denoted as a_0 for consistency."),(0,i.kt)("li",{parentName:"ul"},"The activation values of any layer can be computed using parameters and the activations of the previous layer.")),(0,i.kt)("h3",{id:"inference-algorithm"},"Inference Algorithm"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The computed activation values can be used for making predictions."),(0,i.kt)("li",{parentName:"ul"},"The inference algorithm uses the computed activations to make predictions.")),(0,i.kt)("h3",{id:"conclusion-1"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Neural networks are constructed using layers of neurons."),(0,i.kt)("li",{parentName:"ul"},"Each layer performs computations using parameters and activation functions."),(0,i.kt)("li",{parentName:"ul"},"Activation values are computed for each layer based on the previous layer's activations."),(0,i.kt)("li",{parentName:"ul"},"The inference algorithm utilizes the computed activations to make predictions.")),(0,i.kt)("h2",{id:"building-neural-networks"},"Building Neural Networks"),(0,i.kt)("h3",{id:"introduction-to-forward-propagation"},"Introduction to Forward Propagation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Forward propagation is an algorithm used for making predictions in neural networks."),(0,i.kt)("li",{parentName:"ul"},"It involves computing the activations of neurons in a neural network to go from input to output.")),(0,i.kt)("h3",{id:"motivating-example-handwritten-digit-recognition"},"Motivating Example: Handwritten Digit Recognition"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The example used is binary classification of handwritten digits zero and one."),(0,i.kt)("li",{parentName:"ul"},"An 8x8 image is represented as a matrix of 64 pixel intensity values."),(0,i.kt)("li",{parentName:"ul"},"Neural network architecture:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Input layer: 64 units (corresponding to the 64 input features)"),(0,i.kt)("li",{parentName:"ul"},"First hidden layer: 25 units"),(0,i.kt)("li",{parentName:"ul"},"Second hidden layer: 15 units"),(0,i.kt)("li",{parentName:"ul"},"Output layer: 1 unit (predicting the chance of being digit one)")))),(0,i.kt)("h3",{id:"sequence-of-computations"},"Sequence of Computations"),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Compute a1: Activation values of the first hidden layer."),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"a1 = sigmoid(W1 ","*"," a0 + b1), where a0 is the input feature vector."))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Compute a2: Activation values of the second hidden layer."),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"a2 = sigmoid(W2 ","*"," a1 + b2)"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Compute a3: Activation value of the output layer."),(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"a3 = sigmoid(W3 ","*"," a2 + b3)"))),(0,i.kt)("li",{parentName:"ol"},(0,i.kt)("p",{parentName:"li"},"Optional: Threshold a3 at 0.5 to obtain a binary classification label."))),(0,i.kt)("h3",{id:"forward-propagation-and-activation-functions"},"Forward Propagation and Activation Functions"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Forward propagation is also known as propagating activations from left to right."),(0,i.kt)("li",{parentName:"ul"},"Activation values are computed using the sigmoid activation function.")),(0,i.kt)("h3",{id:"conclusion-2"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Forward propagation is an algorithm used for making predictions in neural networks."),(0,i.kt)("li",{parentName:"ul"},"It involves computing activation values of neurons from input to output."),(0,i.kt)("li",{parentName:"ul"},"Activation values are computed using the sigmoid activation function."),(0,i.kt)("li",{parentName:"ul"},"TensorFlow can be used for implementing neural networks.")))}s.isMDXComponent=!0}}]);