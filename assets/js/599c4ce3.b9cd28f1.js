"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[4879],{3905:(e,n,t)=>{t.d(n,{Zo:()=>s,kt:()=>m});var r=t(7294);function a(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function o(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);n&&(r=r.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,r)}return t}function i(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?o(Object(t),!0).forEach((function(n){a(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function l(e,n){if(null==e)return{};var t,r,a=function(e,n){if(null==e)return{};var t,r,a={},o=Object.keys(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||(a[t]=e[t]);return a}(e,n);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(r=0;r<o.length;r++)t=o[r],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(a[t]=e[t])}return a}var c=r.createContext({}),p=function(e){var n=r.useContext(c),t=n;return e&&(t="function"==typeof e?e(n):i(i({},n),e)),t},s=function(e){var n=p(e.components);return r.createElement(c.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return r.createElement(r.Fragment,{},n)}},u=r.forwardRef((function(e,n){var t=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),u=p(t),m=a,f=u["".concat(c,".").concat(m)]||u[m]||d[m]||o;return t?r.createElement(f,i(i({ref:n},s),{},{components:t})):r.createElement(f,i({ref:n},s))}));function m(e,n){var t=arguments,a=n&&n.mdxType;if("string"==typeof e||a){var o=t.length,i=new Array(o);i[0]=u;var l={};for(var c in n)hasOwnProperty.call(n,c)&&(l[c]=n[c]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var p=2;p<o;p++)i[p]=t[p];return r.createElement.apply(null,i)}return r.createElement.apply(null,t)}u.displayName="MDXCreateElement"},3649:(e,n,t)=>{t.r(n),t.d(n,{frontMatter:()=>o,contentTitle:()=>i,metadata:()=>l,toc:()=>c,default:()=>s});var r=t(7462),a=(t(7294),t(3905));const o={},i="4-Neural network implementation in Python",l={unversionedId:"Advanced Learning Algorithms/Neural network implementation in Python",id:"Advanced Learning Algorithms/Neural network implementation in Python",title:"4-Neural network implementation in Python",description:"Implementing a Dense Layer",source:"@site/docs/Advanced Learning Algorithms/11-Neural network implementation in Python.md",sourceDirName:"Advanced Learning Algorithms",slug:"/Advanced Learning Algorithms/Neural network implementation in Python",permalink:"/docs/Advanced Learning Algorithms/Neural network implementation in Python",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Advanced Learning Algorithms/11-Neural network implementation in Python.md",tags:[],version:"current",sidebarPosition:11,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"3 - TensorFlow implementation",permalink:"/docs/Advanced Learning Algorithms/TensorFlow implementation"},next:{title:"5-Paths to Artificial General Intelligence (AGI)",permalink:"/docs/Advanced Learning Algorithms/Speculations on AGI"}},c=[{value:"Implementing a Dense Layer",id:"implementing-a-dense-layer",children:[{value:"Code for <code>dense</code> function:",id:"code-for-dense-function",children:[],level:3},{value:"Code for Forward Propagation:",id:"code-for-forward-propagation",children:[],level:3}],level:2}],p={toc:c};function s(e){let{components:n,...t}=e;return(0,a.kt)("wrapper",(0,r.Z)({},p,t,{components:n,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"4-neural-network-implementation-in-python"},"4-Neural network implementation in Python"),(0,a.kt)("h2",{id:"implementing-a-dense-layer"},"Implementing a Dense Layer"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"A dense layer represents a single layer of a neural network."),(0,a.kt)("li",{parentName:"ul"},"We can create a function called ",(0,a.kt)("inlineCode",{parentName:"li"},"dense")," to implement this layer."),(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"dense")," function takes the activation from the previous layer, as well as the parameters ",(0,a.kt)("inlineCode",{parentName:"li"},"w")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"b")," for the neurons in the current layer."),(0,a.kt)("li",{parentName:"ul"},"We stack the parameters ",(0,a.kt)("inlineCode",{parentName:"li"},"w")," into a matrix and the parameters ",(0,a.kt)("inlineCode",{parentName:"li"},"b")," into a 1D array."),(0,a.kt)("li",{parentName:"ul"},"The ",(0,a.kt)("inlineCode",{parentName:"li"},"dense")," function takes the activation from the previous layer (",(0,a.kt)("inlineCode",{parentName:"li"},"a_prev"),") and outputs the activations for the current layer.")),(0,a.kt)("h3",{id:"code-for-dense-function"},"Code for ",(0,a.kt)("inlineCode",{parentName:"h3"},"dense")," function:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"def dense(a_prev, w, b):\n    units = w.shape[1]  # Get the number of units in the layer\n    a = np.zeros(units)  # Initialize the activations array with zeros\n\n    for j in range(units):\n        w_col = w[:, j]  # Extract the jth column of the weight matrix\n        z = np.dot(w_col, a_prev) + b[j]  # Compute the weighted sum\n        a[j] = sigmoid(z)  # Apply the sigmoid activation function\n\n    return a\n")),(0,a.kt)("h3",{id:"code-for-forward-propagation"},"Code for Forward Propagation:"),(0,a.kt)("pre",null,(0,a.kt)("code",{parentName:"pre",className:"language-python"},"# Define the parameters for each layer\nw_1, b_1 = ..., ...  # Parameters for the first hidden layer\nw_2, b_2 = ..., ...  # Parameters for the second hidden layer\n\n# Compute the activations for each layer\na_1 = dense(x, w_1, b_1)  # Activation of the first hidden layer\na_2 = dense(a_1, w_2, b_2)  # Activation of the second hidden layer\n\n# If there is an output layer, define the output as the activation of the last layer\nf_x = a_2  # Output of the neural network\n\n# Return the output\nreturn f_x\n")))}s.isMDXComponent=!0}}]);