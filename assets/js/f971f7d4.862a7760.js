"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[9632],{3905:(e,t,i)=>{i.d(t,{Zo:()=>c,kt:()=>m});var n=i(7294);function r(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function a(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function l(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?a(Object(i),!0).forEach((function(t){r(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):a(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function o(e,t){if(null==e)return{};var i,n,r=function(e,t){if(null==e)return{};var i,n,r={},a=Object.keys(e);for(n=0;n<a.length;n++)i=a[n],t.indexOf(i)>=0||(r[i]=e[i]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(n=0;n<a.length;n++)i=a[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(r[i]=e[i])}return r}var u=n.createContext({}),s=function(e){var t=n.useContext(u),i=t;return e&&(i="function"==typeof e?e(t):l(l({},t),e)),i},c=function(e){var t=s(e.components);return n.createElement(u.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},d=n.forwardRef((function(e,t){var i=e.components,r=e.mdxType,a=e.originalType,u=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),d=s(i),m=r,f=d["".concat(u,".").concat(m)]||d[m]||p[m]||a;return i?n.createElement(f,l(l({ref:t},c),{},{components:i})):n.createElement(f,l({ref:t},c))}));function m(e,t){var i=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=i.length,l=new Array(a);l[0]=d;var o={};for(var u in t)hasOwnProperty.call(t,u)&&(o[u]=t[u]);o.originalType=e,o.mdxType="string"==typeof e?e:r,l[1]=o;for(var s=2;s<a;s++)l[s]=i[s];return n.createElement.apply(null,l)}return n.createElement.apply(null,i)}d.displayName="MDXCreateElement"},7097:(e,t,i)=>{i.r(t),i.d(t,{frontMatter:()=>a,contentTitle:()=>l,metadata:()=>o,toc:()=>u,default:()=>c});var n=i(7462),r=(i(7294),i(3905));const a={},l="4 - Mutiple linear regression",o={unversionedId:"Supervised Machine Learning/Mutiple linear regression",id:"Supervised Machine Learning/Mutiple linear regression",title:"4 - Mutiple linear regression",description:"Linear Regression with Multiple Features",source:"@site/docs/Supervised Machine Learning/4-Mutiple linear regression.md",sourceDirName:"Supervised Machine Learning",slug:"/Supervised Machine Learning/Mutiple linear regression",permalink:"/docs/Supervised Machine Learning/Mutiple linear regression",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/4-Mutiple linear regression.md",tags:[],version:"current",sidebarPosition:4,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"3 - Train the model with gradient descent",permalink:"/docs/Supervised Machine Learning/Train the model with gradient descent"},next:{title:"5 - Gradient descent in practice",permalink:"/docs/Supervised Machine Learning/Gradient descent in practice"}},u=[{value:"Linear Regression with Multiple Features",id:"linear-regression-with-multiple-features",children:[{value:"Introduction",id:"introduction",children:[],level:3},{value:"Notation",id:"notation",children:[],level:3},{value:"Model with Multiple Features",id:"model-with-multiple-features",children:[],level:3},{value:"General Model with n Features",id:"general-model-with-n-features",children:[],level:3},{value:"Multiple Linear Regression",id:"multiple-linear-regression",children:[],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2},{value:"Vectorization in Machine Learning part1",id:"vectorization-in-machine-learning-part1",children:[{value:"Introduction",id:"introduction-1",children:[],level:3},{value:"Understanding Vectorization",id:"understanding-vectorization",children:[],level:3},{value:"Non-Vectorized Implementation",id:"non-vectorized-implementation",children:[],level:3},{value:"Vectorized Implementation",id:"vectorized-implementation",children:[],level:3},{value:"Benefits of Vectorization",id:"benefits-of-vectorization",children:[],level:3},{value:"Conclusion",id:"conclusion-1",children:[],level:3}],level:2},{value:"Vectorization in Machine Learning part2",id:"vectorization-in-machine-learning-part2",children:[{value:"Introduction",id:"introduction-2",children:[],level:3},{value:"Understanding Vectorization",id:"understanding-vectorization-1",children:[],level:3},{value:"Benefits of Vectorization",id:"benefits-of-vectorization-1",children:[],level:3},{value:"Example: Multiple Linear Regression",id:"example-multiple-linear-regression",children:[],level:3},{value:"Implementation with NumPy",id:"implementation-with-numpy",children:[],level:3},{value:"Benefits of Vectorization in Machine Learning",id:"benefits-of-vectorization-in-machine-learning",children:[],level:3},{value:"Conclusion",id:"conclusion-2",children:[],level:3}],level:2},{value:"Gradient Descent for Multiple Linear Regression with Vectorization",id:"gradient-descent-for-multiple-linear-regression-with-vectorization",children:[{value:"Introduction",id:"introduction-3",children:[],level:3},{value:"Multiple Linear Regression in Vector Notation",id:"multiple-linear-regression-in-vector-notation",children:[],level:3},{value:"Gradient Descent for Multiple Linear Regression",id:"gradient-descent-for-multiple-linear-regression",children:[],level:3},{value:"Normal Equation Method (Side Note)",id:"normal-equation-method-side-note",children:[],level:3},{value:"Conclusion",id:"conclusion-3",children:[],level:3}],level:2}],s={toc:u};function c(e){let{components:t,...i}=e;return(0,r.kt)("wrapper",(0,n.Z)({},s,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"4---mutiple-linear-regression"},"4 - Mutiple linear regression"),(0,r.kt)("h2",{id:"linear-regression-with-multiple-features"},"Linear Regression with Multiple Features"),(0,r.kt)("h3",{id:"introduction"},"Introduction"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Linear regression can be extended to include multiple features for prediction."),(0,r.kt)("li",{parentName:"ul"},"By considering additional features, we can gather more information to improve predictions.")),(0,r.kt)("h3",{id:"notation"},"Notation"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Variables:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"X\u2081, X\u2082, X\u2083, X\u2084: Denote the four features."),(0,r.kt)("li",{parentName:"ul"},"X\u1d62\u2c7c: Refers to the jth feature of the ith training example."),(0,r.kt)("li",{parentName:"ul"},"X\u1d62: Represents the list of features for the ith training example."),(0,r.kt)("li",{parentName:"ul"},"n: Total number of features (n = 4 in this example)."))),(0,r.kt)("li",{parentName:"ul"},"Example:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"X\xb2: Vector of features for the second training example: ","[1416, 3, 2, 40]",".")))),(0,r.kt)("h3",{id:"model-with-multiple-features"},"Model with Multiple Features"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"In the original linear regression, the model was defined as f\u2092\u1d64\u209c(x) = wx + b."),(0,r.kt)("li",{parentName:"ul"},"With multiple features, the model is defined as follows:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"f\u2092\u1d64\u209c(x) = w\u2081x\u2081 + w\u2082x\u2082 + w\u2083x\u2083 + w\u2084x\u2084 + b."))),(0,r.kt)("li",{parentName:"ul"},"Example for housing price prediction:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Model: f(x) = 0.1x\u2081 + 4x\u2082 + 10x\u2083 - 2x\u2084 + 80."))),(0,r.kt)("li",{parentName:"ul"},"Parameter interpretation:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"b (80): Base price of a house with no features."),(0,r.kt)("li",{parentName:"ul"},"0.1x\u2081: Price increase per square foot."),(0,r.kt)("li",{parentName:"ul"},"4x\u2082: Price increase per bedroom."),(0,r.kt)("li",{parentName:"ul"},"10x\u2083: Price increase per floor."),(0,r.kt)("li",{parentName:"ul"},"-2x\u2084: Price decrease per year of house age.")))),(0,r.kt)("h3",{id:"general-model-with-n-features"},"General Model with n Features"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Model definition with n features: f\u2092\u1d64\u209c(x) = w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099 + b."),(0,r.kt)("li",{parentName:"ul"},"Introduction of vector notation:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"W: List of parameters ","[w\u2081, w\u2082, ..., w\u2099]","."),(0,r.kt)("li",{parentName:"ul"},"X: List of features ","[x\u2081, x\u2082, ..., x\u2099]","."))),(0,r.kt)("li",{parentName:"ul"},"Model can be rewritten as: f\u2092\u1d64\u209c(x) = W\u22c5X + b."),(0,r.kt)("li",{parentName:"ul"},"Dot product of vectors:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Dot product of W and X: W\u22c5X = w\u2081x\u2081 + w\u2082x\u2082 + ... + w\u2099x\u2099."))),(0,r.kt)("li",{parentName:"ul"},"Compact form of the model: f\u2092\u1d64\u209c(x) = W\u22c5X + b.")),(0,r.kt)("h3",{id:"multiple-linear-regression"},"Multiple Linear Regression"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Linear regression with multiple input features is called multiple linear regression."),(0,r.kt)("li",{parentName:"ul"},"It extends the concept of univariate regression, which has only one feature.")),(0,r.kt)("h3",{id:"conclusion"},"Conclusion"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Linear regression can incorporate multiple features for better predictions."),(0,r.kt)("li",{parentName:"ul"},"Multiple linear regression models are defined using vector notation."),(0,r.kt)("li",{parentName:"ul"},"The dot product of vectors simplifies the model representation."),(0,r.kt)("li",{parentName:"ul"},"Multiple linear regression extends univariate regression."),(0,r.kt)("li",{parentName:"ul"},"Vectorization is a useful technique for implementing learning algorithms.")),(0,r.kt)("h2",{id:"vectorization-in-machine-learning-part1"},"Vectorization in Machine Learning part1"),(0,r.kt)("h3",{id:"introduction-1"},"Introduction"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Vectorization is a powerful concept in implementing learning algorithms."),(0,r.kt)("li",{parentName:"ul"},"It makes code shorter and more efficient."),(0,r.kt)("li",{parentName:"ul"},"Taking advantage of numerical linear algebra libraries and GPU hardware further enhances performance.")),(0,r.kt)("h3",{id:"understanding-vectorization"},"Understanding Vectorization"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Vectorization allows concise and efficient code implementation."),(0,r.kt)("li",{parentName:"ul"},"Parameters (w) and features (x) are represented as vectors."),(0,r.kt)("li",{parentName:"ul"},"Indexing in Python starts from 0, unlike linear algebra where it starts from 1."),(0,r.kt)("li",{parentName:"ul"},"NumPy library is widely used for numerical linear algebra operations in Python and machine learning.")),(0,r.kt)("h3",{id:"non-vectorized-implementation"},"Non-Vectorized Implementation"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Non-vectorized code involves multiplying each parameter with its associated feature."),(0,r.kt)("li",{parentName:"ul"},"It becomes inefficient when the number of features (n) is large."),(0,r.kt)("li",{parentName:"ul"},"Using a for loop and summation can improve efficiency but still lacks vectorization.")),(0,r.kt)("h3",{id:"vectorized-implementation"},"Vectorized Implementation"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Vectorized code leverages the dot product operation."),(0,r.kt)("li",{parentName:"ul"},"The dot product is computed using the NumPy dot function."),(0,r.kt)("li",{parentName:"ul"},"The dot function is a vectorized implementation of the dot product."),(0,r.kt)("li",{parentName:"ul"},"It significantly improves efficiency, especially with large values of n.")),(0,r.kt)("h3",{id:"benefits-of-vectorization"},"Benefits of Vectorization"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Vectorization has two main benefits:",(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Shorter code: It reduces the code to a single line."),(0,r.kt)("li",{parentName:"ul"},"Faster execution: It takes advantage of parallel hardware, such as GPUs."))),(0,r.kt)("li",{parentName:"ul"},"The NumPy dot function utilizes parallel hardware, making it more computationally efficient.")),(0,r.kt)("h3",{id:"conclusion-1"},"Conclusion"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Vectorization is a powerful technique in machine learning."),(0,r.kt)("li",{parentName:"ul"},"It simplifies code and improves its performance."),(0,r.kt)("li",{parentName:"ul"},"By leveraging numerical linear algebra libraries and parallel hardware, vectorized code achieves faster execution.")),(0,r.kt)("h2",{id:"vectorization-in-machine-learning-part2"},"Vectorization in Machine Learning part2"),(0,r.kt)("h3",{id:"introduction-2"},"Introduction"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Vectorization is a technique that significantly improves the efficiency of machine learning algorithms."),(0,r.kt)("li",{parentName:"ul"},"It involves performing operations on vectors rather than using explicit for loops."),(0,r.kt)("li",{parentName:"ul"},"Vectorized code is concise, easier to understand, and utilizes parallel hardware for faster execution.")),(0,r.kt)("h3",{id:"understanding-vectorization-1"},"Understanding Vectorization"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Vectorization allows for efficient implementation of algorithms by operating on entire vectors simultaneously."),(0,r.kt)("li",{parentName:"ul"},"Non-vectorized code performs operations one step at a time, resulting in slower execution."),(0,r.kt)("li",{parentName:"ul"},"Vectorized code takes advantage of parallel processing hardware to perform calculations in a single step.")),(0,r.kt)("h3",{id:"benefits-of-vectorization-1"},"Benefits of Vectorization"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Code Efficiency: Vectorized code is shorter and more concise, reducing the number of lines required."),(0,r.kt)("li",{parentName:"ul"},"Faster Execution: By leveraging parallel hardware, vectorized code performs calculations much faster than non-vectorized code."),(0,r.kt)("li",{parentName:"ul"},"Scalability: Vectorization is crucial for handling large datasets and training complex models.")),(0,r.kt)("h3",{id:"example-multiple-linear-regression"},"Example: Multiple Linear Regression"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"In multiple linear regression, vectorization plays a key role in updating the parameters efficiently."),(0,r.kt)("li",{parentName:"ul"},"Without vectorization, each parameter is updated individually using a for loop."),(0,r.kt)("li",{parentName:"ul"},"With vectorization, the computer performs parallel computations to update all parameters simultaneously.")),(0,r.kt)("h3",{id:"implementation-with-numpy"},"Implementation with NumPy"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"NumPy is a popular Python library for numerical computations and vectorization."),(0,r.kt)("li",{parentName:"ul"},"It provides functions like dot product (dot) that operate on vectors efficiently."),(0,r.kt)("li",{parentName:"ul"},"NumPy arrays, or vectors, are used to store and manipulate data in a vectorized manner.")),(0,r.kt)("h3",{id:"benefits-of-vectorization-in-machine-learning"},"Benefits of Vectorization in Machine Learning"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Improved Efficiency: Vectorization significantly speeds up the execution of learning algorithms."),(0,r.kt)("li",{parentName:"ul"},"Handling Large Datasets: Vectorized code is essential for processing large amounts of data effectively."),(0,r.kt)("li",{parentName:"ul"},"Scaling Machine Learning Algorithms: Vectorization enables algorithms to scale well to handle complex models and datasets.")),(0,r.kt)("h3",{id:"conclusion-2"},"Conclusion"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Vectorization is a powerful technique in machine learning for efficient algorithm implementation."),(0,r.kt)("li",{parentName:"ul"},"It leverages parallel processing hardware and libraries like NumPy to perform calculations on vectors."),(0,r.kt)("li",{parentName:"ul"},"Vectorized code is concise, faster, and crucial for handling large datasets and complex models.")),(0,r.kt)("h2",{id:"gradient-descent-for-multiple-linear-regression-with-vectorization"},"Gradient Descent for Multiple Linear Regression with Vectorization"),(0,r.kt)("h3",{id:"introduction-3"},"Introduction"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Review of multiple linear regression and vector notation."),(0,r.kt)("li",{parentName:"ul"},"Applying gradient descent to multiple linear regression with vectorization.")),(0,r.kt)("h3",{id:"multiple-linear-regression-in-vector-notation"},"Multiple Linear Regression in Vector Notation"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Parameters w_1 to w_n and b are combined into a parameter vector w."),(0,r.kt)("li",{parentName:"ul"},"Model: f_w, b(x) = w\u22c5x + b (dot product notation)."),(0,r.kt)("li",{parentName:"ul"},"Cost function: J(w, b).")),(0,r.kt)("h3",{id:"gradient-descent-for-multiple-linear-regression"},"Gradient Descent for Multiple Linear Regression"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Update rule for each parameter w_j: w_j = w_j - \u03b1 ","*"," \u2202J/\u2202w_j."),(0,r.kt)("li",{parentName:"ul"},"Update rule for b: b = b - \u03b1 ","*"," \u2202J/\u2202b."),(0,r.kt)("li",{parentName:"ul"},"Derivative of J with respect to w",(0,r.kt)("em",{parentName:"li"},"1: \u2202J/\u2202w_1 = (1/m) ")," \u2211(f",(0,r.kt)("em",{parentName:"li"},"w,b(x) - y) ")," x_1."),(0,r.kt)("li",{parentName:"ul"},"For multiple features (n > 1): \u2202J/\u2202w",(0,r.kt)("em",{parentName:"li"},"j = (1/m) ")," \u2211(f",(0,r.kt)("em",{parentName:"li"},"w,b(x) - y) ")," x_j."),(0,r.kt)("li",{parentName:"ul"},"Update all parameters w_1 to w_n and b iteratively.")),(0,r.kt)("h3",{id:"normal-equation-method-side-note"},"Normal Equation Method (Side Note)"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Alternative method to find w and b for linear regression."),(0,r.kt)("li",{parentName:"ul"},"Solves for w and b directly without iterations."),(0,r.kt)("li",{parentName:"ul"},"Only applicable to linear regression."),(0,r.kt)("li",{parentName:"ul"},"Not generalized to other learning algorithms."),(0,r.kt)("li",{parentName:"ul"},"Slow for large number of features."),(0,r.kt)("li",{parentName:"ul"},"Often used in mature machine learning libraries.")),(0,r.kt)("h3",{id:"conclusion-3"},"Conclusion"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Gradient descent with vectorization is a powerful approach for multiple linear regression."),(0,r.kt)("li",{parentName:"ul"},"Parameters are updated iteratively using derivative calculations."),(0,r.kt)("li",{parentName:"ul"},"Normal equation method provides an alternative, but less flexible, approach."),(0,r.kt)("li",{parentName:"ul"},"Gradient descent is widely used and offers better scalability and generality.")))}c.isMDXComponent=!0}}]);