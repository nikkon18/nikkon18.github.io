"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[5588],{3905:(e,n,t)=>{t.d(n,{Zo:()=>c,kt:()=>g});var a=t(7294);function i(e,n,t){return n in e?Object.defineProperty(e,n,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[n]=t,e}function r(e,n){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);n&&(a=a.filter((function(n){return Object.getOwnPropertyDescriptor(e,n).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var n=1;n<arguments.length;n++){var t=null!=arguments[n]?arguments[n]:{};n%2?r(Object(t),!0).forEach((function(n){i(e,n,t[n])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(n){Object.defineProperty(e,n,Object.getOwnPropertyDescriptor(t,n))}))}return e}function o(e,n){if(null==e)return{};var t,a,i=function(e,n){if(null==e)return{};var t,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||(i[t]=e[t]);return i}(e,n);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],n.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(i[t]=e[t])}return i}var s=a.createContext({}),u=function(e){var n=a.useContext(s),t=n;return e&&(t="function"==typeof e?e(n):l(l({},n),e)),t},c=function(e){var n=u(e.components);return a.createElement(s.Provider,{value:n},e.children)},d={inlineCode:"code",wrapper:function(e){var n=e.children;return a.createElement(a.Fragment,{},n)}},m=a.forwardRef((function(e,n){var t=e.components,i=e.mdxType,r=e.originalType,s=e.parentName,c=o(e,["components","mdxType","originalType","parentName"]),m=u(t),g=i,p=m["".concat(s,".").concat(g)]||m[g]||d[g]||r;return t?a.createElement(p,l(l({ref:n},c),{},{components:t})):a.createElement(p,l({ref:n},c))}));function g(e,n){var t=arguments,i=n&&n.mdxType;if("string"==typeof e||i){var r=t.length,l=new Array(r);l[0]=m;var o={};for(var s in n)hasOwnProperty.call(n,s)&&(o[s]=n[s]);o.originalType=e,o.mdxType="string"==typeof e?e:i,l[1]=o;for(var u=2;u<r;u++)l[u]=t[u];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}m.displayName="MDXCreateElement"},772:(e,n,t)=>{t.r(n),t.d(n,{frontMatter:()=>r,contentTitle:()=>l,metadata:()=>o,toc:()=>s,default:()=>c});var a=t(7462),i=(t(7294),t(3905));const r={},l="5 - Gradient descent in practice",o={unversionedId:"Supervised Machine Learning/Gradient descent in practice",id:"Supervised Machine Learning/Gradient descent in practice",title:"5 - Gradient descent in practice",description:"Feature Scaling for Improved Gradient Descent",source:"@site/docs/Supervised Machine Learning/5-Gradient descent in practice.md",sourceDirName:"Supervised Machine Learning",slug:"/Supervised Machine Learning/Gradient descent in practice",permalink:"/docs/Supervised Machine Learning/Gradient descent in practice",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/5-Gradient descent in practice.md",tags:[],version:"current",sidebarPosition:5,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"4 - Mutiple linear regression",permalink:"/docs/Supervised Machine Learning/Mutiple linear regression"},next:{title:"6 - classification with logistic regression",permalink:"/docs/Supervised Machine Learning/classification with logistic regression"}},s=[{value:"Feature Scaling for Improved Gradient Descent",id:"feature-scaling-for-improved-gradient-descent",children:[{value:"Introduction",id:"introduction",children:[],level:3},{value:"Understanding the Impact of Feature Size",id:"understanding-the-impact-of-feature-size",children:[],level:3},{value:"Effect of Parameter Choices",id:"effect-of-parameter-choices",children:[],level:3},{value:"Relationship to Gradient Descent",id:"relationship-to-gradient-descent",children:[],level:3},{value:"Importance of Feature Scaling",id:"importance-of-feature-scaling",children:[],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2},{value:"Feature Scaling for Improved Gradient Descent",id:"feature-scaling-for-improved-gradient-descent-1",children:[{value:"Introduction",id:"introduction-1",children:[],level:3},{value:"Maximum Scaling",id:"maximum-scaling",children:[],level:3},{value:"Mean Normalization",id:"mean-normalization",children:[],level:3},{value:"Z-score Normalization",id:"z-score-normalization",children:[],level:3},{value:"Importance of Feature Scaling",id:"importance-of-feature-scaling-1",children:[],level:3},{value:"Conclusion",id:"conclusion-1",children:[],level:3}],level:2},{value:"Understanding Convergence of Gradient Descent",id:"understanding-convergence-of-gradient-descent",children:[{value:"Introduction",id:"introduction-2",children:[],level:3},{value:"Learning Curve",id:"learning-curve",children:[],level:3},{value:"Determining Convergence",id:"determining-convergence",children:[],level:3},{value:"Choosing the Right Threshold",id:"choosing-the-right-threshold",children:[],level:3},{value:"Conclusion",id:"conclusion-2",children:[],level:3}],level:2},{value:"Choosing the Learning Rate in Gradient Descent",id:"choosing-the-learning-rate-in-gradient-descent",children:[{value:"Introduction",id:"introduction-3",children:[],level:3},{value:"Effects of Learning Rate",id:"effects-of-learning-rate",children:[],level:3},{value:"Debugging Gradient Descent",id:"debugging-gradient-descent",children:[],level:3},{value:"Debugging Tip",id:"debugging-tip",children:[],level:3},{value:"Choosing the Learning Rate",id:"choosing-the-learning-rate",children:[],level:3},{value:"Technique for Choosing Alpha",id:"technique-for-choosing-alpha",children:[],level:3},{value:"Importance of Experimentation",id:"importance-of-experimentation",children:[],level:3},{value:"Conclusion",id:"conclusion-3",children:[],level:3}],level:2},{value:"Feature Engineering for Improved Learning Algorithm Performance",id:"feature-engineering-for-improved-learning-algorithm-performance",children:[{value:"Introduction",id:"introduction-4",children:[],level:3},{value:"Initial Features",id:"initial-features",children:[],level:3},{value:"Introducing a New Feature",id:"introducing-a-new-feature",children:[],level:3},{value:"Feature Engineering",id:"feature-engineering",children:[],level:3},{value:"Benefits of Feature Engineering",id:"benefits-of-feature-engineering",children:[],level:3},{value:"Conclusion",id:"conclusion-4",children:[],level:3}],level:2},{value:"Polynomial Regression and Feature Engineering",id:"polynomial-regression-and-feature-engineering",children:[{value:"Introduction",id:"introduction-5",children:[],level:3},{value:"Example: Housing Dataset",id:"example-housing-dataset",children:[],level:3},{value:"Polynomial Regression",id:"polynomial-regression",children:[],level:3},{value:"Choosing Features",id:"choosing-features",children:[],level:3},{value:"Alternative Features",id:"alternative-features",children:[],level:3},{value:"Importance of Feature Engineering",id:"importance-of-feature-engineering",children:[],level:3},{value:"Practical Implementation",id:"practical-implementation",children:[],level:3},{value:"Conclusion",id:"conclusion-5",children:[],level:3}],level:2}],u={toc:s};function c(e){let{components:n,...t}=e;return(0,i.kt)("wrapper",(0,a.Z)({},u,t,{components:n,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"5---gradient-descent-in-practice"},"5 - Gradient descent in practice"),(0,i.kt)("h2",{id:"feature-scaling-for-improved-gradient-descent"},"Feature Scaling for Improved Gradient Descent"),(0,i.kt)("h3",{id:"introduction"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feature scaling can enhance the performance of gradient descent."),(0,i.kt)("li",{parentName:"ul"},"Scaling ensures that features with different value ranges do not hinder convergence."),(0,i.kt)("li",{parentName:"ul"},"Rescaling features to comparable ranges enables faster gradient descent.")),(0,i.kt)("h3",{id:"understanding-the-impact-of-feature-size"},"Understanding the Impact of Feature Size"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Example: Predicting house prices using size (x1) and number of bedrooms (x2)."),(0,i.kt)("li",{parentName:"ul"},"x1 ranges from 300 to 2000 square feet, while x2 ranges from 0 to 5 bedrooms."),(0,i.kt)("li",{parentName:"ul"},"Parameters: w1, w2, and b.")),(0,i.kt)("h3",{id:"effect-of-parameter-choices"},"Effect of Parameter Choices"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Incorrect parameter choices can lead to inaccurate predictions."),(0,i.kt)("li",{parentName:"ul"},"Example: For a house with 2000 sq. ft., 5 bedrooms, and $500,000 price:",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Incorrect parameters: w1 = 50, w2 = 0.1, b = 50.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Predicted price: $100,150,050 (far from actual price)."))),(0,i.kt)("li",{parentName:"ul"},"Correct parameters: w1 = 0.1, w2 = 50, b = 50.",(0,i.kt)("ul",{parentName:"li"},(0,i.kt)("li",{parentName:"ul"},"Predicted price: $500,000 (reasonable estimate).")))))),(0,i.kt)("h3",{id:"relationship-to-gradient-descent"},"Relationship to Gradient Descent"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feature size affects parameter choices and convergence."),(0,i.kt)("li",{parentName:"ul"},"Scatter plot: x1 (size) vs. x2 (bedrooms)."),(0,i.kt)("li",{parentName:"ul"},"Contour plot of cost function: x1 (0-1) vs. x2 (10-100)."),(0,i.kt)("li",{parentName:"ul"},"Tall and skinny contours for large x1 range, small x2 range."),(0,i.kt)("li",{parentName:"ul"},"Small changes in w1 have a significant impact on predictions and cost (J)."),(0,i.kt)("li",{parentName:"ul"},"Gradient descent may oscillate before reaching the global minimum.")),(0,i.kt)("h3",{id:"importance-of-feature-scaling"},"Importance of Feature Scaling"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Scaling features mitigates issues with varying value ranges."),(0,i.kt)("li",{parentName:"ul"},"Rescale x1 and x2 to comparable ranges (e.g., 0-1)."),(0,i.kt)("li",{parentName:"ul"},"Transformed data leads to circular contours in the cost function."),(0,i.kt)("li",{parentName:"ul"},"Gradient descent follows a more direct path to the global minimum.")),(0,i.kt)("h3",{id:"conclusion"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feature scaling is crucial for efficient gradient descent."),(0,i.kt)("li",{parentName:"ul"},"Features with disparate value ranges can impede convergence."),(0,i.kt)("li",{parentName:"ul"},"Rescaling features to comparable ranges enhances the performance of gradient descent."),(0,i.kt)("li",{parentName:"ul"},"Scaling ensures that all features contribute equally to the learning process.")),(0,i.kt)("h2",{id:"feature-scaling-for-improved-gradient-descent-1"},"Feature Scaling for Improved Gradient Descent"),(0,i.kt)("h3",{id:"introduction-1"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feature scaling improves gradient descent performance."),(0,i.kt)("li",{parentName:"ul"},"Scaling makes features with different value ranges comparable."),(0,i.kt)("li",{parentName:"ul"},"Scaling methods: Maximum scaling, mean normalization, Z-score normalization.")),(0,i.kt)("h3",{id:"maximum-scaling"},"Maximum Scaling"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Divide each feature value by its maximum to obtain a scaled version."),(0,i.kt)("li",{parentName:"ul"},"Example: x1 ranges from 3-2000, x2 ranges from 0-5."),(0,i.kt)("li",{parentName:"ul"},"Scale x1: Divide each value by 2000."),(0,i.kt)("li",{parentName:"ul"},"Scale x2: Divide each value by 5."),(0,i.kt)("li",{parentName:"ul"},"Scaled features range from 0-1.")),(0,i.kt)("h3",{id:"mean-normalization"},"Mean Normalization"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Center features around zero by subtracting the mean."),(0,i.kt)("li",{parentName:"ul"},"Divide by the range of values to normalize."),(0,i.kt)("li",{parentName:"ul"},"Example: x1 mean = 600 sq. ft., x2 mean = 2.3 bedrooms."),(0,i.kt)("li",{parentName:"ul"},"Normalize x1: Subtract mean and divide by (2000-300)."),(0,i.kt)("li",{parentName:"ul"},"Normalize x2: Subtract mean and divide by (5-0)."),(0,i.kt)("li",{parentName:"ul"},"Normalized features range from -1 to 1.")),(0,i.kt)("h3",{id:"z-score-normalization"},"Z-score Normalization"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Use the mean and standard deviation to normalize features."),(0,i.kt)("li",{parentName:"ul"},"Calculate mean (Mu) and standard deviation (Sigma) for each feature."),(0,i.kt)("li",{parentName:"ul"},"Example: x1 Mu = 600, Sigma = 450; x2 Mu = 2.3, Sigma = 1.4."),(0,i.kt)("li",{parentName:"ul"},"Normalize x1: Subtract Mu and divide by Sigma."),(0,i.kt)("li",{parentName:"ul"},"Normalize x2: Subtract Mu and divide by Sigma."),(0,i.kt)("li",{parentName:"ul"},"Normalized features have a mean of 0 and standard deviation of 1.")),(0,i.kt)("h3",{id:"importance-of-feature-scaling-1"},"Importance of Feature Scaling"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Scaling facilitates faster convergence of gradient descent."),(0,i.kt)("li",{parentName:"ul"},"Features with similar ranges prevent oscillation and improve performance."),(0,i.kt)("li",{parentName:"ul"},"Aim for features to range between -1 and 1 (flexible guideline)."),(0,i.kt)("li",{parentName:"ul"},"Scaling handles disparate ranges and enhances algorithm efficiency.")),(0,i.kt)("h3",{id:"conclusion-1"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feature scaling is essential for gradient descent effectiveness."),(0,i.kt)("li",{parentName:"ul"},"Scaling methods include maximum scaling, mean normalization, and Z-score normalization."),(0,i.kt)("li",{parentName:"ul"},"Scaling ensures features contribute equally and aids convergence."),(0,i.kt)("li",{parentName:"ul"},"Implementing feature scaling improves the performance of machine learning models.")),(0,i.kt)("h2",{id:"understanding-convergence-of-gradient-descent"},"Understanding Convergence of Gradient Descent"),(0,i.kt)("h3",{id:"introduction-2"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Convergence refers to the process of finding parameters close to the global minimum of the cost function."),(0,i.kt)("li",{parentName:"ul"},"Recognizing convergence helps in selecting an appropriate learning rate (Alpha)."),(0,i.kt)("li",{parentName:"ul"},"Plotting the cost function J at each iteration aids in assessing convergence.")),(0,i.kt)("h3",{id:"learning-curve"},"Learning Curve"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Plot J against the number of iterations of gradient descent."),(0,i.kt)("li",{parentName:"ul"},"Horizontal axis: Number of iterations."),(0,i.kt)("li",{parentName:"ul"},"Vertical axis: Cost J."),(0,i.kt)("li",{parentName:"ul"},"Learning curve shows the change in J after each iteration."),(0,i.kt)("li",{parentName:"ul"},"Ideally, J should decrease with each iteration."),(0,i.kt)("li",{parentName:"ul"},"If J increases after an iteration, Alpha may be too large or there may be a bug in the code."),(0,i.kt)("li",{parentName:"ul"},"Convergence is indicated when the curve levels off and J no longer decreases significantly.")),(0,i.kt)("h3",{id:"determining-convergence"},"Determining Convergence"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Examining the learning curve helps identify if gradient descent is converging."),(0,i.kt)("li",{parentName:"ul"},"Convergence iterations vary across applications."),(0,i.kt)("li",{parentName:"ul"},"Creating a learning curve graph assists in determining when training can be stopped."),(0,i.kt)("li",{parentName:"ul"},"Automatic convergence tests can also be employed."),(0,i.kt)("li",{parentName:"ul"},"Use a small number epsilon as a threshold for J decrease in one iteration."),(0,i.kt)("li",{parentName:"ul"},"If J decreases by less than epsilon, convergence is declared.")),(0,i.kt)("h3",{id:"choosing-the-right-threshold"},"Choosing the Right Threshold"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Selecting the appropriate epsilon threshold can be challenging."),(0,i.kt)("li",{parentName:"ul"},"Graphical analysis of the learning curve provides insights into convergence."),(0,i.kt)("li",{parentName:"ul"},"Observing the learning curve graph warns of potential issues with gradient descent.")),(0,i.kt)("h3",{id:"conclusion-2"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Convergence is crucial in finding optimal parameter values."),(0,i.kt)("li",{parentName:"ul"},"Learning curves display the change in cost J over iterations."),(0,i.kt)("li",{parentName:"ul"},"Proper convergence is characterized by a decreasing J."),(0,i.kt)("li",{parentName:"ul"},"Choosing the right learning rate and threshold epsilon is essential for successful convergence.")),(0,i.kt)("h2",{id:"choosing-the-learning-rate-in-gradient-descent"},"Choosing the Learning Rate in Gradient Descent"),(0,i.kt)("h3",{id:"introduction-3"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The learning rate (Alpha) selection is crucial for effective gradient descent."),(0,i.kt)("li",{parentName:"ul"},"A small Alpha leads to slow convergence, while a large Alpha may prevent convergence."),(0,i.kt)("li",{parentName:"ul"},"Plotting the cost function over iterations helps in identifying issues with the learning rate.")),(0,i.kt)("h3",{id:"effects-of-learning-rate"},"Effects of Learning Rate"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"If the learning rate is too large, the cost may increase instead of decreasing."),(0,i.kt)("li",{parentName:"ul"},"Overshooting the minimum due to large updates causes this behavior."),(0,i.kt)("li",{parentName:"ul"},"A smaller learning rate helps reduce overshooting and ensures gradual descent towards the minimum.")),(0,i.kt)("h3",{id:"debugging-gradient-descent"},"Debugging Gradient Descent"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Consistently increasing costs indicate a possible issue with the learning rate."),(0,i.kt)("li",{parentName:"ul"},"Incorrectly updating parameters using the derivative term can lead to this problem."),(0,i.kt)("li",{parentName:"ul"},"Ensure the code updates parameters as w_1 updated by w_1 - Alpha ","*"," derivative term.")),(0,i.kt)("h3",{id:"debugging-tip"},"Debugging Tip"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Set Alpha to a very small value to debug gradient descent."),(0,i.kt)("li",{parentName:"ul"},"If the cost doesn't decrease on every iteration, there may be a bug in the code."),(0,i.kt)("li",{parentName:"ul"},"Note that using a very small Alpha is not efficient for actual training.")),(0,i.kt)("h3",{id:"choosing-the-learning-rate"},"Choosing the Learning Rate"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"A trade-off exists between convergence speed and the learning rate's size."),(0,i.kt)("li",{parentName:"ul"},"Try a range of values for Alpha and observe the cost function's behavior."),(0,i.kt)("li",{parentName:"ul"},"Plot the cost function J against the number of iterations for each Alpha."),(0,i.kt)("li",{parentName:"ul"},"Select a learning rate that consistently and rapidly decreases the cost.")),(0,i.kt)("h3",{id:"technique-for-choosing-alpha"},"Technique for Choosing Alpha"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Start with a small learning rate, e.g., 0.001, and run gradient descent for a few iterations."),(0,i.kt)("li",{parentName:"ul"},"Increase Alpha gradually, e.g., try 0.003 (three times larger than the previous Alpha)."),(0,i.kt)("li",{parentName:"ul"},"Continue this process, trying larger values like 0.01 (roughly three times larger than the previous Alpha)."),(0,i.kt)("li",{parentName:"ul"},"Find the smallest Alpha that causes the cost to increase and the largest reasonable Alpha."),(0,i.kt)("li",{parentName:"ul"},"Choose a learning rate slightly smaller than the largest reasonable value found.")),(0,i.kt)("h3",{id:"importance-of-experimentation"},"Importance of Experimentation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Trying different learning rates helps in finding the optimal choice."),(0,i.kt)("li",{parentName:"ul"},"It is important to experiment and gain intuition about different Alpha values."),(0,i.kt)("li",{parentName:"ul"},"Feature scaling and the learning rate's impact can be explored in the optional lab.")),(0,i.kt)("h3",{id:"conclusion-3"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Choosing the right learning rate is crucial for effective gradient descent."),(0,i.kt)("li",{parentName:"ul"},"A large learning rate can cause overshooting and prevent convergence."),(0,i.kt)("li",{parentName:"ul"},"Debugging and experimentation help in finding an appropriate learning rate."),(0,i.kt)("li",{parentName:"ul"},"Balancing convergence speed and the learning rate's size is essential for successful training.")),(0,i.kt)("h2",{id:"feature-engineering-for-improved-learning-algorithm-performance"},"Feature Engineering for Improved Learning Algorithm Performance"),(0,i.kt)("h3",{id:"introduction-4"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Choosing or engineering the right features is crucial for a learning algorithm's success."),(0,i.kt)("li",{parentName:"ul"},"Feature engineering involves transforming or combining existing features to improve predictions."),(0,i.kt)("li",{parentName:"ul"},"We'll explore feature engineering using the example of predicting house prices.")),(0,i.kt)("h3",{id:"initial-features"},"Initial Features"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Two features for each house: x_1 (frontage/width) and x_2 (depth)."),(0,i.kt)("li",{parentName:"ul"},"Model: f(x) = w_1x_1 + w_2x_2 + b, where x_1 is frontage and x_2 is depth.")),(0,i.kt)("h3",{id:"introducing-a-new-feature"},"Introducing a New Feature"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Area of the land can be a more predictive feature than frontage and depth individually."),(0,i.kt)("li",{parentName:"ul"},"Define a new feature, x_3, as x_1 ","*"," x_2 (area of the plot of land)."),(0,i.kt)("li",{parentName:"ul"},"Updated model: f_w,b(x) = w_1x_1 + w_2x_2 + w_3x_3 + b."),(0,i.kt)("li",{parentName:"ul"},"The model can now choose parameters w_1, w_2, and w_3 based on feature importance.")),(0,i.kt)("h3",{id:"feature-engineering"},"Feature Engineering"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feature engineering involves transforming or combining features to aid the learning algorithm."),(0,i.kt)("li",{parentName:"ul"},"Use domain knowledge or intuition to design new features for better predictions."),(0,i.kt)("li",{parentName:"ul"},"Creating new features can significantly improve the model's performance.")),(0,i.kt)("h3",{id:"benefits-of-feature-engineering"},"Benefits of Feature Engineering"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Enables fitting curves and non-linear functions, not just straight lines."),(0,i.kt)("li",{parentName:"ul"},"Helps capture complex relationships and patterns in the data.")),(0,i.kt)("h3",{id:"conclusion-4"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feature engineering plays a vital role in improving learning algorithm performance."),(0,i.kt)("li",{parentName:"ul"},"By creating new features based on domain knowledge or intuition, we can enhance predictions."),(0,i.kt)("li",{parentName:"ul"},"Feature engineering allows fitting curves and non-linear functions to capture complex patterns.")),(0,i.kt)("h2",{id:"polynomial-regression-and-feature-engineering"},"Polynomial Regression and Feature Engineering"),(0,i.kt)("h3",{id:"introduction-5"},"Introduction"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Fitting straight lines may not be sufficient for certain datasets."),(0,i.kt)("li",{parentName:"ul"},"Polynomial regression combines feature engineering and multiple linear regression to fit curves."),(0,i.kt)("li",{parentName:"ul"},"Polynomial regression allows for non-linear functions to be fitted to the data.")),(0,i.kt)("h3",{id:"example-housing-dataset"},"Example: Housing Dataset"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Dataset with feature x as the size of houses in square feet."),(0,i.kt)("li",{parentName:"ul"},"Straight lines do not fit the data well."),(0,i.kt)("li",{parentName:"ul"},"The need for curves motivates quadratic and cubic functions.")),(0,i.kt)("h3",{id:"polynomial-regression"},"Polynomial Regression"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Quadratic function example: f(x) = w",(0,i.kt)("em",{parentName:"li"},"1 ")," x + w",(0,i.kt)("em",{parentName:"li"},"2 ")," x^2 + b."),(0,i.kt)("li",{parentName:"ul"},"Cubic function example: f(x) = w",(0,i.kt)("em",{parentName:"li"},"1 ")," x + w",(0,i.kt)("em",{parentName:"li"},"2 ")," x^2 + w_3 ","*"," x^3 + b."),(0,i.kt)("li",{parentName:"ul"},"Additional features are created by raising the original feature to a power."),(0,i.kt)("li",{parentName:"ul"},"Feature scaling becomes important when using higher power features.")),(0,i.kt)("h3",{id:"choosing-features"},"Choosing Features"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The choice of features depends on the problem and desired model."),(0,i.kt)("li",{parentName:"ul"},"Different models with different features can be compared for performance."),(0,i.kt)("li",{parentName:"ul"},"Feature engineering allows for flexibility in selecting features.")),(0,i.kt)("h3",{id:"alternative-features"},"Alternative Features"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Other choices for features include square root functions."),(0,i.kt)("li",{parentName:"ul"},"Example: f(x) = w",(0,i.kt)("em",{parentName:"li"},"1 ")," x + w",(0,i.kt)("em",{parentName:"li"},"2 ")," sqrt(x) + b."),(0,i.kt)("li",{parentName:"ul"},"Feature engineering provides a wide range of feature options.")),(0,i.kt)("h3",{id:"importance-of-feature-engineering"},"Importance of Feature Engineering"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Feature engineering improves model performance and accuracy."),(0,i.kt)("li",{parentName:"ul"},"Polynomial regression enables better modeling of complex relationships.")),(0,i.kt)("h3",{id:"practical-implementation"},"Practical Implementation"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Scikit-learn is a popular machine learning library for implementing regression models."),(0,i.kt)("li",{parentName:"ul"},"Familiarity with implementing algorithms is important alongside using libraries.")),(0,i.kt)("h3",{id:"conclusion-5"},"Conclusion"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"Polynomial regression combines feature engineering and multiple linear regression."),(0,i.kt)("li",{parentName:"ul"},"Feature engineering allows fitting curves and non-linear functions to data."),(0,i.kt)("li",{parentName:"ul"},"Scikit-learn is a valuable tool for practical implementation."),(0,i.kt)("li",{parentName:"ul"},"Understanding algorithm implementation is crucial for model understanding and effective usage.")))}c.isMDXComponent=!0}}]);