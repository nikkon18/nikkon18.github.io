"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[4872],{3905:(e,t,i)=>{i.d(t,{Zo:()=>c,kt:()=>m});var n=i(7294);function a(e,t,i){return t in e?Object.defineProperty(e,t,{value:i,enumerable:!0,configurable:!0,writable:!0}):e[t]=i,e}function o(e,t){var i=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),i.push.apply(i,n)}return i}function r(e){for(var t=1;t<arguments.length;t++){var i=null!=arguments[t]?arguments[t]:{};t%2?o(Object(i),!0).forEach((function(t){a(e,t,i[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(i)):o(Object(i)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(i,t))}))}return e}function l(e,t){if(null==e)return{};var i,n,a=function(e,t){if(null==e)return{};var i,n,a={},o=Object.keys(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||(a[i]=e[i]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)i=o[n],t.indexOf(i)>=0||Object.prototype.propertyIsEnumerable.call(e,i)&&(a[i]=e[i])}return a}var s=n.createContext({}),u=function(e){var t=n.useContext(s),i=t;return e&&(i="function"==typeof e?e(t):r(r({},t),e)),i},c=function(e){var t=u(e.components);return n.createElement(s.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},h=n.forwardRef((function(e,t){var i=e.components,a=e.mdxType,o=e.originalType,s=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),h=u(i),m=a,p=h["".concat(s,".").concat(m)]||h[m]||d[m]||o;return i?n.createElement(p,r(r({ref:t},c),{},{components:i})):n.createElement(p,r({ref:t},c))}));function m(e,t){var i=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=i.length,r=new Array(o);r[0]=h;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,r[1]=l;for(var u=2;u<o;u++)r[u]=i[u];return n.createElement.apply(null,r)}return n.createElement.apply(null,i)}h.displayName="MDXCreateElement"},5763:(e,t,i)=>{i.r(t),i.d(t,{frontMatter:()=>o,contentTitle:()=>r,metadata:()=>l,toc:()=>s,default:()=>c});var n=i(7462),a=(i(7294),i(3905));const o={},r="2 - Regression Model",l={unversionedId:"Supervised Machine Learning/Regression Model",id:"Supervised Machine Learning/Regression Model",title:"2 - Regression Model",description:"Linear Regression lesson1",source:"@site/docs/Supervised Machine Learning/2-Regression Model.md",sourceDirName:"Supervised Machine Learning",slug:"/Supervised Machine Learning/Regression Model",permalink:"/docs/Supervised Machine Learning/Regression Model",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/2-Regression Model.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"1 - Overview of Machine learning",permalink:"/docs/Supervised Machine Learning/Overview of Machine learning"},next:{title:"3 - Train the model with gradient descent",permalink:"/docs/Supervised Machine Learning/Train the model with gradient descent"}},s=[{value:"Linear Regression lesson1",id:"linear-regression-lesson1",children:[{value:"Introduction",id:"introduction",children:[],level:3},{value:"Linear Regression Model",id:"linear-regression-model",children:[],level:3},{value:"Regression vs. Classification",id:"regression-vs-classification",children:[],level:3},{value:"Training Set",id:"training-set",children:[],level:3},{value:"Notation for Describing Data",id:"notation-for-describing-data",children:[],level:3}],level:2},{value:"Linear Regression lesson2",id:"linear-regression-lesson2",children:[{value:"Introduction",id:"introduction-1",children:[],level:3},{value:"Linear Regression",id:"linear-regression",children:[],level:3},{value:"Cost Function",id:"cost-function",children:[],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2},{value:"Cost function formula",id:"cost-function-formula",children:[{value:"Introduction",id:"introduction-2",children:[],level:3},{value:"Cost Function",id:"cost-function-1",children:[],level:3},{value:"Conclusion",id:"conclusion-1",children:[],level:3}],level:2},{value:"Cost Function Intuition",id:"cost-function-intuition",children:[{value:"The cost function J measures how well a choice of w and b fit the training data.",id:"the-cost-function-j-measures-how-well-a-choice-of-w-and-b-fit-the-training-data",children:[],level:3},{value:"J(w, b) = 1/2m * \u03a3(i=1 to m) (fw(x(i)) - y(i))^2",id:"jw-b--12m--\u03c3i1-to-m-fwxi---yi2",children:[],level:3},{value:"Minimizing the Cost Function",id:"minimizing-the-cost-function",children:[],level:3},{value:"Visualizing the Cost Function",id:"visualizing-the-cost-function",children:[],level:3},{value:"Conclusion",id:"conclusion-2",children:[],level:3}],level:2}],u={toc:s};function c(e){let{components:t,...i}=e;return(0,a.kt)("wrapper",(0,n.Z)({},u,i,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"2---regression-model"},"2 - Regression Model"),(0,a.kt)("h2",{id:"linear-regression-lesson1"},"Linear Regression lesson1"),(0,a.kt)("h3",{id:"introduction"},"Introduction"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Supervised learning is a process of training a model using a dataset with labeled outputs."),(0,a.kt)("li",{parentName:"ul"},"Linear regression is a type of supervised learning model that predicts numerical outputs, such as the price of a house based on its size.")),(0,a.kt)("h3",{id:"linear-regression-model"},"Linear Regression Model"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Linear regression model fits a straight line to the data."),(0,a.kt)("li",{parentName:"ul"},"The model can be used to predict the price of a house based on its size."),(0,a.kt)("li",{parentName:"ul"},"Linear regression is a type of regression model.")),(0,a.kt)("h3",{id:"regression-vs-classification"},"Regression vs. Classification"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Regression models predict numerical outputs, while classification models predict discrete categories."),(0,a.kt)("li",{parentName:"ul"},"Linear regression is an example of a regression model.")),(0,a.kt)("h3",{id:"training-set"},"Training Set"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The dataset used to train the model is called a training set."),(0,a.kt)("li",{parentName:"ul"},"The input variable in machine learning notation is denoted as lowercase x."),(0,a.kt)("li",{parentName:"ul"},"The output variable is denoted as lowercase y."),(0,a.kt)("li",{parentName:"ul"},"The training set contains multiple training examples, with each row corresponding to a different example."),(0,a.kt)("li",{parentName:"ul"},"The total number of examples is denoted as m."),(0,a.kt)("li",{parentName:"ul"},"A specific example is denoted as (x^i, y^i), where i is an index into the training set.")),(0,a.kt)("h3",{id:"notation-for-describing-data"},"Notation for Describing Data"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Standard notation to denote the input is lowercase x."),(0,a.kt)("li",{parentName:"ul"},"The output variable is denoted as lowercase y."),(0,a.kt)("li",{parentName:"ul"},"To refer to a specific training example, use the notation x^(i), y^(i)."),(0,a.kt)("li",{parentName:"ul"},"The superscript i refers to a specific row in the dataset.")),(0,a.kt)("h2",{id:"linear-regression-lesson2"},"Linear Regression lesson2"),(0,a.kt)("h3",{id:"introduction-1"},"Introduction"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Supervised learning algorithm inputs a dataset."),(0,a.kt)("li",{parentName:"ul"},"Training set in supervised learning includes input features and output targets."),(0,a.kt)("li",{parentName:"ul"},"The job of the function f is to take a new input x and output an estimate or prediction y-hat."),(0,a.kt)("li",{parentName:"ul"},"The prediction y-hat is the estimated value of y."),(0,a.kt)("li",{parentName:"ul"},"The model's prediction is the estimated value of y."),(0,a.kt)("li",{parentName:"ul"},"The function f is called the model, x is the input or input feature, and y-hat is the output or prediction.")),(0,a.kt)("h3",{id:"linear-regression"},"Linear Regression"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Linear regression is the process of fitting a straight line to a set of data points."),(0,a.kt)("li",{parentName:"ul"},"The function f is a straight line represented as ",(0,a.kt)("inlineCode",{parentName:"li"},"f(x) = wx + b"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"w")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"b")," are numbers chosen to determine the prediction ",(0,a.kt)("inlineCode",{parentName:"li"},"y-hat")," based on the input feature ",(0,a.kt)("inlineCode",{parentName:"li"},"x"),"."),(0,a.kt)("li",{parentName:"ul"},"Linear regression is a simple and easy-to-work-with foundation for more complex models."),(0,a.kt)("li",{parentName:"ul"},"Linear regression with one variable is called univariate linear regression.")),(0,a.kt)("h3",{id:"cost-function"},"Cost Function"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Constructing a cost function is one of the most important things in making linear regression work."),(0,a.kt)("li",{parentName:"ul"},"The cost function measures the difference between the predicted value and the actual value of the output target."),(0,a.kt)("li",{parentName:"ul"},"The goal is to minimize the cost function to get the best-fit line for the data.")),(0,a.kt)("h3",{id:"conclusion"},"Conclusion"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Supervised learning algorithms input a dataset and output a function f that makes predictions based on the input features."),(0,a.kt)("li",{parentName:"ul"},"Linear regression is a simple and easy-to-work-with foundation for more complex models."),(0,a.kt)("li",{parentName:"ul"},"Constructing a cost function is important to minimize the difference between predicted and actual output values.")),(0,a.kt)("h2",{id:"cost-function-formula"},"Cost function formula"),(0,a.kt)("h3",{id:"introduction-2"},"Introduction"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Linear regression is the process of fitting a straight line to a set of data points."),(0,a.kt)("li",{parentName:"ul"},"The function f is a straight line represented as ",(0,a.kt)("inlineCode",{parentName:"li"},"f(x) = wx + b"),"."),(0,a.kt)("li",{parentName:"ul"},(0,a.kt)("inlineCode",{parentName:"li"},"w")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"b")," are called the parameters or coefficients of the model."),(0,a.kt)("li",{parentName:"ul"},"The goal of linear regression is to choose values for ",(0,a.kt)("inlineCode",{parentName:"li"},"w")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"b")," so that the line defined by ",(0,a.kt)("inlineCode",{parentName:"li"},"f")," fits the data well.")),(0,a.kt)("h3",{id:"cost-function-1"},"Cost Function"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The cost function measures how well a line fits the training data."),(0,a.kt)("li",{parentName:"ul"},"The squared error cost function is widely used in machine learning for linear regression."),(0,a.kt)("li",{parentName:"ul"},"The cost function is defined as ",(0,a.kt)("inlineCode",{parentName:"li"},"J(w, b) = 1/(2m) * sum((f(x^i) - y^i)^2)")," where ",(0,a.kt)("inlineCode",{parentName:"li"},"m")," is the number of training examples."),(0,a.kt)("li",{parentName:"ul"},"The cost function compares the predicted value ",(0,a.kt)("inlineCode",{parentName:"li"},"f(x^i)")," to the true target value ",(0,a.kt)("inlineCode",{parentName:"li"},"y^i"),"."),(0,a.kt)("li",{parentName:"ul"},"The cost function is used to find values of ",(0,a.kt)("inlineCode",{parentName:"li"},"w")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"b")," that make the cost function small."),(0,a.kt)("li",{parentName:"ul"},"By convention, the cost function is divided by ",(0,a.kt)("inlineCode",{parentName:"li"},"2m")," to make calculations look neater.")),(0,a.kt)("h3",{id:"conclusion-1"},"Conclusion"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Linear regression involves fitting a straight line to a set of data points."),(0,a.kt)("li",{parentName:"ul"},"The cost function measures how well a line fits the training data."),(0,a.kt)("li",{parentName:"ul"},"The squared error cost function is widely used in machine learning for linear regression."),(0,a.kt)("li",{parentName:"ul"},"The cost function is used to find values of ",(0,a.kt)("inlineCode",{parentName:"li"},"w")," and ",(0,a.kt)("inlineCode",{parentName:"li"},"b")," that make the cost function small.")),(0,a.kt)("h2",{id:"cost-function-intuition"},"Cost Function Intuition"),(0,a.kt)("h3",{id:"the-cost-function-j-measures-how-well-a-choice-of-w-and-b-fit-the-training-data"},"The cost function J measures how well a choice of w and b fit the training data."),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The goal is to find values for w and b that minimize J, making the squared errors as small as possible.")),(0,a.kt)("h3",{id:"jw-b--12m--\u03c3i1-to-m-fwxi---yi2"},"J(w, b) = 1/2m ","*"," \u03a3(i=1 to m) (fw(x(i)) - y(i))^2"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Foreach training example (x(i), y(i)), the cost function calculates the squared difference between the predicted value fw(x(i)) and the actual value y(i)."),(0,a.kt)("li",{parentName:"ul"},"The cost function then takes the average of these squared differences across all training examples."),(0,a.kt)("li",{parentName:"ul"},"The factor of 1/2m is included to make calculations simpler.")),(0,a.kt)("h3",{id:"minimizing-the-cost-function"},"Minimizing the Cost Function"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The goal is to find values for w and b that minimize the cost function J."),(0,a.kt)("li",{parentName:"ul"},"This is done using optimization algorithms such as gradient descent."),(0,a.kt)("li",{parentName:"ul"},"Gradient descent iteratively adjusts the values of w and b to find the minimum of the cost function."),(0,a.kt)("li",{parentName:"ul"},"By updating the values of w and b in the direction of steepest descent, the algorithm converges to the minimum.")),(0,a.kt)("h3",{id:"visualizing-the-cost-function"},"Visualizing the Cost Function"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The cost function J(w, b) can be visualized as a bowl-shaped surface in three-dimensional space."),(0,a.kt)("li",{parentName:"ul"},"The x-axis represents the value of w, the y-axis represents the value of b, and the z-axis represents the value of J."),(0,a.kt)("li",{parentName:"ul"},"The goal is to find the values of w and b that correspond to the lowest point on the surface, indicating the minimum cost.")),(0,a.kt)("h3",{id:"conclusion-2"},"Conclusion"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The cost function measures how well a choice of w and b fit the training data."),(0,a.kt)("li",{parentName:"ul"},"The goal is to find values for w and b that minimize the cost function."),(0,a.kt)("li",{parentName:"ul"},"Optimization algorithms such as gradient descent are used to iteratively update the values of w and b to converge to the minimum of the cost function.")))}c.isMDXComponent=!0}}]);