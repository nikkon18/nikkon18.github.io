"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[4748],{3905:(e,i,t)=>{t.d(i,{Zo:()=>d,kt:()=>c});var a=t(7294);function n(e,i,t){return i in e?Object.defineProperty(e,i,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[i]=t,e}function r(e,i){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);i&&(a=a.filter((function(i){return Object.getOwnPropertyDescriptor(e,i).enumerable}))),t.push.apply(t,a)}return t}function l(e){for(var i=1;i<arguments.length;i++){var t=null!=arguments[i]?arguments[i]:{};i%2?r(Object(t),!0).forEach((function(i){n(e,i,t[i])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):r(Object(t)).forEach((function(i){Object.defineProperty(e,i,Object.getOwnPropertyDescriptor(t,i))}))}return e}function o(e,i){if(null==e)return{};var t,a,n=function(e,i){if(null==e)return{};var t,a,n={},r=Object.keys(e);for(a=0;a<r.length;a++)t=r[a],i.indexOf(t)>=0||(n[t]=e[t]);return n}(e,i);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)t=r[a],i.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(n[t]=e[t])}return n}var s=a.createContext({}),u=function(e){var i=a.useContext(s),t=i;return e&&(t="function"==typeof e?e(i):l(l({},i),e)),t},d=function(e){var i=u(e.components);return a.createElement(s.Provider,{value:i},e.children)},m={inlineCode:"code",wrapper:function(e){var i=e.children;return a.createElement(a.Fragment,{},i)}},g=a.forwardRef((function(e,i){var t=e.components,n=e.mdxType,r=e.originalType,s=e.parentName,d=o(e,["components","mdxType","originalType","parentName"]),g=u(t),c=n,h=g["".concat(s,".").concat(c)]||g[c]||m[c]||r;return t?a.createElement(h,l(l({ref:i},d),{},{components:t})):a.createElement(h,l({ref:i},d))}));function c(e,i){var t=arguments,n=i&&i.mdxType;if("string"==typeof e||n){var r=t.length,l=new Array(r);l[0]=g;var o={};for(var s in i)hasOwnProperty.call(i,s)&&(o[s]=i[s]);o.originalType=e,o.mdxType="string"==typeof e?e:n,l[1]=o;for(var u=2;u<r;u++)l[u]=t[u];return a.createElement.apply(null,l)}return a.createElement.apply(null,t)}g.displayName="MDXCreateElement"},3363:(e,i,t)=>{t.r(i),t.d(i,{frontMatter:()=>r,contentTitle:()=>l,metadata:()=>o,toc:()=>s,default:()=>d});var a=t(7462),n=(t(7294),t(3905));const r={},l="7 - The problem of overditting",o={unversionedId:"Supervised Machine Learning/The problem of overditting",id:"Supervised Machine Learning/The problem of overditting",title:"7 - The problem of overditting",description:"Understanding Overfitting and Underfitting",source:"@site/docs/Supervised Machine Learning/7-The problem of overditting.md",sourceDirName:"Supervised Machine Learning",slug:"/Supervised Machine Learning/The problem of overditting",permalink:"/docs/Supervised Machine Learning/The problem of overditting",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/7-The problem of overditting.md",tags:[],version:"current",sidebarPosition:7,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"6 - classification with logistic regression",permalink:"/docs/Supervised Machine Learning/classification with logistic regression"},next:{title:"1 - Neural networks intuition",permalink:"/docs/Advanced Learning Algorithms/Neural networks intuition"}},s=[{value:"Understanding Overfitting and Underfitting",id:"understanding-overfitting-and-underfitting",children:[{value:"Introduction",id:"introduction",children:[],level:3},{value:"Overfitting",id:"overfitting",children:[],level:3},{value:"Underfitting",id:"underfitting",children:[],level:3},{value:"Linear Regression Example",id:"linear-regression-example",children:[],level:3},{value:"Quadratic Regression Example",id:"quadratic-regression-example",children:[],level:3},{value:"Overfitting with a Fourth-Order Polynomial",id:"overfitting-with-a-fourth-order-polynomial",children:[],level:3},{value:"Goldilocks Principle",id:"goldilocks-principle",children:[],level:3},{value:"Overfitting in Classification",id:"overfitting-in-classification",children:[],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2},{value:"Addressing Overfitting in Machine Learning",id:"addressing-overfitting-in-machine-learning",children:[{value:"Introduction",id:"introduction-1",children:[],level:3},{value:"Option 1: Collect More Training Data",id:"option-1-collect-more-training-data",children:[],level:3},{value:"Option 2: Use Fewer Features",id:"option-2-use-fewer-features",children:[],level:3},{value:"Option 3: Regularization",id:"option-3-regularization",children:[],level:3},{value:"Conclusion",id:"conclusion-1",children:[],level:3}],level:2},{value:"Regularization in Machine Learning",id:"regularization-in-machine-learning",children:[{value:"Introduction",id:"introduction-2",children:[],level:3},{value:"Example Motivation",id:"example-motivation",children:[],level:3},{value:"Regularization Principle",id:"regularization-principle",children:[],level:3},{value:"Regularization Implementation",id:"regularization-implementation",children:[],level:3},{value:"Effect of Lambda",id:"effect-of-lambda",children:[],level:3},{value:"Summary of Regularization",id:"summary-of-regularization",children:[],level:3},{value:"Conclusion",id:"conclusion-2",children:[],level:3}],level:2},{value:"Notes on Regularized Linear Regression with Gradient Descent",id:"notes-on-regularized-linear-regression-with-gradient-descent",children:[{value:"Introduction",id:"introduction-3",children:[],level:3},{value:"Gradient Descent for Regularized Linear Regression",id:"gradient-descent-for-regularized-linear-regression",children:[],level:3},{value:"Intuition Behind the Update Rule",id:"intuition-behind-the-update-rule",children:[],level:3},{value:"Derivatives for Regularized Linear Regression",id:"derivatives-for-regularized-linear-regression",children:[],level:3},{value:"Conclusion",id:"conclusion-3",children:[],level:3}],level:2},{value:"Regularized Logistic Regression",id:"regularized-logistic-regression",children:[{value:"Introduction",id:"introduction-4",children:[],level:3},{value:"The Cost Function for Regularized Logistic Regression",id:"the-cost-function-for-regularized-logistic-regression",children:[],level:3},{value:"Gradient Descent for Regularized Logistic Regression",id:"gradient-descent-for-regularized-logistic-regression",children:[],level:3},{value:"Implementation of Regularized Logistic Regression",id:"implementation-of-regularized-logistic-regression",children:[],level:3},{value:"Conclusion",id:"conclusion-4",children:[],level:3}],level:2}],u={toc:s};function d(e){let{components:i,...t}=e;return(0,n.kt)("wrapper",(0,a.Z)({},u,t,{components:i,mdxType:"MDXLayout"}),(0,n.kt)("h1",{id:"7---the-problem-of-overditting"},"7 - The problem of overditting"),(0,n.kt)("h2",{id:"understanding-overfitting-and-underfitting"},"Understanding Overfitting and Underfitting"),(0,n.kt)("h3",{id:"introduction"},"Introduction"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Overfitting and underfitting are common problems in machine learning algorithms."),(0,n.kt)("li",{parentName:"ul"},"Linear regression and logistic regression are effective algorithms but can encounter these issues."),(0,n.kt)("li",{parentName:"ul"},"Overfitting results in poor performance, while underfitting indicates high bias.")),(0,n.kt)("h3",{id:"overfitting"},"Overfitting"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Overfitting occurs when an algorithm excessively fits the training data."),(0,n.kt)("li",{parentName:"ul"},"It leads to poor generalization to new, unseen examples."),(0,n.kt)("li",{parentName:"ul"},"Overfitting is characterized by high variance in the model."),(0,n.kt)("li",{parentName:"ul"},"The algorithm tries too hard to fit every training example, resulting in a complex and wiggly curve."),(0,n.kt)("li",{parentName:"ul"},"This behavior is undesirable and hampers prediction accuracy.")),(0,n.kt)("h3",{id:"underfitting"},"Underfitting"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Underfitting happens when an algorithm fails to capture the underlying patterns in the training data."),(0,n.kt)("li",{parentName:"ul"},"The model is too simplistic to fit the training set well."),(0,n.kt)("li",{parentName:"ul"},"Underfitting is associated with high bias."),(0,n.kt)("li",{parentName:"ul"},"The algorithm has a strong preconception or bias that doesn't align with the data.")),(0,n.kt)("h3",{id:"linear-regression-example"},"Linear Regression Example"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Consider the task of predicting housing prices based on the size of the house."),(0,n.kt)("li",{parentName:"ul"},"A linear regression model can be fitted to the data."),(0,n.kt)("li",{parentName:"ul"},"If the linear model is too simple, it may underfit the data, resulting in poor predictions."),(0,n.kt)("li",{parentName:"ul"},"This model has low flexibility and doesn't capture the non-linear relationship between size and price.")),(0,n.kt)("h3",{id:"quadratic-regression-example"},"Quadratic Regression Example"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"A quadratic regression model with two features (size and size squared) can fit the data better."),(0,n.kt)("li",{parentName:"ul"},"This model captures the non-linear relationship and improves predictions."),(0,n.kt)("li",{parentName:"ul"},"It strikes a balance between fitting the training set and generalizing to new examples."),(0,n.kt)("li",{parentName:"ul"},"Generalization refers to the ability to make accurate predictions on unseen data.")),(0,n.kt)("h3",{id:"overfitting-with-a-fourth-order-polynomial"},"Overfitting with a Fourth-Order Polynomial"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Fitting a fourth-order polynomial to the data allows the model to pass through all training examples exactly."),(0,n.kt)("li",{parentName:"ul"},"Although it fits the training set perfectly, it results in a wiggly curve."),(0,n.kt)("li",{parentName:"ul"},"This model overfits the data and has high variance."),(0,n.kt)("li",{parentName:"ul"},"It is unlikely to generalize well to new examples and may produce unreliable predictions.")),(0,n.kt)("h3",{id:"goldilocks-principle"},"Goldilocks Principle"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The goal in machine learning is to find a model that neither underfits nor overfits the data."),(0,n.kt)("li",{parentName:"ul"},"Too few features lead to underfitting, while too many features cause overfitting."),(0,n.kt)("li",{parentName:"ul"},"Striking the right balance is crucial for optimal performance."),(0,n.kt)("li",{parentName:"ul"},'This principle is similar to Goldilocks and the Three Bears, where the ideal temperature is "just right."')),(0,n.kt)("h3",{id:"overfitting-in-classification"},"Overfitting in Classification"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Overfitting is applicable to classification tasks as well."),(0,n.kt)("li",{parentName:"ul"},"A logistic regression model can be used to classify tumors as malignant or benign."),(0,n.kt)("li",{parentName:"ul"},"A simple linear decision boundary may underfit the data and have high bias."),(0,n.kt)("li",{parentName:"ul"},"Adding quadratic features allows for a more flexible decision boundary that fits the data better."),(0,n.kt)("li",{parentName:"ul"},"This balanced model achieves reasonable predictions and generalization."),(0,n.kt)("li",{parentName:"ul"},"Fitting a high-order polynomial may result in an overfitting problem with high variance.")),(0,n.kt)("h3",{id:"conclusion"},"Conclusion"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Overfitting and underfitting are common challenges in machine learning."),(0,n.kt)("li",{parentName:"ul"},"Overfitting occurs when the model fits the training data too well, leading to poor generalization."),(0,n.kt)("li",{parentName:"ul"},"Underfitting happens when the model fails to capture the underlying patterns in the data."),(0,n.kt)("li",{parentName:"ul"},"Striking the right balance between the number of features and the complexity of the model is crucial."),(0,n.kt)("li",{parentName:"ul"},"The goal is to find a model that neither underfits nor overfits, achieving optimal performance and generalization.")),(0,n.kt)("h2",{id:"addressing-overfitting-in-machine-learning"},"Addressing Overfitting in Machine Learning"),(0,n.kt)("h3",{id:"introduction-1"},"Introduction"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Overfitting occurs when a model fits the training data too closely, resulting in poor generalization to new examples."),(0,n.kt)("li",{parentName:"ul"},"There are several approaches to address overfitting in machine learning algorithms.")),(0,n.kt)("h3",{id:"option-1-collect-more-training-data"},"Option 1: Collect More Training Data"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Increasing the size of the training set can help reduce overfitting."),(0,n.kt)("li",{parentName:"ul"},"With more training examples, the algorithm can learn to fit a smoother function."),(0,n.kt)("li",{parentName:"ul"},"The learning algorithm will have a better chance of generalizing well to unseen data."),(0,n.kt)("li",{parentName:"ul"},"Collecting more data is not always feasible but can be highly effective when available.")),(0,n.kt)("h3",{id:"option-2-use-fewer-features"},"Option 2: Use Fewer Features"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Overfitting can also occur when there are too many features relative to the amount of training data."),(0,n.kt)("li",{parentName:"ul"},"By reducing the number of features, the model becomes less complex and prone to overfitting."),(0,n.kt)("li",{parentName:"ul"},"Feature selection involves choosing the most relevant features for the prediction task."),(0,n.kt)("li",{parentName:"ul"},"Selecting a smaller subset of features can help mitigate overfitting.")),(0,n.kt)("h3",{id:"option-3-regularization"},"Option 3: Regularization"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularization is a technique to reduce overfitting by preventing the model parameters from becoming too large."),(0,n.kt)("li",{parentName:"ul"},"It encourages the learning algorithm to shrink the parameter values without eliminating features entirely."),(0,n.kt)("li",{parentName:"ul"},"By reducing the impact of some features, regularization helps prevent overfitting while retaining useful information."),(0,n.kt)("li",{parentName:"ul"},"Regularization is particularly useful in models with high-dimensional feature spaces.")),(0,n.kt)("h3",{id:"conclusion-1"},"Conclusion"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Overfitting is a common challenge in machine learning, but there are effective strategies to address it."),(0,n.kt)("li",{parentName:"ul"},"Collecting more training data, using fewer features, and applying regularization are key approaches."),(0,n.kt)("li",{parentName:"ul"},"Each approach has its advantages and considerations, and the choice depends on the specific problem at hand."),(0,n.kt)("li",{parentName:"ul"},"Understanding and mitigating overfitting is crucial for building models that generalize well and make accurate predictions.")),(0,n.kt)("h2",{id:"regularization-in-machine-learning"},"Regularization in Machine Learning"),(0,n.kt)("h3",{id:"introduction-2"},"Introduction"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularization helps reduce overfitting in machine learning models."),(0,n.kt)("li",{parentName:"ul"},"It involves penalizing large parameter values to promote simpler models."),(0,n.kt)("li",{parentName:"ul"},"Regularization is implemented by modifying the cost function of the learning algorithm.")),(0,n.kt)("h3",{id:"example-motivation"},"Example Motivation"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"A quadratic function provides a good fit to the data, while a high-order polynomial overfits."),(0,n.kt)("li",{parentName:"ul"},"Introducing a modified cost function with additional terms penalizes large parameter values."),(0,n.kt)("li",{parentName:"ul"},"By minimizing this function, the model is encouraged to have smaller parameter values.")),(0,n.kt)("h3",{id:"regularization-principle"},"Regularization Principle"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Smaller parameter values lead to simpler models and reduce overfitting."),(0,n.kt)("li",{parentName:"ul"},"Regularization penalizes all parameter values to achieve a smoother, less complex function."),(0,n.kt)("li",{parentName:"ul"},"The regularization term is added to the original cost function, striking a balance between fitting the training data and keeping parameter values small.")),(0,n.kt)("h3",{id:"regularization-implementation"},"Regularization Implementation"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularization is typically applied to all parameters, as it's challenging to determine the most important ones."),(0,n.kt)("li",{parentName:"ul"},"The regularization parameter, lambda, controls the strength of regularization."),(0,n.kt)("li",{parentName:"ul"},"The regularization term is calculated as lambda times the sum of squared parameter values."),(0,n.kt)("li",{parentName:"ul"},"To ease parameter selection, lambda is divided by 2m, scaling both terms equally.")),(0,n.kt)("h3",{id:"effect-of-lambda"},"Effect of Lambda"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Choosing lambda = 0 leads to overfitting, with a complex model that fits the training data too closely."),(0,n.kt)("li",{parentName:"ul"},"Setting lambda to a very large value (e.g., 10^10) heavily weights the regularization term."),(0,n.kt)("li",{parentName:"ul"},"Large lambda values result in underfitting, where the model becomes too simple."),(0,n.kt)("li",{parentName:"ul"},"The appropriate lambda value balances between fitting the data and keeping parameter values small.")),(0,n.kt)("h3",{id:"summary-of-regularization"},"Summary of Regularization"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularization adds a regularization term to the cost function to reduce overfitting."),(0,n.kt)("li",{parentName:"ul"},"It promotes simpler models by penalizing large parameter values."),(0,n.kt)("li",{parentName:"ul"},"The regularization parameter, lambda, controls the trade-off between fitting the data and regularization."),(0,n.kt)("li",{parentName:"ul"},"Proper selection of lambda leads to a well-balanced model that avoids overfitting.")),(0,n.kt)("h3",{id:"conclusion-2"},"Conclusion"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularization is a powerful technique to address overfitting in machine learning."),(0,n.kt)("li",{parentName:"ul"},"It helps create simpler models by penalizing large parameter values."),(0,n.kt)("li",{parentName:"ul"},"By finding the right balance between fitting the data and regularization, models can generalize better."),(0,n.kt)("li",{parentName:"ul"},"Regularization can be applied to various machine learning algorithms like linear regression and logistic regression.")),(0,n.kt)("h2",{id:"notes-on-regularized-linear-regression-with-gradient-descent"},"Notes on Regularized Linear Regression with Gradient Descent"),(0,n.kt)("h3",{id:"introduction-3"},"Introduction"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularized linear regression helps reduce overfitting in the model."),(0,n.kt)("li",{parentName:"ul"},"The cost function for regularized linear regression consists of the squared error cost function and an additional regularization term."),(0,n.kt)("li",{parentName:"ul"},"The goal is to find parameters w and b that minimize the regularized cost function.")),(0,n.kt)("h3",{id:"gradient-descent-for-regularized-linear-regression"},"Gradient Descent for Regularized Linear Regression"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The gradient descent algorithm is used to update the parameters w and b iteratively."),(0,n.kt)("li",{parentName:"ul"},"The update rule for w_j, where j is from 1 to n, is given by:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"w_j := w_j(1 - alpha(lambda/m)) - (alpha/m)(sum((h_theta(x_i) - y_i)x_ij) + lambda","*","w_j)"))),(0,n.kt)("li",{parentName:"ul"},"The update rule for b remains the same as before."),(0,n.kt)("li",{parentName:"ul"},"Alpha is the learning rate, lambda is the regularization parameter, and m is the training set size.")),(0,n.kt)("h3",{id:"intuition-behind-the-update-rule"},"Intuition Behind the Update Rule"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The update rule for w_j can be rewritten as:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"w_j := w_j(1 - alpha(lambda/m)) - (alpha/m)(sum((h_theta(x_i) - y_i)x_ij)"))),(0,n.kt)("li",{parentName:"ul"},"The first term in parentheses, 1 - alpha(lambda/m), is slightly less than 1 and causes the value of w_j to shrink on each iteration."),(0,n.kt)("li",{parentName:"ul"},"The second term is the usual update for unregularized linear regression."),(0,n.kt)("li",{parentName:"ul"},"Regularization shrinks the value of w_j, promoting a simpler model.")),(0,n.kt)("h3",{id:"derivatives-for-regularized-linear-regression"},"Derivatives for Regularized Linear Regression"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The derivative of the cost function with respect to w_j is given by:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"1/m ","*"," sum((h_theta(x_i) - y_i)x_ij) + (lambda/m)","*","w_j"))),(0,n.kt)("li",{parentName:"ul"},"The derivative of the cost function with respect to b remains the same as before."),(0,n.kt)("li",{parentName:"ul"},"The regularization term affects the derivative of w_j, adding an additional term.")),(0,n.kt)("h3",{id:"conclusion-3"},"Conclusion"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularized linear regression uses gradient descent to update the parameters."),(0,n.kt)("li",{parentName:"ul"},"The update rule includes a regularization term that shrinks the parameter values on each iteration."),(0,n.kt)("li",{parentName:"ul"},"Derivatives of the cost function are modified to include the regularization term."),(0,n.kt)("li",{parentName:"ul"},"Regularization helps reduce overfitting and improves model performance, especially with many features and a small training set.")),(0,n.kt)("h2",{id:"regularized-logistic-regression"},"Regularized Logistic Regression"),(0,n.kt)("h3",{id:"introduction-4"},"Introduction"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularized logistic regression helps prevent overfitting in the model."),(0,n.kt)("li",{parentName:"ul"},"When training logistic regression with many features, there is a higher risk of overfitting.")),(0,n.kt)("h3",{id:"the-cost-function-for-regularized-logistic-regression"},"The Cost Function for Regularized Logistic Regression"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The cost function for logistic regression includes a regularization term."),(0,n.kt)("li",{parentName:"ul"},"The regularization term penalizes the parameters w_1, w_2, ..., w_n to prevent them from being too large."),(0,n.kt)("li",{parentName:"ul"},"The cost function for regularized logistic regression is given by:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"J(w, b) = (1/m) ",(0,n.kt)("em",{parentName:"li"}," sum((-y_i ")," log(f(x",(0,n.kt)("em",{parentName:"li"},"i))) - ((1 - y_i) ")," log(1 - f(x",(0,n.kt)("em",{parentName:"li"},"i)))) + (lambda/(2m)) ")," sum(w_j^2)"),(0,n.kt)("li",{parentName:"ul"},"f(x_i) is the logistic function applied to z.")))),(0,n.kt)("h3",{id:"gradient-descent-for-regularized-logistic-regression"},"Gradient Descent for Regularized Logistic Regression"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"The update rules for gradient descent in regularized logistic regression are similar to regularized linear regression."),(0,n.kt)("li",{parentName:"ul"},"The update rule for w_j is given by:",(0,n.kt)("ul",{parentName:"li"},(0,n.kt)("li",{parentName:"ul"},"w_j := w_j(1 - alpha(lambda/m)) - (alpha/m)(sum((f(x_i) - y_i) ","*"," x_ij)) + (lambda/m)","*","w_j"))),(0,n.kt)("li",{parentName:"ul"},"The update rule for b remains the same as before.")),(0,n.kt)("h3",{id:"implementation-of-regularized-logistic-regression"},"Implementation of Regularized Logistic Regression"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Gradient descent is used to minimize the cost function."),(0,n.kt)("li",{parentName:"ul"},"The update rules for w_j and b are applied iteratively."),(0,n.kt)("li",{parentName:"ul"},"The regularization term is added to the derivative of the cost function with respect to w_j.")),(0,n.kt)("h3",{id:"conclusion-4"},"Conclusion"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"Regularized logistic regression helps prevent overfitting in models with many features."),(0,n.kt)("li",{parentName:"ul"},"The cost function includes a regularization term to penalize large parameter values."),(0,n.kt)("li",{parentName:"ul"},"Gradient descent is used to minimize the cost function and update the parameters."),(0,n.kt)("li",{parentName:"ul"},"Regularization is an important technique in machine learning to improve model performance and prevent overfitting."),(0,n.kt)("li",{parentName:"ul"},"Understanding linear regression, logistic regression, and regularization are valuable skills in the real-world applications of machine learning.")))}d.isMDXComponent=!0}}]);