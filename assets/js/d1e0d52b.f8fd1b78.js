"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[4579],{3905:(e,t,n)=>{n.d(t,{Zo:()=>s,kt:()=>p});var i=n(7294);function a(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function o(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function r(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?o(Object(n),!0).forEach((function(t){a(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):o(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,a=function(e,t){if(null==e)return{};var n,i,a={},o=Object.keys(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||(a[n]=e[n]);return a}(e,t);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(i=0;i<o.length;i++)n=o[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(a[n]=e[n])}return a}var c=i.createContext({}),u=function(e){var t=i.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):r(r({},t),e)),n},s=function(e){var t=u(e.components);return i.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},m=i.forwardRef((function(e,t){var n=e.components,a=e.mdxType,o=e.originalType,c=e.parentName,s=l(e,["components","mdxType","originalType","parentName"]),m=u(n),p=a,v=m["".concat(c,".").concat(p)]||m[p]||d[p]||o;return n?i.createElement(v,r(r({ref:t},s),{},{components:n})):i.createElement(v,r({ref:t},s))}));function p(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var o=n.length,r=new Array(o);r[0]=m;var l={};for(var c in t)hasOwnProperty.call(t,c)&&(l[c]=t[c]);l.originalType=e,l.mdxType="string"==typeof e?e:a,r[1]=l;for(var u=2;u<o;u++)r[u]=n[u];return i.createElement.apply(null,r)}return i.createElement.apply(null,n)}m.displayName="MDXCreateElement"},9135:(e,t,n)=>{n.r(t),n.d(t,{frontMatter:()=>o,contentTitle:()=>r,metadata:()=>l,toc:()=>c,default:()=>s});var i=n(7462),a=(n(7294),n(3905));const o={},r="7-Activation Functions",l={unversionedId:"Advanced Learning Algorithms/Activation Functions",id:"Advanced Learning Algorithms/Activation Functions",title:"7-Activation Functions",description:"Alternatives to the sigmoid activation",source:"@site/docs/Advanced Learning Algorithms/14-Activation Functions.md",sourceDirName:"Advanced Learning Algorithms",slug:"/Advanced Learning Algorithms/Activation Functions",permalink:"/docs/Advanced Learning Algorithms/Activation Functions",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Advanced Learning Algorithms/14-Activation Functions.md",tags:[],version:"current",sidebarPosition:14,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"6-Neural Network Training",permalink:"/docs/Advanced Learning Algorithms/Training a Neural Network in TensorFlow"},next:{title:"8-Multiclass Classification",permalink:"/docs/Advanced Learning Algorithms/Multiclass Classification"}},c=[{value:"Alternatives to the sigmoid activation",id:"alternatives-to-the-sigmoid-activation",children:[{value:"Introduction",id:"introduction",children:[],level:3},{value:"Modeling Awareness",id:"modeling-awareness",children:[],level:3},{value:"ReLU Activation Function",id:"relu-activation-function",children:[],level:3},{value:"Other Activation Functions",id:"other-activation-functions",children:[],level:3},{value:"Choosing Activation Functions",id:"choosing-activation-functions",children:[],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2},{value:"Choosing Activation Functions in Neural Networks",id:"choosing-activation-functions-in-neural-networks",children:[{value:"Introduction",id:"introduction-1",children:[],level:3},{value:"Activation Functions for the Output Layer",id:"activation-functions-for-the-output-layer",children:[],level:3},{value:"Activation Functions for Hidden Layers",id:"activation-functions-for-hidden-layers",children:[],level:3},{value:"Recommendations for Choosing Activation Functions",id:"recommendations-for-choosing-activation-functions",children:[],level:3},{value:"Other Activation Functions",id:"other-activation-functions-1",children:[],level:3},{value:"Importance of Activation Functions",id:"importance-of-activation-functions",children:[],level:3},{value:"Conclusion",id:"conclusion-1",children:[],level:3}],level:2},{value:"Why do we need a activation functions?",id:"why-do-we-need-a-activation-functions",children:[{value:"Introduction",id:"introduction-2",children:[],level:3},{value:"Example of a Neural Network",id:"example-of-a-neural-network",children:[],level:3},{value:"Computation in the Neural Network",id:"computation-in-the-neural-network",children:[],level:3},{value:"Simplification and Linear Function",id:"simplification-and-linear-function",children:[],level:3},{value:"Linear Activation and Linear Regression",id:"linear-activation-and-linear-regression",children:[],level:3},{value:"Equivalent Models",id:"equivalent-models",children:[],level:3},{value:"Importance of Activation Functions",id:"importance-of-activation-functions-1",children:[],level:3},{value:"Conclusion",id:"conclusion-2",children:[],level:3}],level:2}],u={toc:c};function s(e){let{components:t,...n}=e;return(0,a.kt)("wrapper",(0,i.Z)({},u,n,{components:t,mdxType:"MDXLayout"}),(0,a.kt)("h1",{id:"7-activation-functions"},"7-Activation Functions"),(0,a.kt)("h2",{id:"alternatives-to-the-sigmoid-activation"},"Alternatives to the sigmoid activation"),(0,a.kt)("h3",{id:"introduction"},"Introduction"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Activation functions determine the output of a neuron and introduce non-linearity to neural networks."),(0,a.kt)("li",{parentName:"ul"},"Different activation functions can make neural networks more powerful."),(0,a.kt)("li",{parentName:"ul"},"Let's explore alternative activation functions and their implications.")),(0,a.kt)("h3",{id:"modeling-awareness"},"Modeling Awareness"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Example: Demand prediction for a t-shirt based on price, shipping cost, marketing, and material."),(0,a.kt)("li",{parentName:"ul"},"Previous approach: Model awareness as a binary number (0 or 1)."),(0,a.kt)("li",{parentName:"ul"},"Consideration: Awareness can vary on a continuous scale."),(0,a.kt)("li",{parentName:"ul"},"New approach: Allow awareness to be any non-negative number.")),(0,a.kt)("h3",{id:"relu-activation-function"},"ReLU Activation Function"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"ReLU (Rectified Linear Unit) is a commonly used activation function."),(0,a.kt)("li",{parentName:"ul"},"Mathematical equation: g(z) = max(0, z)"),(0,a.kt)("li",{parentName:"ul"},"Graphical representation: 0 to the left, straight line to the right.")),(0,a.kt)("h3",{id:"other-activation-functions"},"Other Activation Functions"),(0,a.kt)("ol",null,(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"Sigmoid Activation Function:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Mathematical equation: g(z) = 1 / (1 + e^(-z))"),(0,a.kt)("li",{parentName:"ul"},"Range: 0 to 1"))),(0,a.kt)("li",{parentName:"ol"},(0,a.kt)("p",{parentName:"li"},"Linear Activation Function:"),(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Mathematical equation: g(z) = z"),(0,a.kt)("li",{parentName:"ul"},"Range: -\u221e to +\u221e")))),(0,a.kt)("h3",{id:"choosing-activation-functions"},"Choosing Activation Functions"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Sigmoid: Suitable for binary classification problems."),(0,a.kt)("li",{parentName:"ul"},"ReLU: Commonly used for hidden layers in deep neural networks."),(0,a.kt)("li",{parentName:"ul"},"Linear: Used for regression problems or when no activation is desired.")),(0,a.kt)("h3",{id:"conclusion"},"Conclusion"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Activation functions introduce non-linearity to neural networks."),(0,a.kt)("li",{parentName:"ul"},"Sigmoid, ReLU, and linear are commonly used activation functions."),(0,a.kt)("li",{parentName:"ul"},"The choice depends on the problem and the layer in the neural network."),(0,a.kt)("li",{parentName:"ul"},"These activation functions enable the construction of powerful neural networks.")),(0,a.kt)("h2",{id:"choosing-activation-functions-in-neural-networks"},"Choosing Activation Functions in Neural Networks"),(0,a.kt)("h3",{id:"introduction-1"},"Introduction"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Activation functions are crucial in neural networks for determining the output of neurons."),(0,a.kt)("li",{parentName:"ul"},"Different activation functions are selected based on the layer and the type of problem being solved.")),(0,a.kt)("h3",{id:"activation-functions-for-the-output-layer"},"Activation Functions for the Output Layer"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The choice of activation function for the output layer depends on the target label or ground truth label, y."),(0,a.kt)("li",{parentName:"ul"},"For binary classification problems (y = 0 or 1), the sigmoid activation function is the most natural choice.",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Sigmoid predicts the probability that y equals one."))),(0,a.kt)("li",{parentName:"ul"},"For regression problems (continuous values), the linear activation function is recommended.",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Example: Predicting stock price changes."))),(0,a.kt)("li",{parentName:"ul"},"For non-negative values (e.g., predicting house prices), the ReLU activation function is the most suitable choice.")),(0,a.kt)("h3",{id:"activation-functions-for-hidden-layers"},"Activation Functions for Hidden Layers"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"The ReLU activation function is the most common choice for hidden layers in modern neural networks."),(0,a.kt)("li",{parentName:"ul"},"Historically, sigmoid activation functions were widely used, but ReLU has become more prevalent."),(0,a.kt)("li",{parentName:"ul"},"ReLU is preferred due to its computational efficiency and the fact that it avoids flat regions, which can slow down learning.")),(0,a.kt)("h3",{id:"recommendations-for-choosing-activation-functions"},"Recommendations for Choosing Activation Functions"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Output Layer:",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Binary classification: Use sigmoid activation function."),(0,a.kt)("li",{parentName:"ul"},"Regression: Use linear activation function."),(0,a.kt)("li",{parentName:"ul"},"Non-negative values: Use ReLU activation function."))),(0,a.kt)("li",{parentName:"ul"},"Hidden Layers: Use ReLU as the default activation function.",(0,a.kt)("ul",{parentName:"li"},(0,a.kt)("li",{parentName:"ul"},"Implement in TensorFlow using the appropriate syntax.")))),(0,a.kt)("h3",{id:"other-activation-functions-1"},"Other Activation Functions"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Researchers have proposed various activation functions beyond sigmoid and ReLU."),(0,a.kt)("li",{parentName:"ul"},"These include tanh, LeakyReLU, and swish, which may offer slight performance improvements in specific cases."),(0,a.kt)("li",{parentName:"ul"},"However, for most applications, sigmoid and ReLU are sufficient and widely used.")),(0,a.kt)("h3",{id:"importance-of-activation-functions"},"Importance of Activation Functions"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Activation functions are necessary components of neural networks."),(0,a.kt)("li",{parentName:"ul"},"Linear activation functions or the absence of activation functions do not work effectively."),(0,a.kt)("li",{parentName:"ul"},"Activation functions enable neural networks to learn complex patterns and make non-linear transformations.")),(0,a.kt)("h3",{id:"conclusion-1"},"Conclusion"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Choosing the right activation functions is crucial for neural network performance."),(0,a.kt)("li",{parentName:"ul"},"Sigmoid is recommended for binary classification, linear for regression, and ReLU for hidden layers."),(0,a.kt)("li",{parentName:"ul"},"ReLU has become the most common choice due to its efficiency and avoidance of flat regions."),(0,a.kt)("li",{parentName:"ul"},"Other activation functions exist but are typically not necessary for most applications."),(0,a.kt)("li",{parentName:"ul"},"Activation functions are essential for enabling neural networks to learn and make non-linear transformations.")),(0,a.kt)("h2",{id:"why-do-we-need-a-activation-functions"},"Why do we need a activation functions?"),(0,a.kt)("h3",{id:"introduction-2"},"Introduction"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Neural networks require activation functions for their proper functioning."),(0,a.kt)("li",{parentName:"ul"},"Using a linear activation function in every neuron of a neural network leads to issues."),(0,a.kt)("li",{parentName:"ul"},"Linear activation in all nodes results in a neural network equivalent to linear regression."),(0,a.kt)("li",{parentName:"ul"},"This defeats the purpose of using a neural network for complex modeling.")),(0,a.kt)("h3",{id:"example-of-a-neural-network"},"Example of a Neural Network"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Consider a neural network with one input node (x), one hidden unit (a1), and one output unit (a2)."),(0,a.kt)("li",{parentName:"ul"},"Parameters: w1, b1, w2, b2."),(0,a.kt)("li",{parentName:"ul"},"Activation function: g(z) = z.")),(0,a.kt)("h3",{id:"computation-in-the-neural-network"},"Computation in the Neural Network"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"a1 = w1 ","*"," x + b1 (using the activation function g)"),(0,a.kt)("li",{parentName:"ul"},"a2 = w2 ","*"," a1 + b2 (using the activation function g)")),(0,a.kt)("h3",{id:"simplification-and-linear-function"},"Simplification and Linear Function"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"By substituting the expression for a1, a2 becomes w2 ",(0,a.kt)("em",{parentName:"li"}," (w1 ")," x + b1) + b2."),(0,a.kt)("li",{parentName:"ul"},"Simplifying further, a2 = w ",(0,a.kt)("em",{parentName:"li"}," x + b (where w = w2 ")," w1 and b = w2 ","*"," b1 + b2)."),(0,a.kt)("li",{parentName:"ul"},"Thus, a2 is just a linear function of the input x.")),(0,a.kt)("h3",{id:"linear-activation-and-linear-regression"},"Linear Activation and Linear Regression"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Using a neural network with one hidden layer and one output layer with linear activation is equivalent to linear regression."),(0,a.kt)("li",{parentName:"ul"},"Multiple layers in a neural network cannot compute more complex features than a linear function."),(0,a.kt)("li",{parentName:"ul"},"Linear algebra supports this observation as a linear function of a linear function is still linear.")),(0,a.kt)("h3",{id:"equivalent-models"},"Equivalent Models"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Using linear activation in all hidden layers and the output layer yields an output equivalent to linear regression."),(0,a.kt)("li",{parentName:"ul"},"Alternatively, using linear activation in hidden layers and a logistic activation in the output layer makes the model equivalent to logistic regression."),(0,a.kt)("li",{parentName:"ul"},"The output a4 can be expressed as 1 / (1 + e^(-wx + b)) for certain values of w and b.")),(0,a.kt)("h3",{id:"importance-of-activation-functions-1"},"Importance of Activation Functions"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Neural networks require activation functions other than linear activation."),(0,a.kt)("li",{parentName:"ul"},"Using linear activation in hidden layers restricts the network's ability to learn complex features."),(0,a.kt)("li",{parentName:"ul"},"Common practice is to use ReLU activation for hidden layers.")),(0,a.kt)("h3",{id:"conclusion-2"},"Conclusion"),(0,a.kt)("ul",null,(0,a.kt)("li",{parentName:"ul"},"Neural networks need activation functions for effective function approximation."),(0,a.kt)("li",{parentName:"ul"},"Linear activation functions limit the network's potential compared to more sophisticated models."),(0,a.kt)("li",{parentName:"ul"},"Neural networks are versatile and can handle binary classification and regression problems.")))}s.isMDXComponent=!0}}]);