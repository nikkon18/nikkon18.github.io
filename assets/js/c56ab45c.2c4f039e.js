"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[6950],{3905:(e,i,n)=>{n.d(i,{Zo:()=>u,kt:()=>p});var t=n(7294);function o(e,i,n){return i in e?Object.defineProperty(e,i,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[i]=n,e}function a(e,i){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(e);i&&(t=t.filter((function(i){return Object.getOwnPropertyDescriptor(e,i).enumerable}))),n.push.apply(n,t)}return n}function r(e){for(var i=1;i<arguments.length;i++){var n=null!=arguments[i]?arguments[i]:{};i%2?a(Object(n),!0).forEach((function(i){o(e,i,n[i])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(i){Object.defineProperty(e,i,Object.getOwnPropertyDescriptor(n,i))}))}return e}function s(e,i){if(null==e)return{};var n,t,o=function(e,i){if(null==e)return{};var n,t,o={},a=Object.keys(e);for(t=0;t<a.length;t++)n=a[t],i.indexOf(n)>=0||(o[n]=e[n]);return o}(e,i);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(t=0;t<a.length;t++)n=a[t],i.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var l=t.createContext({}),c=function(e){var i=t.useContext(l),n=i;return e&&(n="function"==typeof e?e(i):r(r({},i),e)),n},u=function(e){var i=c(e.components);return t.createElement(l.Provider,{value:i},e.children)},d={inlineCode:"code",wrapper:function(e){var i=e.children;return t.createElement(t.Fragment,{},i)}},g=t.forwardRef((function(e,i){var n=e.components,o=e.mdxType,a=e.originalType,l=e.parentName,u=s(e,["components","mdxType","originalType","parentName"]),g=c(n),p=o,m=g["".concat(l,".").concat(p)]||g[p]||d[p]||a;return n?t.createElement(m,r(r({ref:i},u),{},{components:n})):t.createElement(m,r({ref:i},u))}));function p(e,i){var n=arguments,o=i&&i.mdxType;if("string"==typeof e||o){var a=n.length,r=new Array(a);r[0]=g;var s={};for(var l in i)hasOwnProperty.call(i,l)&&(s[l]=i[l]);s.originalType=e,s.mdxType="string"==typeof e?e:o,r[1]=s;for(var c=2;c<a;c++)r[c]=n[c];return t.createElement.apply(null,r)}return t.createElement.apply(null,n)}g.displayName="MDXCreateElement"},1437:(e,i,n)=>{n.r(i),n.d(i,{frontMatter:()=>a,contentTitle:()=>r,metadata:()=>s,toc:()=>l,default:()=>u});var t=n(7462),o=(n(7294),n(3905));const a={},r="6 - classification with logistic regression",s={unversionedId:"Supervised Machine Learning/classification with logistic regression",id:"Supervised Machine Learning/classification with logistic regression",title:"6 - classification with logistic regression",description:"Logistic Regression",source:"@site/docs/Supervised Machine Learning/6-classification with logistic regression.md",sourceDirName:"Supervised Machine Learning",slug:"/Supervised Machine Learning/classification with logistic regression",permalink:"/docs/Supervised Machine Learning/classification with logistic regression",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Supervised Machine Learning/6-classification with logistic regression.md",tags:[],version:"current",sidebarPosition:6,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"5 - Gradient descent in practice",permalink:"/docs/Supervised Machine Learning/Gradient descent in practice"},next:{title:"7 - The problem of overditting",permalink:"/docs/Supervised Machine Learning/The problem of overditting"}},l=[{value:"Logistic Regression",id:"logistic-regression",children:[{value:"Classification Problems",id:"classification-problems",children:[],level:3},{value:"Binary Classification",id:"binary-classification",children:[],level:3},{value:"Positive and Negative Classes",id:"positive-and-negative-classes",children:[],level:3},{value:"Building a Classification Algorithm",id:"building-a-classification-algorithm",children:[],level:3},{value:"Threshold and Decision Boundary",id:"threshold-and-decision-boundary",children:[],level:3},{value:"Limitations of Linear Regression for Classification",id:"limitations-of-linear-regression-for-classification",children:[],level:3},{value:"Logistic Regression",id:"logistic-regression-1",children:[],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2},{value:"Logistic Regression",id:"logistic-regression-2",children:[{value:"Introduction",id:"introduction",children:[],level:3},{value:"Sigmoid Function",id:"sigmoid-function",children:[],level:3},{value:"Logistic Regression Model",id:"logistic-regression-model",children:[],level:3},{value:"Interpreting Logistic Regression Output",id:"interpreting-logistic-regression-output",children:[],level:3},{value:"Conclusion",id:"conclusion-1",children:[],level:3}],level:2},{value:"Logistic Regression: Understanding Decision Boundaries",id:"logistic-regression-understanding-decision-boundaries",children:[{value:"Introduction",id:"introduction-1",children:[],level:3},{value:"Decision Boundary",id:"decision-boundary",children:[],level:3},{value:"Deriving Decision Boundary",id:"deriving-decision-boundary",children:[],level:3},{value:"Visualizing Decision Boundary",id:"visualizing-decision-boundary",children:[],level:3},{value:"Non-Linear Decision Boundaries",id:"non-linear-decision-boundaries",children:[],level:3},{value:"Linear Decision Boundaries",id:"linear-decision-boundaries",children:[],level:3},{value:"Conclusion",id:"conclusion-2",children:[],level:3}],level:2},{value:"Cost Function for Logistic Regression",id:"cost-function-for-logistic-regression",children:[{value:"Introduction",id:"introduction-2",children:[],level:3},{value:"Logistic Regression Model",id:"logistic-regression-model-1",children:[],level:3},{value:"Squared Error Cost Function for Linear Regression",id:"squared-error-cost-function-for-linear-regression",children:[],level:3},{value:"Non-Convexity of Squared Error Cost Function for Logistic Regression",id:"non-convexity-of-squared-error-cost-function-for-logistic-regression",children:[],level:3},{value:"Redefining the Cost Function for Logistic Regression",id:"redefining-the-cost-function-for-logistic-regression",children:[],level:3},{value:"Analysis of Loss Function for y = 1",id:"analysis-of-loss-function-for-y--1",children:[],level:3},{value:"Analysis of Loss Function for y = 0",id:"analysis-of-loss-function-for-y--0",children:[],level:3},{value:"Conclusion",id:"conclusion-3",children:[],level:3}],level:2},{value:"Lecture Notes: Logistic Regression",id:"lecture-notes-logistic-regression",children:[{value:"Introduction",id:"introduction-3",children:[],level:3},{value:"Finding the Parameters",id:"finding-the-parameters",children:[],level:3},{value:"Derivatives of the Cost Function",id:"derivatives-of-the-cost-function",children:[],level:3},{value:"Gradient Descent for Logistic Regression",id:"gradient-descent-for-logistic-regression",children:[],level:3},{value:"Convergence and Vectorization",id:"convergence-and-vectorization",children:[],level:3}],level:2}],c={toc:l};function u(e){let{components:i,...n}=e;return(0,o.kt)("wrapper",(0,t.Z)({},c,n,{components:i,mdxType:"MDXLayout"}),(0,o.kt)("h1",{id:"6---classification-with-logistic-regression"},"6 - classification with logistic regression"),(0,o.kt)("h2",{id:"logistic-regression"},"Logistic Regression"),(0,o.kt)("h3",{id:"classification-problems"},"Classification Problems"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Linear regression is not suitable for classification problems"),(0,o.kt)("li",{parentName:"ul"},"Classification predicts discrete values, not continuous"),(0,o.kt)("li",{parentName:"ul"},"Examples: spam email detection, fraudulent transaction detection, tumor classification")),(0,o.kt)("h3",{id:"binary-classification"},"Binary Classification"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Binary classification: output variable y has two possible values (0 or 1)"),(0,o.kt)("li",{parentName:"ul"},"Classifying between two possible classes or categories"),(0,o.kt)("li",{parentName:"ul"},"Common designations: no/yes, false/true, 0/1")),(0,o.kt)("h3",{id:"positive-and-negative-classes"},"Positive and Negative Classes"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"False/zero class: negative class"),(0,o.kt)("li",{parentName:"ul"},"True/one class: positive class"),(0,o.kt)("li",{parentName:"ul"},"Example: spam classification",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Non-spam email: negative example (y = 0)"),(0,o.kt)("li",{parentName:"ul"},"Spam email: positive training example (y = 1)"))),(0,o.kt)("li",{parentName:"ul"},"Negative and positive do not imply good versus bad, but the absence or presence of something")),(0,o.kt)("h3",{id:"building-a-classification-algorithm"},"Building a Classification Algorithm"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Example: classifying tumor malignancy"),(0,o.kt)("li",{parentName:"ul"},"Training set: tumor size (x) and corresponding label (y)"),(0,o.kt)("li",{parentName:"ul"},"Linear regression can be attempted, but it predicts continuous values")),(0,o.kt)("h3",{id:"threshold-and-decision-boundary"},"Threshold and Decision Boundary"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Applying a threshold (e.g., 0.5) to distinguish categories"),(0,o.kt)("li",{parentName:"ul"},"Threshold intersects the best-fit line, creating a decision boundary"),(0,o.kt)("li",{parentName:"ul"},"Values below the threshold: predicted as 0"),(0,o.kt)("li",{parentName:"ul"},"Values equal to or above the threshold: predicted as 1")),(0,o.kt)("h3",{id:"limitations-of-linear-regression-for-classification"},"Limitations of Linear Regression for Classification"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Addition of a new training example can significantly shift the decision boundary"),(0,o.kt)("li",{parentName:"ul"},"Linear regression fails to maintain consistent classification with new data")),(0,o.kt)("h3",{id:"logistic-regression-1"},"Logistic Regression"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Logistic regression: algorithm for binary classification"),(0,o.kt)("li",{parentName:"ul"},"Output value between 0 and 1"),(0,o.kt)("li",{parentName:"ul"},"Solves binary classification problems effectively"),(0,o.kt)("li",{parentName:"ul"},'Name can be confusing as it contains "regression" while used for classification')),(0,o.kt)("h3",{id:"conclusion"},"Conclusion"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Logistic regression is the preferred algorithm for binary classification"),(0,o.kt)("li",{parentName:"ul"},"Linear regression is not suitable for classification due to its limitations")),(0,o.kt)("h2",{id:"logistic-regression-2"},"Logistic Regression"),(0,o.kt)("h3",{id:"introduction"},"Introduction"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Logistic regression is a widely used classification algorithm"),(0,o.kt)("li",{parentName:"ul"},"Used for classifying whether a tumor is malignant or benign"),(0,o.kt)("li",{parentName:"ul"},'Positive class (malignant): labeled as 1, represented by "yes"'),(0,o.kt)("li",{parentName:"ul"},'Negative class (benign): labeled as 0, represented by "no"')),(0,o.kt)("h3",{id:"sigmoid-function"},"Sigmoid Function"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Sigmoid function (logistic function) is important in logistic regression"),(0,o.kt)("li",{parentName:"ul"},"Outputs values between 0 and 1"),(0,o.kt)("li",{parentName:"ul"},"Denoted as g(z), where z is a linear function of features (wx + b)"),(0,o.kt)("li",{parentName:"ul"},"Formula: g(z) = 1 / (1 + e^(-z))"),(0,o.kt)("li",{parentName:"ul"},"e is a mathematical constant (~2.7)"),(0,o.kt)("li",{parentName:"ul"},"Sigmoid function has an S-shaped curve")),(0,o.kt)("h3",{id:"logistic-regression-model"},"Logistic Regression Model"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Combining the linear function (wx + b) and sigmoid function (g(z))"),(0,o.kt)("li",{parentName:"ul"},"Logistic regression model: f(x) = g(wx + b)"),(0,o.kt)("li",{parentName:"ul"},"Outputs a value between 0 and 1"),(0,o.kt)("li",{parentName:"ul"},"Interpretation: probability of y = 1 given input x")),(0,o.kt)("h3",{id:"interpreting-logistic-regression-output"},"Interpreting Logistic Regression Output"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Output probability represents the chance of y = 1"),(0,o.kt)("li",{parentName:"ul"},"Example: if model outputs 0.7 for a tumor size x"),(0,o.kt)("li",{parentName:"ul"},"Interpretation: 70% chance that the tumor is malignant (y = 1)"),(0,o.kt)("li",{parentName:"ul"},"Complementary probability: 30% chance that the tumor is benign (y = 0)")),(0,o.kt)("h3",{id:"conclusion-1"},"Conclusion"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Logistic regression is a widely used classification algorithm"),(0,o.kt)("li",{parentName:"ul"},"Applies the sigmoid function to a linear function of features"),(0,o.kt)("li",{parentName:"ul"},"Output represents the probability of the positive class (y = 1)"),(0,o.kt)("li",{parentName:"ul"},"Complementary probability represents the probability of the negative class (y = 0)")),(0,o.kt)("h2",{id:"logistic-regression-understanding-decision-boundaries"},"Logistic Regression: Understanding Decision Boundaries"),(0,o.kt)("h3",{id:"introduction-1"},"Introduction"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Recap of logistic regression model"),(0,o.kt)("li",{parentName:"ul"},"Two steps: compute z = wx + b, apply sigmoid function g(z)"),(0,o.kt)("li",{parentName:"ul"},"Formula: f(x) = g(wx + b)")),(0,o.kt)("h3",{id:"decision-boundary"},"Decision Boundary"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Defining when to predict y = 1 or y = 0"),(0,o.kt)("li",{parentName:"ul"},"Threshold approach: set threshold (e.g., 0.5)"),(0,o.kt)("li",{parentName:"ul"},"If f(x) >= 0.5, predict y = 1"),(0,o.kt)("li",{parentName:"ul"},"If f(x) < 0.5, predict y = 0")),(0,o.kt)("h3",{id:"deriving-decision-boundary"},"Deriving Decision Boundary"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"f(x) = g(z), so f(x) >= 0.5 if g(z) >= 0.5"),(0,o.kt)("li",{parentName:"ul"},"g(z) >= 0.5 when z >= 0"),(0,o.kt)("li",{parentName:"ul"},"z = wx + b, so z >= 0 when wx + b >= 0"),(0,o.kt)("li",{parentName:"ul"},"Decision boundary: wx + b = 0")),(0,o.kt)("h3",{id:"visualizing-decision-boundary"},"Visualizing Decision Boundary"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Example: two features (x1, x2)"),(0,o.kt)("li",{parentName:"ul"},"Training set with positive (y = 1) and negative (y = 0) examples"),(0,o.kt)("li",{parentName:"ul"},"Decision boundary equation: wx1 + wx2 + b = 0"),(0,o.kt)("li",{parentName:"ul"},"Example parameters: w1 = 1, w2 = 1, b = -3"),(0,o.kt)("li",{parentName:"ul"},"Decision boundary: x1 + x2 - 3 = 0")),(0,o.kt)("h3",{id:"non-linear-decision-boundaries"},"Non-Linear Decision Boundaries"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Polynomial features in logistic regression"),(0,o.kt)("li",{parentName:"ul"},"Example with z = w1x1^2 + w2x2^2 + b"),(0,o.kt)("li",{parentName:"ul"},"Decision boundary: x1^2 + x2^2 = 1 (circle)"),(0,o.kt)("li",{parentName:"ul"},"Complex decision boundaries possible with higher-order polynomials"),(0,o.kt)("li",{parentName:"ul"},"Logistic regression can fit complex data")),(0,o.kt)("h3",{id:"linear-decision-boundaries"},"Linear Decision Boundaries"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Without higher-order polynomials, decision boundary is linear"),(0,o.kt)("li",{parentName:"ul"},"Decision boundary will always be a straight line"),(0,o.kt)("li",{parentName:"ul"},"Features x1, x2, x3, etc. result in linear decision boundary")),(0,o.kt)("h3",{id:"conclusion-2"},"Conclusion"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Logistic regression uses decision boundary to make predictions"),(0,o.kt)("li",{parentName:"ul"},"Decision boundary determined by parameters (w) and bias (b)"),(0,o.kt)("li",{parentName:"ul"},"Can be linear or non-linear depending on features and polynomials")),(0,o.kt)("p",null,"In conclusion, logistic regression models use decision boundaries to predict whether an instance belongs to a certain class. The decision boundary is determined by the parameters (w) and bias (b) of the logistic regression model. It can be linear or non-linear, depending on the features and the use of polynomial terms. Logistic regression is capable of fitting complex data by incorporating higher-order polynomials. However, if only linear features are used, the decision boundary will always be a straight line. Understanding decision boundaries helps in visualizing how logistic regression makes predictions and how different models can be obtained by varying the parameters and features."),(0,o.kt)("h2",{id:"cost-function-for-logistic-regression"},"Cost Function for Logistic Regression"),(0,o.kt)("h3",{id:"introduction-2"},"Introduction"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The cost function measures how well a set of parameters fits the training data."),(0,o.kt)("li",{parentName:"ul"},"The squared error cost function is not ideal for logistic regression."),(0,o.kt)("li",{parentName:"ul"},"We need a different cost function that can help us choose better parameters.")),(0,o.kt)("h3",{id:"logistic-regression-model-1"},"Logistic Regression Model"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Training set consists of rows representing patient visits and diagnoses."),(0,o.kt)("li",{parentName:"ul"},"Each training example has n features (e.g., tumor size, age)."),(0,o.kt)("li",{parentName:"ul"},"Target label y takes values 0 or 1."),(0,o.kt)("li",{parentName:"ul"},"Logistic regression model: f(x) = 1 / (1 + e^(-wx + b))")),(0,o.kt)("h3",{id:"squared-error-cost-function-for-linear-regression"},"Squared Error Cost Function for Linear Regression"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Cost function for linear regression: J(w, b) = (1/2m) ","*"," \u2211(f(x) - y)^2"),(0,o.kt)("li",{parentName:"ul"},"This cost function is convex and suitable for linear regression.")),(0,o.kt)("h3",{id:"non-convexity-of-squared-error-cost-function-for-logistic-regression"},"Non-Convexity of Squared Error Cost Function for Logistic Regression"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Using squared error cost function with logistic regression leads to a non-convex cost function."),(0,o.kt)("li",{parentName:"ul"},"Gradient descent may get stuck in local minima."),(0,o.kt)("li",{parentName:"ul"},"We need a convex cost function to guarantee convergence to the global minimum.")),(0,o.kt)("h3",{id:"redefining-the-cost-function-for-logistic-regression"},"Redefining the Cost Function for Logistic Regression"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Modify the cost function to ensure convexity."),(0,o.kt)("li",{parentName:"ul"},"Introduce a loss function L(f(x), y) for a single training example."),(0,o.kt)("li",{parentName:"ul"},"Define the loss function for logistic regression:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"If y = 1, loss = -log(f(x))"),(0,o.kt)("li",{parentName:"ul"},"If y = 0, loss = -log(1 - f(x))")))),(0,o.kt)("h3",{id:"analysis-of-loss-function-for-y--1"},"Analysis of Loss Function for y = 1"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Plotting the loss function for y = 1:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Logarithm of f(x) is a convex curve intersecting the horizontal axis at f = 1."),(0,o.kt)("li",{parentName:"ul"},"Negative log of f(x) is a flipped curve that incentivizes accurate predictions."),(0,o.kt)("li",{parentName:"ul"},"Higher loss for lower predicted values when true label is 1.")))),(0,o.kt)("h3",{id:"analysis-of-loss-function-for-y--0"},"Analysis of Loss Function for y = 0"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Plotting the loss function for y = 0:",(0,o.kt)("ul",{parentName:"li"},(0,o.kt)("li",{parentName:"ul"},"Loss is negative log of 1 - f(x)."),(0,o.kt)("li",{parentName:"ul"},"Higher loss as predicted value moves away from true label 0."),(0,o.kt)("li",{parentName:"ul"},"Loss approaches infinity as prediction approaches 1.")))),(0,o.kt)("h3",{id:"conclusion-3"},"Conclusion"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Squared error cost function is unsuitable for logistic regression."),(0,o.kt)("li",{parentName:"ul"},"New loss function ensures convexity of the cost function."),(0,o.kt)("li",{parentName:"ul"},"Gradient descent can reliably find the global minimum."),(0,o.kt)("li",{parentName:"ul"},"The cost function is an average of the loss function on all training examples."),(0,o.kt)("li",{parentName:"ul"},"In the next video, we'll define the overall cost function for the entire training set and simplify its notation.")),(0,o.kt)("h2",{id:"lecture-notes-logistic-regression"},"Lecture Notes: Logistic Regression"),(0,o.kt)("h3",{id:"introduction-3"},"Introduction"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Logistic regression is a model used for classification tasks."),(0,o.kt)("li",{parentName:"ul"},"Parameters w and b need to be determined to fit the model."),(0,o.kt)("li",{parentName:"ul"},"The goal is to minimize the cost function J(w, b) by using gradient descent.")),(0,o.kt)("h3",{id:"finding-the-parameters"},"Finding the Parameters"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The parameters w and b are chosen to make predictions or estimate the probability of a label being one."),(0,o.kt)("li",{parentName:"ul"},"The cost function J(w, b) is minimized using gradient descent."),(0,o.kt)("li",{parentName:"ul"},"Gradient descent involves updating parameters iteratively: w = w - \u03b1 ",(0,o.kt)("em",{parentName:"li"}," \u2202J/\u2202w and b = b - \u03b1 ")," \u2202J/\u2202b.")),(0,o.kt)("h3",{id:"derivatives-of-the-cost-function"},"Derivatives of the Cost Function"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The derivative of J with respect to w",(0,o.kt)("em",{parentName:"li"},"j is given by 1/m ")," \u2211(f - y) _ x_j."),(0,o.kt)("li",{parentName:"ul"},"f is the predicted value, y is the label, x_j is the j-th feature, and m is the number of training examples."),(0,o.kt)("li",{parentName:"ul"},"The derivative of J with respect to b is given by 1/m ","*"," \u2211(f - y).")),(0,o.kt)("h3",{id:"gradient-descent-for-logistic-regression"},"Gradient Descent for Logistic Regression"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"The derivative expressions are plugged into the gradient descent update equations."),(0,o.kt)("li",{parentName:"ul"},"Logistic regression and linear regression share similar update equations, but the definition of f(x) differs."),(0,o.kt)("li",{parentName:"ul"},"In logistic regression, f(x) is the sigmoid function applied to wx + b.")),(0,o.kt)("h3",{id:"convergence-and-vectorization"},"Convergence and Vectorization"),(0,o.kt)("ul",null,(0,o.kt)("li",{parentName:"ul"},"Similar to linear regression, monitoring convergence is essential for gradient descent in logistic regression."),(0,o.kt)("li",{parentName:"ul"},"Vectorization can be used to speed up the computation of gradient descent."),(0,o.kt)("li",{parentName:"ul"},"Feature scaling, which scales features to similar ranges, can aid convergence in logistic regression.")))}u.isMDXComponent=!0}}]);