"use strict";(self.webpackChunkmyblog=self.webpackChunkmyblog||[]).push([[6796],{3905:(e,t,n)=>{n.d(t,{Zo:()=>u,kt:()=>m});var i=n(7294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function a(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,i)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?a(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):a(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,i,r=function(e,t){if(null==e)return{};var n,i,r={},a=Object.keys(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);for(i=0;i<a.length;i++)n=a[i],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var s=i.createContext({}),p=function(e){var t=i.useContext(s),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},u=function(e){var t=p(e.components);return i.createElement(s.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return i.createElement(i.Fragment,{},t)}},d=i.forwardRef((function(e,t){var n=e.components,r=e.mdxType,a=e.originalType,s=e.parentName,u=l(e,["components","mdxType","originalType","parentName"]),d=p(n),m=r,h=d["".concat(s,".").concat(m)]||d[m]||c[m]||a;return n?i.createElement(h,o(o({ref:t},u),{},{components:n})):i.createElement(h,o({ref:t},u))}));function m(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var a=n.length,o=new Array(a);o[0]=d;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:r,o[1]=l;for(var p=2;p<a;p++)o[p]=n[p];return i.createElement.apply(null,o)}return i.createElement.apply(null,n)}d.displayName="MDXCreateElement"},3485:(e,t,n)=>{n.r(t),n.d(t,{frontMatter:()=>a,contentTitle:()=>o,metadata:()=>l,toc:()=>s,default:()=>u});var i=n(7462),r=(n(7294),n(3905));const a={},o="6-Neural Network Training",l={unversionedId:"Advanced Learning Algorithms/Training a Neural Network in TensorFlow",id:"Advanced Learning Algorithms/Training a Neural Network in TensorFlow",title:"6-Neural Network Training",description:"TensorFlow implementation",source:"@site/docs/Advanced Learning Algorithms/13-Training a Neural Network in TensorFlow.md",sourceDirName:"Advanced Learning Algorithms",slug:"/Advanced Learning Algorithms/Training a Neural Network in TensorFlow",permalink:"/docs/Advanced Learning Algorithms/Training a Neural Network in TensorFlow",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Advanced Learning Algorithms/13-Training a Neural Network in TensorFlow.md",tags:[],version:"current",sidebarPosition:13,frontMatter:{},sidebar:"tutorialSidebar",previous:{title:"5-Paths to Artificial General Intelligence (AGI)",permalink:"/docs/Advanced Learning Algorithms/Speculations on AGI"},next:{title:"7-Activation Functions",permalink:"/docs/Advanced Learning Algorithms/Activation Functions"}},s=[{value:"TensorFlow implementation",id:"tensorflow-implementation",children:[{value:"Neural Network Architecture",id:"neural-network-architecture",children:[],level:3},{value:"Training Process",id:"training-process",children:[],level:3},{value:"Key Points",id:"key-points",children:[],level:3}],level:2},{value:"Training Details",id:"training-details",children:[{value:"Introduction",id:"introduction",children:[],level:3},{value:"Logistic Regression Model Training",id:"logistic-regression-model-training",children:[],level:3},{value:"Training a Neural Network in TensorFlow",id:"training-a-neural-network-in-tensorflow",children:[{value:"Step 1: Compute Output",id:"step-1-compute-output",children:[],level:4},{value:"Step 2: Specify Loss Function",id:"step-2-specify-loss-function",children:[],level:4},{value:"Step 3: Minimize Cost Function",id:"step-3-minimize-cost-function",children:[],level:4}],level:3},{value:"Conclusion",id:"conclusion",children:[],level:3}],level:2}],p={toc:s};function u(e){let{components:t,...n}=e;return(0,r.kt)("wrapper",(0,i.Z)({},p,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"6-neural-network-training"},"6-Neural Network Training"),(0,r.kt)("h2",{id:"tensorflow-implementation"},"TensorFlow implementation"),(0,r.kt)("h3",{id:"neural-network-architecture"},"Neural Network Architecture"),(0,r.kt)("p",null,"Our neural network architecture consists of an input layer, two hidden layers, and an output layer. The input layer takes in images represented by X, while the output layer predicts whether the image is a zero or a one. The first hidden layer has 25 units with a sigmoid activation function, followed by a second hidden layer with 15 units."),(0,r.kt)("h3",{id:"training-process"},"Training Process"),(0,r.kt)("p",null,"we can use these code in TensorFlow to train this network:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(25, activation='sigmoid'),\n    tf.keras.layers.Dense(15, activation='sigmoid'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')\n\nmodel.fit(X, Y, epochs=10)\n")),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Specify the Model"),(0,r.kt)("p",{parentName:"li"},"We start by defining the model using TensorFlow's ",(0,r.kt)("inlineCode",{parentName:"p"},"Sequential")," API. The ",(0,r.kt)("inlineCode",{parentName:"p"},"Sequential")," model allows us to sequentially string together the layers of the neural network. In our case, we have three layers: the first hidden layer with 25 units and a sigmoid activation function, followed by the second hidden layer, and finally the output layer.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Compile the Model"),(0,r.kt)("p",{parentName:"li"},"Once the model is specified, we need to compile it using the ",(0,r.kt)("inlineCode",{parentName:"p"},"compile")," function. The key step here is to specify the loss function we want to use. In our example, we use the binary cross-entropy loss function, which will be explained in more detail in the next video.")),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Train the Model"),(0,r.kt)("p",{parentName:"li"},"After compiling the model, we can now train it using the ",(0,r.kt)("inlineCode",{parentName:"p"},"fit")," function. This function fits the model to the training dataset ",(0,r.kt)("inlineCode",{parentName:"p"},"X")," and the corresponding ground truth labels ",(0,r.kt)("inlineCode",{parentName:"p"},"Y"),". The parameter ",(0,r.kt)("inlineCode",{parentName:"p"},"epochs")," determines the number of steps or iterations of the learning algorithm, such as gradient descent, to run."))),(0,r.kt)("h3",{id:"key-points"},"Key Points"),(0,r.kt)("p",null,"It's crucial to understand the code and the underlying concepts behind it. Simply calling the code without comprehension can lead to difficulties when things don't work as expected. Having a conceptual mental framework of the training process helps with debugging and troubleshooting."),(0,r.kt)("h2",{id:"training-details"},"Training Details"),(0,r.kt)("h3",{id:"introduction"},"Introduction"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Details of training a neural network in TensorFlow code"),(0,r.kt)("li",{parentName:"ul"},"Comparison with logistic regression model training")),(0,r.kt)("h3",{id:"logistic-regression-model-training"},"Logistic Regression Model Training"),(0,r.kt)("ol",null,(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Specify the input-output function of logistic regression:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Logistic regression predicts f(x) = G(z), where G is the sigmoid function applied to the dot product of weights (W) and input features (X) plus the bias term (B)."),(0,r.kt)("li",{parentName:"ul"},"The sigmoid function is defined as: G(z) = 1 / (1 + e^(-z)), where z = W\xb7X + B."))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Specify the loss and cost functions:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Loss function for a single training example (x, y): negative y log(f(x)) - (1 - y) log(1 - f(x))."),(0,r.kt)("li",{parentName:"ul"},"Cost function (J) is the average of the loss function over the entire training set."))),(0,r.kt)("li",{parentName:"ol"},(0,r.kt)("p",{parentName:"li"},"Minimize the cost function using gradient descent:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Update weights (W) and bias (B) using the gradients of J with respect to W and B."),(0,r.kt)("li",{parentName:"ul"},"Update equations: W = W - \u03b1 ",(0,r.kt)("em",{parentName:"li"}," \u2202J/\u2202W, B = B - \u03b1 ")," \u2202J/\u2202B.")))),(0,r.kt)("h3",{id:"training-a-neural-network-in-tensorflow"},"Training a Neural Network in TensorFlow"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"The same three steps are used to train a neural network.")),(0,r.kt)("h4",{id:"step-1-compute-output"},"Step 1: Compute Output"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Specify the neural network architecture using TensorFlow code.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Example code:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(25, activation='sigmoid', input_shape=(input_size,)),\n    tf.keras.layers.Dense(15, activation='sigmoid'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"The code specifies the number of hidden units in each layer and the activation function used."))),(0,r.kt)("h4",{id:"step-2-specify-loss-function"},"Step 2: Specify Loss Function"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Use the binary cross-entropy loss function for classification problems:"),(0,r.kt)("ul",{parentName:"li"},(0,r.kt)("li",{parentName:"ul"},"Loss function: -y log(f(x)) - (1 - y) log(1 - f(x)), where y is the ground truth label and f(x) is the output of the neural network."))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Example code:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},"model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer='sgd')\n")))),(0,r.kt)("h4",{id:"step-3-minimize-cost-function"},"Step 3: Minimize Cost Function"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Use an optimization algorithm (e.g., gradient descent) to minimize the cost function.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"TensorFlow's ",(0,r.kt)("inlineCode",{parentName:"p"},"fit")," function handles the optimization process.")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"Example code:"),(0,r.kt)("pre",{parentName:"li"},(0,r.kt)("code",{parentName:"pre"},"model.fit(X, y, epochs=100)\n"))),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("p",{parentName:"li"},"TensorFlow uses backpropagation to compute the partial derivatives required for optimization."))),(0,r.kt)("h3",{id:"conclusion"},"Conclusion"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"TensorFlow simplifies the training process by providing high-level functions and libraries."),(0,r.kt)("li",{parentName:"ul"},"Libraries like TensorFlow and PyTorch are commonly used for neural network implementations."),(0,r.kt)("li",{parentName:"ul"},"Understanding the underlying principles helps in troubleshooting and customization.")))}u.isMDXComponent=!0}}]);